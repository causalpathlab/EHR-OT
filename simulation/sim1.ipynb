{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $D$: total number of features\n",
    "\n",
    "* $d_{1}$: number of features with higher frequency in a domain 1\n",
    "\n",
    "* $d_{2}$: number of features with higher frequency in a domain 2\n",
    "\n",
    "\n",
    "* $d_{1} \\sim \\operatorname{Unif}(0, \\lfloor D/4 \\rfloor)$\n",
    "\n",
    "* $d_{2} \\sim \\operatorname{Unif}(0, \\lfloor D/4 \\rfloor)$ ($d_{1} + d_{2} < D$)\n",
    "\n",
    "* $k \\in [D]$ for indexing feature\n",
    "\n",
    "* Let $\\Delta_{r} = \\{j \\in [D]: \\textrm{feature } j \\textrm{ is more frequent in a domain } r \\}$\n",
    "\n",
    "* Sample $\\Delta_{1} \\subseteq [D]$ such that $|\\Delta_{1}| = d_{1}$ \n",
    "\n",
    "* Sample $\\Delta_{2} \\subseteq [D]\\backslash \\Delta_{1}$ such that $|\\Delta_{2}| = d_{2}$\n",
    "\n",
    "* Let $\\alpha_{1} \\overset{\\Delta}{=} \\left( \\alpha_{11}, \\ldots, \\alpha_{1D} \\right)$ be a feature frequency vector for a domain 1\n",
    "\n",
    "* Let $\\alpha_{2} \\overset{\\Delta}{=} \\left( \\alpha_{21}, \\ldots, \\alpha_{2D} \\right)$ be a feature frequency vector for a domain 2\n",
    "\n",
    "\n",
    "* For each $k \\in [D]$: \n",
    "\n",
    "    * If $k \\in \\Delta_{1}$, $\\alpha_{1k} > \\alpha_{2k}$\n",
    "\n",
    "    * If $k \\in \\Delta_{2}$, $\\alpha_{2k} > \\alpha_{1k}$\n",
    "\n",
    "    * Otherwise $\\alpha_{1k} = \\alpha_{2k}$\n",
    "\n",
    "\n",
    "* Sample $\\rho_{1} \\sim \\operatorname{Dir}(\\alpha_{1})$\n",
    "\n",
    "* Sample $\\rho_{2} \\sim \\operatorname{Dir}(\\alpha_{2})$\n",
    "\n",
    "* Sample a code-specific contribution to mortality: $W_{k} \\sim  \\max\\{ \\mathcal{N}\\!\\left(0,1\\right), 0\\}$\n",
    "\n",
    "* For each patient $i$ in a domain $r$\n",
    "\n",
    "    * $\\tilde{X}_{i} \\sim \\operatorname{Multi}(n_{i}; \\rho_{r})$ where $\\tilde{X}_{i}$ is a vector of counts for each diagnosis code/feature, $n_i = 3k$.\n",
    "\n",
    "\t* We set $X_{ik} = \\min \\left\\{ \\tilde{X}_{ik}, 1 \\right\\}$ where $\\tilde{X}_{ik}$ is a count for a diagnosis code/feature $k$, $k \\in \\{1,..., [D]\\}$\n",
    "\n",
    "* For all patient $i$ in a domain $r$\n",
    "\n",
    "    * $b = \\operatorname{Median}(\\sum_{k} W_{k} \\tilde{X}_{ik})$\n",
    "\n",
    "* For each patient $i$ in a domain $r$\n",
    "\n",
    "\t* pathogenic score $\\bar{p}_{i} = \\operatorname{sigmoid}(\\sum_{k} W_{k} \\tilde{X}_{ik} + b)$ (aka a liability model)\n",
    "\n",
    "\t* Sample $Y_{i} \\sim \\operatorname{Bern}(\\bar{p}_{i})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems with simulation scheme and ETM\n",
    "\n",
    "- If using one $\\rho$ per domain or one $\\rho$ per patient, learn the same representation for all patients\n",
    "\n",
    "- If using multiple $\\rho$ per domain (e.g. 3 $\\rho$'s per domain), ETM learns the representation for different $\\rho$'s, but no other information in the differences of codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/wanxinli/deep_patient/synthetic_exp\")\n",
    "\n",
    "from common import *\n",
    "from deep_patient.sda import SDA\n",
    "from math import floor, exp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import dirichlet\n",
    "import ot\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from scipy import sparse\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "base_dir = \"/home/wanxinli/deep_patient\"\n",
    "data_dir = \"outputs/synthetic/simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Simulation scheme\n",
    "\"\"\"\n",
    "\n",
    "def simulate(D, d_1, d_2, num_patient):\n",
    "    \"\"\" \n",
    "    Simulate features and labels for domain 1 and domain 2\n",
    "    :param int D:  total number of features\n",
    "    :param int d_1: number of features with higher frequency in domain 1\n",
    "    :param int d_2: number of features with higher frequency in domain 2\n",
    "    :param int num_patient: number of patients in each domain\n",
    "\n",
    "    Variables in the implementation are consistent with the variables in the scheme\n",
    "\n",
    "    :return\n",
    "        list[list[int]] domain 1 features\n",
    "        list[int] domain 1 labels\n",
    "        list[list[int]] domain 2 features\n",
    "        list[int] domain 2 labels\n",
    "    \"\"\"\n",
    "\n",
    "    d_1 = randint(0, floor(0.25*D))\n",
    "    d_2 = randint(0, floor(0.25*D))\n",
    "    delta_1 = np.random.choice(size = d_1, a = range(1, D+1), replace=False)\n",
    "    remaining_set = list(set(list(range(1, D+1)))-set(delta_1))\n",
    "    delta_2 = np.random.choice(size = d_1, a = remaining_set, replace=False)\n",
    "    \n",
    "    # We set the proportions of d_1 codes, d_2 codes, and (D-d_1-d_2) codes to be 2:1:1.5\n",
    "    unit_1 = 1/(0.5*d_1-0.5*d_2+1.5*D)\n",
    "    alpha_1 = [2*unit_1]*d_1\n",
    "    alpha_1.extend([unit_1]*d_2)\n",
    "    alpha_1.extend([1.5*unit_1]*(D-d_1-d_2))\n",
    "\n",
    "    # We set the proportions of d_1 codes, d_2 codes, and (D-d_1-d_2) codes to be 1:2:1.5\n",
    "    unit_2 = 1/(-0.5*d_1+0.5*d_2+1.5*D)\n",
    "    alpha_2 = [2*unit_2]*d_1\n",
    "    alpha_2.extend([unit_2]*d_2)\n",
    "    alpha_2.extend([1.5*unit_2]*(D-d_1-d_2))    \n",
    "\n",
    "    def gen_feature_vector_label(alpha):\n",
    "        \"\"\" \n",
    "        Generate feature vectors and labels\n",
    "        :param list[float] alpha: concentration parameteres for the dirichlet distribution\n",
    "        \"\"\"\n",
    "\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + exp(-x))\n",
    "\n",
    "        rho = dirichlet(alpha=alpha, size=1)[0]\n",
    "        W = np.random.normal(size=D)\n",
    "        W = [max(0, W_k) for W_k in W] # only sample positive weights\n",
    "        X = []\n",
    "        Y = []\n",
    "        b = 0\n",
    "        all_sum = []\n",
    "\n",
    "        for _ in range(num_patient):\n",
    "            X_i = np.random.multinomial(3*len(rho), rho)\n",
    "            for k in range(len(X_i)):\n",
    "                if X_i[k] > 0:\n",
    "                    X_i[k] = 1 # dominant effect\n",
    "            X.append(X_i)\n",
    "            cur_sum = np.sum(np.multiply(W, X_i))\n",
    "            all_sum.append(cur_sum)\n",
    "        # plt.hist(all_sum)\n",
    "        # plt.show()\n",
    "\n",
    "        all_sum = np.array(all_sum)\n",
    "        print(\"all_sum is:\", all_sum)\n",
    "        b = -np.mean(all_sum) # TODO: not a good choice of b if -b pushes many p_i to 0.5 (too much randomness)\n",
    "        \n",
    "        P = []\n",
    "        for cur_sum in all_sum:\n",
    "            p_i = sigmoid(cur_sum+b)\n",
    "            P.append(p_i)\n",
    "            Y_i = 0\n",
    "            if p_i >= 0.5: # TODO: mimic exact logistic regression, change to np.random.binomial later\n",
    "                Y_i = 1\n",
    "            # Y_i = np.random.binomial(1, p_i) # too much noise, domain 1 data cannot learn well\n",
    "            Y.append(int(Y_i))\n",
    "        # print(\"P is:\", P)\n",
    "\n",
    "            \n",
    "        return X, Y, W, b\n",
    "    \n",
    "    def feature_vector_to_feature(feature_vectors):\n",
    "        \"\"\" \n",
    "        Convert feature vectors to features\n",
    "        :param list[list[int]]: feature vectors consisting of indicators\n",
    "\n",
    "        Returns\n",
    "            - features consisting of actual codes\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for feature_vector in feature_vectors:\n",
    "            features.append([i for i, e in enumerate(feature_vector) if e != 0])\n",
    "        return features\n",
    "    \n",
    "    def pad_features(features_list):\n",
    "        \"\"\" \n",
    "        Pad features to the same length (maximum length of the original features)\\\n",
    "            in each domain by -1\n",
    "        \"\"\"\n",
    "        max_len = 0\n",
    "        for features in features_list:\n",
    "            max_len = max(max_len, len(features))\n",
    "\n",
    "        for i in range(len(features_list)):\n",
    "            features_list[i] += [-1] * (max_len - len(features_list[i]))\n",
    "        return features_list\n",
    "\n",
    "\n",
    "\n",
    "    feature_vector_1, label_1, W_1, b_1 = gen_feature_vector_label(alpha_1)\n",
    "    feature_1 = pad_features(feature_vector_to_feature(feature_vector_1))\n",
    "    feature_vector_2, label_2, W_2, b_2 = gen_feature_vector_label(alpha_2)\n",
    "    feature_2 = pad_features(feature_vector_to_feature(feature_vector_2))\n",
    "    return np.array(feature_1), label_1, np.array(feature_2), label_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Wrapper function with different set ups for simulate()\n",
    "\"\"\"\n",
    "def simulate_1():\n",
    "    D = 20\n",
    "    d_1 = 5\n",
    "    d_2 = 5\n",
    "    num_patient = 100\n",
    "    return simulate(D, d_1, d_2, num_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train deep patient model and generate representations for males and females\n",
    "\"\"\"\n",
    "\n",
    "def custom_train_reps(male_seqs, female_seqs):\n",
    "    \"\"\" \n",
    "    Customized training algorithm for generating male representations and female representations\n",
    "    \n",
    "    :returns: male representations, female representations\n",
    "    \"\"\"\n",
    "\n",
    "    # customized parameters\n",
    "    nhidden = 5\n",
    "    nlayer = 1\n",
    "\n",
    "    # for males\n",
    "    # initiate the model\n",
    "    male_sda = SDA(male_seqs.shape[1],\n",
    "                nhidden=nhidden,\n",
    "                nlayer=nlayer,\n",
    "                param={\n",
    "        'epochs': 10,\n",
    "        'batch_size': 5,\n",
    "        'corrupt_lvl': 0.05\n",
    "    })\n",
    "\n",
    "    # train the model\n",
    "    male_sda.train(male_seqs)\n",
    "\n",
    "    # apply the mode\n",
    "    male_reps = male_sda.apply(male_seqs)\n",
    "\n",
    "    # for females\n",
    "    # initiate the model\n",
    "    female_sda = SDA(female_seqs.shape[1],\n",
    "                nhidden=nhidden,\n",
    "                nlayer=nlayer,\n",
    "                param={\n",
    "        'epochs': 10,\n",
    "        'batch_size': 5,\n",
    "        'corrupt_lvl': 0.05\n",
    "    })\n",
    "\n",
    "    # train the model\n",
    "    female_sda.train(female_seqs)\n",
    "\n",
    "    # apply the mode\n",
    "    female_reps = female_sda.apply(female_seqs)\n",
    "    return male_reps, female_reps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sum is: [4.5507281  1.91361405 1.44642349 4.11981221 4.11981221 4.5507281\n",
      " 2.44348841 3.12274729 5.54779302 4.08353754 1.91361405 4.5507281\n",
      " 1.44642349 1.91361405 4.5507281  2.44348841 4.5507281  3.65262165\n",
      " 2.91067898 5.54779302 4.5507281  4.08353754 4.08353754 2.91067898\n",
      " 2.44348841 2.91067898 4.11981221 2.44348841 3.12274729 4.08353754\n",
      " 2.65555672 4.5507281  4.5507281  5.08060246 2.91067898 2.44348841\n",
      " 1.91361405 1.91361405 1.91361405 1.91361405 2.91067898 4.5507281\n",
      " 5.54779302 1.91361405 1.44642349 1.91361405 2.44348841 2.91067898\n",
      " 2.91067898 2.91067898 6.75692626 2.91067898 4.5507281  1.44642349\n",
      " 1.44642349 4.5507281  4.11981221 2.44348841 2.44348841 1.44642349\n",
      " 1.44642349 2.91067898 4.08353754 1.91361405 4.11981221 4.08353754\n",
      " 2.91067898 1.91361405 2.91067898 2.91067898 2.91067898 3.12274729\n",
      " 1.91361405 5.16355288 2.91067898 2.91067898 2.91067898 2.91067898\n",
      " 2.91067898 1.91361405 1.44642349 1.44642349 1.44642349 1.91361405\n",
      " 4.5507281  4.5507281  5.54779302 1.91361405 1.91361405 5.08060246\n",
      " 1.44642349 1.44642349 1.91361405 1.91361405 1.44642349 1.44642349\n",
      " 3.65262165 1.91361405 4.5507281  3.12274729]\n",
      "all_sum is: [4.78284267 4.73498502 2.2003682  4.78284267 1.98361733 3.64705235\n",
      " 2.65988793 2.61203027 4.73498502 3.28830087 2.65988793 4.95173589\n",
      " 1.5240976  4.2754653  3.64705235 2.65988793 5.41125562 3.64705235\n",
      " 2.65988793 2.65988793 1.98361733 2.82878114 2.65988793 4.78284267\n",
      " 2.61203027 4.78284267 3.28830087 2.15251055 2.65988793 3.28830087\n",
      " 3.28830087 5.41125562 2.65988793 3.28830087 3.28830087 1.98361733\n",
      " 4.32332295 2.65988793 4.78284267 2.65988793 1.98361733 4.95173589\n",
      " 4.10657208 5.41125562 2.82878114 4.10657208 4.10657208 2.65988793\n",
      " 2.65988793 4.78284267 2.2003682  2.65988793 4.73498502 4.10657208\n",
      " 4.10657208 3.28830087 4.32332295 1.98361733 4.78284267 3.28830087\n",
      " 4.95173589 3.28830087 1.98361733 4.73498502 5.41125562 4.78284267\n",
      " 3.64705235 4.73498502 4.78284267 2.65988793 2.65988793 2.2003682\n",
      " 4.73498502 3.28830087 2.65988793 2.15251055 2.65988793 2.15251055\n",
      " 2.61203027 5.41125562 4.2754653  4.32332295 2.65988793 2.82878114\n",
      " 5.41125562 4.73498502 4.78284267 3.28830087 1.98361733 2.61203027\n",
      " 5.41125562 2.2003682  4.78284267 2.2003682  5.41125562 4.73498502\n",
      " 2.65988793 1.98361733 2.65988793 1.98361733]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.529\n",
      "(*) epoch 2, cost 3.427\n",
      "(*) epoch 3, cost 3.170\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.252\n",
      "(*) epoch 2, cost 4.353\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.24125729 0.56747259 0.56747259 0.56747259 0.56747259 1.24125729\n",
      " 1.00014578 1.24125729 0.56747259 0.56747259 1.24125729 0.56747259\n",
      " 0.56747259 0.56747259 0.56747259 1.24125729 0.56747259 1.24125729\n",
      " 1.24125729 1.24125729 0.56747259 1.24125729 1.24125729 0.56747259\n",
      " 1.24125729 0.56747259 0.56747259 0.56747259 1.24125729 0.56747259\n",
      " 0.56747259 1.24125729 0.68872628 1.24125729 1.24125729 1.24125729\n",
      " 1.24125729 0.56747259 1.24125729 1.24125729 1.24125729 0.56747259\n",
      " 0.56747259 0.56747259 1.24125729 0.56747259 1.24125729 1.24125729\n",
      " 1.24125729 0.56747259 0.56747259 0.56747259 1.24125729 1.24125729\n",
      " 0.56747259 0.56747259 0.56747259 1.24125729 0.56747259 1.24125729\n",
      " 0.56747259 0.56747259 0.56747259 1.24125729 1.24125729 1.24125729\n",
      " 0.56747259 1.24125729 0.56747259 0.56747259 0.56747259 1.24125729\n",
      " 0.56747259 1.24125729 0.56747259 0.56747259 0.56747259 1.24125729\n",
      " 1.24125729 0.56747259 1.24125729 0.56747259 1.24125729 1.24125729\n",
      " 1.24125729 0.56747259 1.24125729 0.56747259 1.24125729 1.72404054\n",
      " 1.24125729 1.24125729 0.56747259 0.56747259 0.56747259 1.24125729\n",
      " 1.24125729 0.56747259 0.56747259 0.56747259]\n",
      "all_sum is: [0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  1.21171641 0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 1.21171641 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302  0.7625302  0.7625302\n",
      " 0.7625302  0.7625302  0.7625302  0.7625302 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.436\n",
      "(*) epoch 2, cost 1.964\n",
      "(*) epoch 3, cost 1.810\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.737\n",
      "(*) epoch 2, cost 2.366\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.55456262 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.55456262 4.06772503 4.06772503 4.06772503 4.55456262 4.55456262\n",
      " 4.06772503 4.06772503 4.55456262 4.55456262 4.06772503 4.55456262\n",
      " 4.55456262 4.06772503 4.06772503 4.06772503 4.06772503 4.55456262\n",
      " 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.55456262 4.06772503 4.06772503 4.55456262\n",
      " 4.06772503 4.55456262 4.06772503 4.55456262 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.55456262 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.55456262 4.06772503 4.06772503 4.06772503\n",
      " 4.55456262 4.06772503 4.06772503 4.06772503 4.06772503 4.55456262\n",
      " 4.06772503 4.55456262 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.06772503 4.06772503 4.55456262 4.55456262\n",
      " 4.06772503 4.55456262 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503 4.06772503\n",
      " 4.06772503 4.06772503 4.06772503 4.55456262 4.06772503 4.06772503\n",
      " 4.55456262 4.06772503 4.06772503 4.55456262]\n",
      "all_sum is: [2.0410716  2.0410716  2.73743314 2.0410716  2.0410716  2.07720716\n",
      " 2.07720716 1.38760919 2.0410716  1.38760919 2.0410716  2.0410716\n",
      " 1.69461806 2.0410716  2.07720716 2.07720716 2.0410716  2.0410716\n",
      " 2.0410716  2.0410716  2.07720716 2.0410716  2.07720716 2.0410716\n",
      " 2.07720716 2.0410716  2.0410716  1.38760919 2.0410716  2.0410716\n",
      " 2.0410716  1.38760919 2.0410716  1.42374475 2.07720716 2.07720716\n",
      " 1.38760919 2.0410716  2.0410716  2.0410716  2.0410716  2.07720716\n",
      " 2.07720716 2.07720716 2.07720716 2.0410716  1.42374475 2.0410716\n",
      " 2.0410716  2.07720716 2.0410716  2.07720716 2.0410716  2.73743314\n",
      " 2.0410716  2.07720716 2.07720716 2.70129758 2.0410716  2.0410716\n",
      " 2.0410716  2.0410716  2.73743314 2.0410716  2.07720716 2.0410716\n",
      " 1.38760919 2.0410716  2.0410716  1.42374475 1.38760919 1.42374475\n",
      " 2.07720716 2.0410716  2.0410716  2.0410716  2.70129758 2.0410716\n",
      " 2.0410716  1.38760919 2.0410716  2.0410716  2.0410716  1.42374475\n",
      " 1.38760919 2.0410716  2.0410716  2.0410716  2.0410716  2.07720716\n",
      " 2.70129758 2.07720716 1.38760919 1.38760919 2.07720716 2.0410716\n",
      " 2.0410716  2.07720716 2.04783518 2.0410716 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.750\n",
      "(*) epoch 2, cost 1.553\n",
      "(*) epoch 3, cost 1.321\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.577\n",
      "(*) epoch 2, cost 2.961\n",
      "(*) epoch 3, cost 2.562\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 0.90514224\n",
      " 0.90514224 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 0.90514224 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 0.90514224 2.15273711 2.15273711 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 0.90514224 2.15273711 2.15273711\n",
      " 2.15273711 2.15273711 2.15273711 2.15273711]\n",
      "all_sum is: [2.61255301 1.26320245 1.26320245 2.61255301 1.26320245 2.60705849\n",
      " 1.26320245 1.26320245 1.26320245 1.26320245 2.61255301 1.26320245\n",
      " 1.26320245 2.61255301 1.26320245 2.61255301 1.26320245 1.26320245\n",
      " 1.26320245 2.61255301 1.26320245 1.26320245 1.26320245 2.61255301\n",
      " 1.26320245 1.26320245 2.61255301 1.26320245 1.26320245 1.26320245\n",
      " 1.26320245 1.26320245 1.26320245 1.26320245 2.61255301 2.61255301\n",
      " 1.26320245 1.26320245 1.26320245 1.26320245 1.26320245 2.61255301\n",
      " 2.02793747 2.61255301 1.26320245 2.02793747 1.26320245 1.26320245\n",
      " 1.26320245 2.02793747 2.61255301 1.26320245 1.26320245 2.61255301\n",
      " 2.02793747 1.26320245 2.02793747 1.26320245 1.26320245 1.26320245\n",
      " 2.61255301 1.26320245 2.61255301 1.26320245 1.26320245 1.26320245\n",
      " 1.26320245 2.61255301 2.61255301 1.26320245 1.26320245 1.26320245\n",
      " 1.26320245 2.61255301 1.26320245 1.26320245 1.26320245 1.26320245\n",
      " 1.26320245 1.26320245 1.26320245 2.02793747 2.61255301 2.02793747\n",
      " 2.61255301 1.26320245 1.26320245 2.61255301 2.61255301 1.26320245\n",
      " 1.26320245 2.61255301 1.26320245 2.02793747 1.26320245 2.61255301\n",
      " 2.61255301 2.61255301 1.26320245 1.26320245]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.738\n",
      "(*) epoch 2, cost 2.249\n",
      "(*) epoch 3, cost 1.800\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.241\n",
      "(*) epoch 2, cost 2.465\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.65655199\n",
      " 1.06774968 1.06774968 1.06774968 1.08811533 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.65655199 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968 1.06774968\n",
      " 1.06774968 1.06774968 1.06774968 1.06774968]\n",
      "all_sum is: [0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052 0.22726052\n",
      " 0.22726052 0.22726052 0.22726052 0.22726052]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.721\n",
      "(*) epoch 2, cost 0.687\n",
      "(*) epoch 3, cost 0.439\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.926\n",
      "(*) epoch 2, cost 1.170\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.60517532 4.45787177 2.81810575 3.60517532 4.45787177 4.45787177\n",
      " 3.60517532 3.60517532 4.45787177 4.45787177 2.81810575 3.60517532\n",
      " 3.60517532 3.60517532 3.60517532 3.60517532 4.45787177 3.60517532\n",
      " 4.45787177 4.45787177 3.60517532 3.67080219 3.60517532 4.45787177\n",
      " 4.45787177 3.60517532 3.60517532 4.45787177 4.45787177 3.60517532\n",
      " 4.45787177 3.60517532 3.60517532 3.60517532 4.45787177 3.60517532\n",
      " 3.60517532 3.60517532 3.60517532 4.45787177 4.45787177 3.60517532\n",
      " 3.60517532 3.60517532 4.45787177 3.60517532 4.45787177 3.60517532\n",
      " 4.45787177 4.45787177 3.60517532 4.45787177 3.60517532 4.45787177\n",
      " 4.45787177 4.45787177 3.60517532 4.45787177 3.60517532 4.45787177\n",
      " 4.45787177 3.60517532 3.60517532 3.60517532 4.45787177 3.60517532\n",
      " 4.45787177 3.60517532 3.60517532 4.45787177 4.45787177 4.45787177\n",
      " 3.60517532 4.45787177 3.60517532 4.45787177 3.60517532 4.45787177\n",
      " 3.60517532 3.60517532 3.60517532 4.45787177 3.60517532 3.60517532\n",
      " 3.60517532 4.45787177 4.45787177 3.60517532 3.60517532 4.45787177\n",
      " 3.60517532 4.45787177 4.45787177 3.60517532 4.45787177 3.60517532\n",
      " 4.45787177 3.60517532 4.45787177 3.60517532]\n",
      "all_sum is: [0.6986322  0.6986322  0.6986322  0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  2.26750999 0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  0.6986322  2.26750999 0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  2.26750999 2.26750999 2.26750999\n",
      " 0.6986322  0.6986322  2.26750999 0.6986322  0.6986322  0.6986322\n",
      " 0.9563849  2.26750999 0.6986322  0.6986322  0.6986322  2.26750999\n",
      " 0.6986322  2.26750999 0.6986322  0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  2.26750999 0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  2.26750999 2.26750999 0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  2.26750999 0.6986322  0.6986322\n",
      " 0.6986322  2.26750999 0.6986322  0.6986322  0.6986322  2.26750999\n",
      " 2.26750999 0.6986322  0.6986322  0.6986322  0.6986322  2.26750999\n",
      " 0.6986322  0.6986322  0.6986322  2.26750999 0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  0.6986322  0.6986322  2.26750999\n",
      " 0.6986322  0.6986322  2.26750999 0.6986322  0.6986322  0.6986322\n",
      " 0.6986322  0.6986322  0.6986322  0.6986322 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.841\n",
      "(*) epoch 2, cost 2.190\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.251\n",
      "(*) epoch 2, cost 0.998\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.         0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.90333625 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.         0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.90333625\n",
      " 0.38591433 0.38591433 0.38591433 0.90333625 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.90333625\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.90333625 0.38591433 0.38591433 0.38591433 0.38591433 0.38591433\n",
      " 0.38591433 0.38591433 0.38591433 0.38591433]\n",
      "all_sum is: [3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 4.32350684\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 4.32350684 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 4.32350684 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551 3.47043551\n",
      " 3.47043551 3.47043551 3.47043551 3.47043551]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.255\n",
      "(*) epoch 2, cost 2.128\n",
      "(*) epoch 3, cost 1.819\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.526\n",
      "(*) epoch 2, cost 2.096\n",
      "(*) epoch 3, cost 1.743\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.14103199 1.8175141  1.10642506 3.65516718 1.97868506 0.14103199\n",
      " 1.97868506 1.97868506 3.65516718 2.94407813 1.97868506 0.14103199\n",
      " 3.65516718 1.10642506 2.94407813 0.14103199 1.97868506 0.14103199\n",
      " 1.97868506 1.10642506 1.10642506 2.94407813 2.94407813 2.94407813\n",
      " 1.10642506 1.97868506 2.94407813 2.94407813 1.8175141  0.14103199\n",
      " 0.14103199 3.65516718 1.97868506 1.10642506 2.94407813 1.97868506\n",
      " 1.10642506 1.97868506 0.14103199 2.94407813 1.10642506 1.97868506\n",
      " 1.10642506 2.94407813 1.10642506 2.94407813 0.14103199 2.94407813\n",
      " 0.14103199 1.97868506 1.97868506 1.10642506 0.85212103 1.97868506\n",
      " 2.6897741  3.65516718 1.97868506 1.97868506 1.10642506 1.10642506\n",
      " 2.6897741  2.94407813 1.10642506 1.10642506 0.85212103 0.14103199\n",
      " 1.97868506 0.14103199 2.6897741  1.97868506 1.97868506 1.10642506\n",
      " 1.97868506 2.94407813 2.94407813 2.94407813 1.97868506 1.10642506\n",
      " 1.10642506 1.97868506 1.97868506 2.6897741  2.94407813 1.10642506\n",
      " 2.94407813 2.94407813 2.94407813 0.14103199 0.14103199 2.94407813\n",
      " 0.14103199 1.97868506 1.10642506 1.97868506 1.8175141  0.14103199\n",
      " 2.94407813 0.85212103 1.97868506 2.94407813]\n",
      "all_sum is: [5.52632044 3.87218485 4.30418181 2.59491193 3.87218485 5.83691639\n",
      " 3.87218485 5.31062993 5.31062993 3.87218485 3.02690889 5.52632044\n",
      " 3.87218485 3.87218485 5.52632044 4.03335701 5.31062993 5.31062993\n",
      " 4.03335701 3.87218485 5.31062993 2.59491193 4.30418181 3.87218485\n",
      " 5.31062993 5.52632044 3.87218485 7.39676248 5.52632044 5.9583174\n",
      " 5.74262689 4.68104448 6.96476552 5.9583174  5.31062993 5.31062993\n",
      " 5.52632044 6.96476552 6.96476552 5.31062993 4.46535397 3.87218485\n",
      " 5.31062993 3.87218485 5.52632044 3.87218485 3.87218485 4.30418181\n",
      " 6.96476552 4.30418181 3.87218485 5.31062993 3.87218485 4.30418181\n",
      " 4.30418181 4.24904752 3.87218485 6.96476552 5.31062993 4.30418181\n",
      " 6.96476552 5.6874926  6.96476552 3.87218485 5.31062993 2.59491193\n",
      " 5.52632044 5.31062993 4.03335701 5.9583174  5.31062993 5.52632044\n",
      " 4.68104448 4.24904752 3.87218485 5.31062993 3.87218485 5.74262689\n",
      " 3.87218485 4.46535397 3.87218485 3.87218485 3.87218485 5.52632044\n",
      " 3.87218485 5.9583174  5.31062993 3.87218485 5.31062993 3.87218485\n",
      " 4.24904752 2.59491193 3.87218485 5.31062993 5.74262689 5.74262689\n",
      " 5.52632044 3.87218485 3.87218485 4.30418181]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.559\n",
      "(*) epoch 2, cost 2.816\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.237\n",
      "(*) epoch 2, cost 3.583\n",
      "(*) epoch 3, cost 3.008\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 2.25503596 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388 1.67100388\n",
      " 1.67100388 1.67100388 1.67100388 1.67100388]\n",
      "all_sum is: [1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801 1.57856801\n",
      " 1.57856801 1.57856801 1.57856801 1.57856801]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.471\n",
      "(*) epoch 2, cost 1.636\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.022\n",
      "(*) epoch 2, cost 0.744\n",
      "(*) epoch 3, cost 0.438\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.83623802 2.73953001 0.83623802 2.73953001 3.40921484 2.73953001\n",
      " 0.83623802 0.83623802 0.1665532  3.40921484 0.83623802 0.83623802\n",
      " 0.83623802 2.73953001 3.40921484 3.40921484 0.83623802 0.83623802\n",
      " 0.83623802 3.40921484 0.83623802 2.73953001 0.1665532  2.73953001\n",
      " 3.40921484 0.83623802 3.40921484 3.40921484 0.83623802 0.83623802\n",
      " 0.83623802 0.1665532  0.83623802 0.83623802 0.83623802 0.83623802\n",
      " 2.73953001 0.1665532  0.83623802 2.73953001 2.73953001 0.83623802\n",
      " 0.83623802 0.83623802 0.1665532  0.1665532  2.73953001 2.73953001\n",
      " 0.83623802 0.83623802 0.1665532  3.40921484 3.40921484 0.83623802\n",
      " 3.40921484 3.40921484 3.40921484 2.73953001 2.73953001 0.83623802\n",
      " 0.83623802 0.83623802 3.40921484 0.83623802 2.73953001 0.1665532\n",
      " 0.83623802 0.1665532  3.40921484 3.40921484 0.1665532  0.83623802\n",
      " 0.83623802 3.40921484 2.73953001 0.83623802 0.83623802 0.83623802\n",
      " 0.83623802 3.40921484 3.40921484 3.40921484 0.83623802 0.1665532\n",
      " 3.40921484 2.73953001 2.73953001 3.40921484 3.40921484 3.40921484\n",
      " 0.83623802 2.73953001 2.73953001 3.40921484 0.1665532  3.40921484\n",
      " 0.83623802 0.1665532  3.40921484 0.83623802]\n",
      "all_sum is: [0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.77844576 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462 0.53921462\n",
      " 0.53921462 0.53921462 0.53921462 0.53921462]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.988\n",
      "(*) epoch 2, cost 1.878\n",
      "(*) epoch 3, cost 1.548\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.808\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.32792468 2.32792468 3.19155733 2.32792468 2.32792468 2.32792468\n",
      " 2.67779679 2.32792468 2.67779679 2.32792468 2.32792468 2.32792468\n",
      " 2.67779679 2.32792468 2.32792468 2.67779679 2.32792468 2.67779679\n",
      " 2.84168522 2.32792468 2.67779679 2.32792468 2.32792468 2.67779679\n",
      " 2.32792468 2.32792468 2.32792468 2.67779679 2.32792468 2.32792468\n",
      " 2.67779679 2.32792468 2.32792468 2.32792468 2.67779679 2.32792468\n",
      " 2.32792468 2.32792468 2.67779679 2.32792468 2.32792468 2.32792468\n",
      " 2.32792468 2.32792468 2.32792468 2.32792468 2.32792468 2.32792468\n",
      " 2.32792468 2.32792468 2.67779679 2.67779679 2.32792468 2.67779679\n",
      " 2.67779679 2.32792468 2.32792468 2.32792468 2.32792468 2.32792468\n",
      " 2.32792468 2.32792468 2.32792468 2.32792468 2.32792468 2.32792468\n",
      " 2.32792468 2.32792468 2.67779679 2.32792468 2.32792468 2.32792468\n",
      " 2.67779679 2.67779679 2.84168522 2.32792468 2.67779679 2.32792468\n",
      " 2.67779679 2.67779679 2.67779679 2.32792468 2.67779679 2.67779679\n",
      " 2.32792468 2.67779679 2.32792468 2.67779679 2.67779679 2.32792468\n",
      " 2.32792468 2.32792468 2.32792468 2.67779679 2.67779679 2.32792468\n",
      " 2.67779679 2.32792468 2.32792468 2.32792468]\n",
      "all_sum is: [3.22324583 3.22324583 3.22324583 3.33063466 3.33063466 3.33063466\n",
      " 4.40957749 3.22324583 3.22324583 4.22508744 3.33063466 3.33063466\n",
      " 3.22324583 3.22324583 3.33063466 3.22324583 3.33063466 3.33063466\n",
      " 3.22324583 3.14614461 3.33063466 3.14614461 3.33063466 4.22508744\n",
      " 3.33063466 3.33063466 3.33063466 3.22324583 3.22324583 3.22324583\n",
      " 3.33063466 3.22324583 3.33063466 3.33063466 3.33063466 3.33063466\n",
      " 3.22324583 3.33063466 3.22324583 3.33063466 3.33063466 3.33063466\n",
      " 3.33063466 3.33063466 3.22324583 3.33063466 3.33063466 3.33063466\n",
      " 3.22324583 1.4696588  3.33063466 3.33063466 3.22324583 4.40957749\n",
      " 1.57704762 3.33063466 3.22324583 3.22324583 3.03875578 3.33063466\n",
      " 3.14614461 3.33063466 3.22324583 3.22324583 3.33063466 3.33063466\n",
      " 3.33063466 3.33063466 3.22324583 3.14614461 3.33063466 3.33063466\n",
      " 3.33063466 3.33063466 3.33063466 3.22324583 3.22324583 3.33063466\n",
      " 3.14614461 3.33063466 3.22324583 3.33063466 3.22324583 1.57704762\n",
      " 3.22324583 3.33063466 3.33063466 3.33063466 3.33063466 3.14614461\n",
      " 3.33063466 3.33063466 3.33063466 3.33063466 3.22324583 3.33063466\n",
      " 3.22324583 3.33063466 3.14614461 4.30218867]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.878\n",
      "(*) epoch 2, cost 1.844\n",
      "(*) epoch 3, cost 1.563\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.052\n",
      "(*) epoch 2, cost 3.239\n",
      "(*) epoch 3, cost 2.775\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.49035001 0.43934819 0.43934819 0.43934819 0.49035001 0.43934819\n",
      " 0.43934819 0.43934819 0.49035001 0.43934819 0.43934819 0.43934819\n",
      " 0.49035001 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.49035001 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.49035001 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.49035001 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.49035001\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.49035001\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.49035001 0.43934819 0.43934819 0.43934819\n",
      " 0.49035001 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.49035001 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 1.10355171 0.43934819 0.43934819 0.43934819 0.43934819 0.43934819\n",
      " 0.43934819 0.43934819 0.43934819 0.43934819]\n",
      "all_sum is: [2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.1284391  2.99968915\n",
      " 2.1284391  2.99968915 2.1284391  2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.1284391  2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.1284391  2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.1284391  2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.1284391  2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.1284391  2.1284391\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.1284391  2.1284391  2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.1284391  2.1284391  2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915 2.99968915\n",
      " 2.99968915 2.99968915 2.99968915 2.99968915]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.655\n",
      "(*) epoch 2, cost 1.147\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.289\n",
      "(*) epoch 2, cost 2.108\n",
      "(*) epoch 3, cost 1.794\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278 3.76725278\n",
      " 3.76725278 3.76725278 3.76725278 3.76725278]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sum is: [0.78494449 4.4988713  3.24020744 3.24020744 3.24020744 3.24020744\n",
      " 3.24020744 2.29338633 2.29338633 2.29338633 3.24020744 2.29338633\n",
      " 3.24020744 1.73176559 1.73176559 2.29338633 2.29338633 3.24020744\n",
      " 2.29338633 2.29338633 2.29338633 3.24020744 3.24020744 3.24020744\n",
      " 4.4988713  1.73176559 2.29338633 0.28794686 2.29338633 3.24020744\n",
      " 3.24020744 3.24020744 3.24020744 0.78494449 3.24020744 1.73176559\n",
      " 2.29338633 3.24020744 2.29338633 1.7963887  3.24020744 1.73176559\n",
      " 2.29338633 3.24020744 3.24020744 2.29338633 1.73176559 3.24020744\n",
      " 0.78494449 2.29338633 3.24020744 3.24020744 1.73176559 3.24020744\n",
      " 3.24020744 3.24020744 1.7963887  1.73176559 2.29338633 3.24020744\n",
      " 1.73176559 3.24020744 2.29338633 3.24020744 0.78494449 0.78494449\n",
      " 0.78494449 3.24020744 1.73176559 3.24020744 3.24020744 2.29338633\n",
      " 2.74320981 2.29338633 0.78494449 3.24020744 2.29338633 3.24020744\n",
      " 3.24020744 2.29338633 3.24020744 2.29338633 0.78494449 2.29338633\n",
      " 3.24020744 3.24020744 2.29338633 1.73176559 2.29338633 1.7963887\n",
      " 2.29338633 1.23476796 3.24020744 3.24020744 2.29338633 2.29338633\n",
      " 3.24020744 3.24020744 2.29338633 1.73176559]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.413\n",
      "(*) epoch 2, cost 1.084\n",
      "(*) epoch 3, cost 0.819\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.655\n",
      "(*) epoch 2, cost 4.303\n",
      "(*) epoch 3, cost 3.991\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.59143403 0.59143403 0.59143403 0.59143403 0.59143403 1.22219284\n",
      " 0.59143403 1.22219284 0.59143403 0.59143403 0.59143403 0.59143403\n",
      " 1.64915315 0.59143403 0.59143403 0.59143403 0.         0.59143403\n",
      " 0.         0.59143403 0.59143403 0.59143403 0.         0.59143403\n",
      " 0.59143403 0.59143403 0.59143403 0.59143403 1.22219284 0.59143403\n",
      " 0.59143403 0.59143403 0.59143403 0.59143403 0.59143403 0.\n",
      " 0.59143403 1.22219284 0.59143403 0.59143403 0.59143403 0.59143403\n",
      " 0.59143403 0.59143403 0.59143403 0.         0.59143403 0.59143403\n",
      " 0.         1.22219284 0.59143403 0.59143403 0.59143403 0.59143403\n",
      " 1.22219284 0.59143403 0.59143403 0.59143403 1.22219284 0.59143403\n",
      " 0.59143403 0.59143403 0.         0.59143403 0.59143403 0.\n",
      " 0.59143403 0.59143403 0.59143403 0.59143403 0.         0.\n",
      " 0.59143403 0.59143403 0.59143403 0.59143403 1.22219284 0.59143403\n",
      " 0.59143403 0.59143403 0.59143403 0.59143403 0.59143403 0.59143403\n",
      " 0.59143403 0.59143403 0.59143403 1.22219284 1.22219284 0.59143403\n",
      " 1.22219284 0.59143403 0.59143403 0.59143403 0.59143403 0.59143403\n",
      " 0.59143403 0.59143403 0.         0.59143403]\n",
      "all_sum is: [5.89272607 4.76455454 4.76455454 4.76455454 4.63273454 3.50456302\n",
      " 4.76455454 5.89272607 5.128826   3.50456302 4.63273454 5.89272607\n",
      " 5.128826   5.128826   4.63273454 4.76455454 4.76455454 5.89272607\n",
      " 4.76455454 3.86883447 4.63273454 3.50456302 3.50456302 5.89272607\n",
      " 5.89272607 4.63273454 4.76455454 5.89272607 5.89272607 5.89272607\n",
      " 4.63273454 3.50456302 4.63273454 6.25699752 4.76455454 4.63273454\n",
      " 4.63273454 5.89272607 4.76455454 4.63273454 4.76455454 4.63273454\n",
      " 4.63273454 5.89272607 4.76455454 5.89272607 3.50456302 4.76455454\n",
      " 5.89272607 4.63273454 4.76455454 5.89272607 4.76455454 4.63273454\n",
      " 4.63273454 4.63273454 4.76455454 4.63273454 4.63273454 3.50456302\n",
      " 4.997006   4.76455454 5.89272607 5.89272607 4.63273454 4.76455454\n",
      " 4.76455454 4.63273454 5.34041997 4.63273454 4.63273454 4.76455454\n",
      " 4.63273454 5.89272607 4.63273454 4.63273454 4.76455454 5.89272607\n",
      " 5.89272607 4.63273454 4.63273454 4.76455454 6.25699752 3.50456302\n",
      " 5.89272607 5.128826   5.89272607 4.63273454 3.50456302 4.76455454\n",
      " 3.50456302 5.89272607 5.89272607 4.76455454 3.50456302 4.63273454\n",
      " 4.76455454 5.89272607 4.63273454 5.89272607]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.913\n",
      "(*) epoch 2, cost 2.369\n",
      "(*) epoch 3, cost 2.178\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.989\n",
      "(*) epoch 2, cost 2.528\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.26579176 1.26837185 1.26579176 1.26837185 1.26579176 1.26837185\n",
      " 1.26837185 1.26579176 1.26837185 1.26579176 1.26837185 1.26837185\n",
      " 1.26579176 1.26837185 1.26579176 1.26837185 1.26579176 1.26579176\n",
      " 1.26579176 2.91261652 1.72799146 1.26579176 1.26579176 1.26579176\n",
      " 1.26579176 1.26579176 1.26579176 1.26579176 1.72541137 1.26579176\n",
      " 1.28356008 1.26579176 1.26579176 1.28356008 1.26579176 2.91261652\n",
      " 1.28356008 1.26837185 1.26579176 2.91003643 1.26579176 1.26579176\n",
      " 1.26837185 1.26579176 1.26579176 1.26579176 1.26579176 1.26837185\n",
      " 1.28614017 1.26579176 1.26579176 1.26837185 1.26579176 1.28356008\n",
      " 1.74317969 1.26579176 1.26579176 1.26579176 1.26579176 1.26579176\n",
      " 1.26579176 1.28614017 1.26837185 1.26579176 1.26579176 2.91003643\n",
      " 1.26579176 1.26579176 1.26579176 1.26579176 1.26579176 1.26579176\n",
      " 2.93038484 1.26579176 1.26579176 1.26579176 2.91261652 1.26579176\n",
      " 1.26579176 1.26579176 1.26579176 1.26579176 1.28356008 1.26579176\n",
      " 1.26579176 1.26579176 1.26837185 1.28356008 2.91003643 1.26579176\n",
      " 1.26579176 1.72541137 1.26579176 1.28356008 1.26837185 1.26837185\n",
      " 1.26579176 1.26579176 1.28356008 1.26579176]\n",
      "all_sum is: [0.95866731 0.95866731 0.95866731 0.95866731 0.95866731 1.51342906\n",
      " 0.95866731 0.95866731 0.95866731 0.95866731 1.51342906 0.95866731\n",
      " 0.95866731 0.95866731 1.51342906 0.95866731 1.51342906 0.95866731\n",
      " 1.51342906 0.         2.1636411  2.1636411  0.95866731 2.1636411\n",
      " 0.95866731 2.71840285 0.95866731 1.51342906 0.         0.95866731\n",
      " 0.95866731 2.1636411  2.71840285 0.95866731 0.95866731 1.51342906\n",
      " 0.95866731 1.20497379 0.95866731 0.95866731 0.95866731 2.1636411\n",
      " 0.95866731 1.51342906 1.51342906 1.64581795 2.1636411  0.95866731\n",
      " 0.95866731 0.         0.95866731 1.20497379 2.71840285 0.95866731\n",
      " 0.         1.51342906 0.95866731 0.95866731 2.1636411  0.95866731\n",
      " 2.71840285 0.95866731 2.71840285 2.85079174 1.51342906 1.51342906\n",
      " 0.95866731 1.51342906 0.95866731 0.95866731 1.51342906 2.1636411\n",
      " 0.95866731 1.51342906 2.1636411  0.95866731 0.95866731 2.1636411\n",
      " 2.2005797  0.95866731 0.95866731 0.95866731 0.95866731 0.95866731\n",
      " 2.1636411  0.95866731 0.95866731 0.55476175 0.95866731 2.1636411\n",
      " 1.51342906 1.51342906 0.95866731 0.95866731 0.95866731 2.1636411\n",
      " 0.95866731 1.51342906 0.95866731 0.95866731]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.679\n",
      "(*) epoch 2, cost 2.226\n",
      "(*) epoch 3, cost 1.915\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.091\n",
      "(*) epoch 2, cost 3.218\n",
      "(*) epoch 3, cost 2.952\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391 2.10315391\n",
      " 2.10315391 2.10315391 2.10315391 2.10315391]\n",
      "all_sum is: [1.00770064 1.00770064 1.00770064 1.00770064 1.00770064 1.46637075\n",
      " 1.46637075 1.00770064 1.00770064 1.46637075 1.00770064 1.46637075\n",
      " 1.46637075 1.00770064 1.00770064 1.00770064 1.81695983 1.00770064\n",
      " 1.46637075 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064\n",
      " 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064 1.00770064\n",
      " 1.46637075 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064\n",
      " 1.00770064 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064\n",
      " 1.00770064 1.46637075 1.46637075 1.00770064 1.00770064 1.00770064\n",
      " 1.00770064 1.00770064 1.46637075 1.00770064 1.46637075 1.00770064\n",
      " 1.00770064 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064\n",
      " 1.00770064 1.00770064 1.46637075 1.46637075 1.00770064 1.00770064\n",
      " 1.00770064 1.46637075 1.00770064 1.46637075 1.00770064 1.46637075\n",
      " 1.00770064 1.00770064 1.46637075 1.00770064 1.00770064 1.46637075\n",
      " 1.46637075 1.00770064 1.00770064 1.00770064 1.00770064 1.00770064\n",
      " 1.46637075 1.00770064 1.00770064 1.00770064 1.46637075 1.46637075\n",
      " 1.00770064 1.00770064 1.00770064 1.00770064 1.00770064 1.00770064\n",
      " 1.00770064 1.00770064 1.46637075 1.00770064]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.760\n",
      "(*) epoch 2, cost 1.430\n",
      "(*) epoch 3, cost 1.188\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.929\n",
      "(*) epoch 2, cost 1.226\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.35419469 3.10593493 2.35419469 3.10593493 3.10593493 3.10593493\n",
      " 2.35419469 3.10593493 2.35419469 2.35419469 2.35419469 3.10593493\n",
      " 2.35419469 2.35419469 2.35419469 2.35419469 3.10593493 2.35419469\n",
      " 2.35419469 3.10593493 3.10593493 2.35419469 2.35419469 2.35419469\n",
      " 2.35419469 2.35419469 3.10593493 2.35419469 3.10593493 2.35419469\n",
      " 2.35419469 2.35419469 3.10593493 2.35419469 3.10593493 3.10593493\n",
      " 2.35419469 3.10593493 2.35419469 2.35419469 3.10593493 3.10593493\n",
      " 3.10593493 2.35419469 2.35419469 3.10593493 2.35419469 2.35419469\n",
      " 3.10593493 3.10593493 2.35419469 3.10593493 3.10593493 2.35419469\n",
      " 2.35419469 2.35419469 2.35419469 3.10593493 3.10593493 2.35419469\n",
      " 2.35419469 3.10593493 2.35419469 2.35419469 2.35419469 3.10593493\n",
      " 2.35419469 2.35419469 2.35419469 2.35419469 3.10593493 2.35419469\n",
      " 3.10593493 3.10593493 3.10593493 2.35419469 3.10593493 2.35419469\n",
      " 3.10593493 3.10593493 2.35419469 3.10593493 2.35419469 3.10593493\n",
      " 2.35419469 3.10593493 2.35419469 3.10593493 2.35419469 3.10593493\n",
      " 3.10593493 2.35419469 2.35419469 3.10593493 3.10593493 3.10593493\n",
      " 3.10593493 2.35419469 3.10593493 3.10593493]\n",
      "all_sum is: [0.         1.57231583 2.18352175 2.18352175 1.57231583 1.22418011\n",
      " 0.61297419 0.61297419 2.18352175 1.57054757 0.61297419 1.20856378\n",
      " 2.43274389 0.61297419 1.57231583 1.57054757 2.18352175 1.57231583\n",
      " 2.18352175 0.61297419 2.18352175 0.61297419 1.57054757 1.57231583\n",
      " 1.57231583 1.22418011 1.22418011 1.57231583 1.57231583 2.18352175\n",
      " 1.57231583 0.61297419 2.43274389 2.43274389 0.61297419 2.18352175\n",
      " 1.57054757 0.86219633 2.18352175 0.86042806 1.57231583 0.61297419\n",
      " 2.18352175 1.82153796 0.61297419 0.61297419 0.61297419 2.43274389\n",
      " 0.61297419 2.18352175 0.95934164 2.18352175 1.22418011 1.22418011\n",
      " 1.57231583 1.57231583 1.57231583 1.82153796 1.57231583 1.47340225\n",
      " 1.22418011 0.61297419 1.82153796 1.57231583 1.22418011 0.61297419\n",
      " 1.22418011 2.43274389 0.86219633 0.61297419 2.18352175 0.86219633\n",
      " 2.18352175 1.57231583 0.61297419 1.82153796 2.18352175 2.18352175\n",
      " 0.         1.57231583 1.57231583 1.8197697  2.43274389 1.57231583\n",
      " 0.61297419 0.61297419 0.61297419 1.57054757 0.95934164 0.\n",
      " 1.47340225 1.57231583 1.22418011 1.22418011 1.57231583 1.57231583\n",
      " 1.57231583 1.22418011 1.57231583 2.18352175]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.128\n",
      "(*) epoch 2, cost 1.155\n",
      "(*) epoch 3, cost 0.793\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.948\n",
      "(*) epoch 2, cost 4.194\n",
      "(*) epoch 3, cost 3.977\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.04947436 2.43093242 2.43093242 2.43093242 2.43093242 1.04947436\n",
      " 2.43093242 2.43093242 1.04947436 2.43093242 1.04947436 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 1.04947436 2.43093242 2.43093242\n",
      " 1.04947436 1.04947436 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 1.04947436 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 1.04947436 2.43093242 2.43093242\n",
      " 2.43093242 1.04947436 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 1.04947436 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 1.04947436 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 1.04947436 1.04947436 2.43093242 2.43093242 1.04947436 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 2.43093242 1.04947436 2.43093242 1.04947436 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242 2.43093242\n",
      " 2.43093242 2.43093242 2.43093242 1.04947436 2.43093242 2.43093242\n",
      " 1.04947436 2.43093242 1.04947436 2.43093242]\n",
      "all_sum is: [0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874 0.50399874\n",
      " 0.50399874 0.50399874 0.50399874 0.50399874]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.863\n",
      "(*) epoch 2, cost 2.094\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.080\n",
      "(*) epoch 2, cost 1.809\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.85201318 0.85201318 0.85201318 2.59502052 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 2.59502052 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 2.59502052 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318 0.85201318\n",
      " 0.85201318 0.85201318 0.85201318 0.85201318]\n",
      "all_sum is: [0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 1.54306148\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 1.54306148 0.60678992 0.60678992 0.60678992 0.60678992 1.57981284\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 1.57981284 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 1.71074658 1.54306148 1.54306148 0.77447501\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 1.54306148 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 0.77447501 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.77447501 0.92341748 1.54306148 0.60678992 0.60678992\n",
      " 1.54306148 0.77447501 1.54306148 1.54306148 0.60678992 0.77447501\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 1.57981284 0.60678992 0.60678992 0.60678992 0.60678992\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992 0.60678992 0.77447501\n",
      " 0.60678992 0.60678992 0.60678992 0.60678992]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.276\n",
      "(*) epoch 2, cost 0.731\n",
      "(*) epoch 3, cost 0.385\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.064\n",
      "(*) epoch 2, cost 2.035\n",
      "(*) epoch 3, cost 1.980\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.21130613 1.28829709 0.6446837  0.6446837  0.6446837  0.6446837\n",
      " 0.6446837  0.6446837  1.28829709 0.6446837  0.6446837  1.28829709\n",
      " 0.6446837  0.6446837  0.6446837  1.21130613 1.28829709 0.6446837\n",
      " 1.28829709 0.6446837  0.6446837  0.6446837  0.6446837  0.6446837\n",
      " 0.6446837  0.6446837  0.6446837  0.6446837  1.28829709 1.28829709\n",
      " 0.6446837  0.6446837  1.28829709 0.6446837  1.28829709 0.6446837\n",
      " 0.6446837  0.6446837  1.33016469 1.28829709 0.6446837  0.6446837\n",
      " 0.6446837  0.6446837  1.28829709 1.28829709 1.28829709 1.28829709\n",
      " 1.28829709 0.6446837  0.6446837  1.28829709 0.6446837  0.6446837\n",
      " 0.6446837  0.6446837  0.6446837  1.28829709 0.6446837  1.21130613\n",
      " 1.28829709 0.6446837  0.6446837  0.6446837  0.6446837  0.6446837\n",
      " 0.6446837  1.28829709 1.28829709 1.28829709 1.28829709 0.6446837\n",
      " 0.6446837  1.28829709 1.28829709 0.6446837  0.6446837  1.28829709\n",
      " 0.6446837  0.6446837  1.28829709 0.6446837  1.28829709 0.6446837\n",
      " 1.28829709 0.6446837  1.28829709 1.28829709 0.6446837  1.28829709\n",
      " 0.6446837  0.6446837  0.6446837  1.28829709 0.6446837  1.28829709\n",
      " 0.6446837  0.6446837  1.28829709 1.28829709]\n",
      "all_sum is: [3.05223276 1.18348283 2.70854636 4.13841569 4.5772963  4.5772963\n",
      " 4.5772963  2.70854636 3.79472929 2.70854636 4.5772963  4.5772963\n",
      " 3.05223276 3.05223276 4.5772963  3.05223276 4.5772963  1.18348283\n",
      " 1.18348283 3.79472929 3.05223276 4.5772963  4.5772963  4.13841569\n",
      " 2.70854636 4.5772963  3.05223276 2.70854636 3.05223276 5.66347923\n",
      " 4.5772963  3.05223276 3.05223276 4.5772963  3.05223276 3.05223276\n",
      " 3.05223276 4.5772963  4.13841569 4.5772963  3.05223276 4.5772963\n",
      " 3.05223276 1.18348283 3.79472929 4.5772963  2.70854636 4.5772963\n",
      " 4.5772963  4.5772963  1.18348283 5.66347923 4.5772963  4.5772963\n",
      " 4.5772963  2.70854636 4.5772963  4.5772963  5.66347923 2.70854636\n",
      " 2.70854636 2.70854636 4.5772963  5.66347923 4.5772963  4.5772963\n",
      " 4.5772963  2.70854636 1.18348283 4.5772963  4.5772963  4.5772963\n",
      " 3.79472929 4.5772963  4.5772963  4.5772963  5.66347923 4.5772963\n",
      " 5.66347923 4.5772963  3.05223276 1.18348283 3.05223276 4.5772963\n",
      " 4.5772963  2.70854636 4.5772963  3.05223276 1.18348283 5.66347923\n",
      " 4.5772963  4.5772963  5.66347923 3.05223276 4.5772963  4.5772963\n",
      " 4.5772963  4.5772963  3.05223276 3.79472929]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.000\n",
      "(*) epoch 2, cost 1.696\n",
      "(*) epoch 3, cost 1.479\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.624\n",
      "(*) epoch 2, cost 3.531\n",
      "(*) epoch 3, cost 3.069\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.33169686 0.33169686 0.27755809 1.41836951 0.27755809 0.33169686\n",
      " 0.33169686 0.33169686 1.41836951 0.33169686 1.41836951 0.33169686\n",
      " 0.33169686 0.25976365 0.25976365 0.33169686 0.33169686 0.27819021\n",
      " 0.25976365 0.33169686 0.33169686 1.41836951 0.33169686 0.3494913\n",
      " 0.25976365 1.43616395 0.27755809 0.27755809 0.3494913  0.33169686\n",
      " 0.33169686 1.41836951 0.3494913  0.25976365 0.33169686 0.33169686\n",
      " 0.33169686 1.3464363  0.38128271 0.33169686 0.27755809 1.41836951\n",
      " 0.27755809 0.33169686 0.3494913  0.27755809 1.3464363  1.41836951\n",
      " 0.33169686 0.33169686 0.33169686 0.3494913  0.3494913  0.33169686\n",
      " 1.41836951 0.33169686 0.33169686 0.33169686 0.25976365 0.3494913\n",
      " 0.33169686 0.33169686 0.3494913  0.33169686 1.41836951 0.3494913\n",
      " 0.33169686 0.36791786 0.25976365 0.25976365 0.3494913  0.25976365\n",
      " 0.3494913  0.25976365 0.25976365 0.25976365 0.33169686 0.3494913\n",
      " 1.43616395 0.33169686 1.43616395 0.25976365 0.27819021 1.43616395\n",
      " 0.33169686 0.27755809 0.25976365 0.3494913  0.25976365 0.33169686\n",
      " 1.43679607 0.27755809 0.33169686 0.25976365 0.3494913  0.33169686\n",
      " 1.3464363  0.33169686 1.3464363  0.3494913 ]\n",
      "all_sum is: [1.68345735 1.37905887 1.68345735 3.14831582 1.37905887 1.37905887\n",
      " 1.37905887 1.37905887 1.68345735 0.95891558 1.68345735 1.37905887\n",
      " 1.37905887 1.37905887 1.37905887 1.37905887 1.37905887 1.37905887\n",
      " 3.14831582 1.68345735 3.4527143  0.95891558 1.37905887 1.37905887\n",
      " 3.14831582 1.68345735 3.14831582 1.37905887 1.37905887 1.37905887\n",
      " 1.68345735 1.37905887 3.14831582 3.14831582 3.14831582 1.37905887\n",
      " 1.37905887 1.37905887 3.14831582 1.37905887 1.37905887 0.95891558\n",
      " 1.37905887 1.37905887 1.37905887 1.68345735 3.4527143  1.37905887\n",
      " 1.37905887 1.68345735 3.4527143  1.37905887 1.37905887 1.37905887\n",
      " 3.14831582 1.68345735 1.37905887 1.37905887 1.37905887 1.37905887\n",
      " 1.37905887 1.37905887 1.68345735 3.14831582 1.68345735 1.37905887\n",
      " 1.37905887 3.14831582 3.14831582 0.95891558 1.26331407 1.37905887\n",
      " 1.68345735 1.68345735 1.37905887 1.37905887 3.4527143  1.68345735\n",
      " 1.68345735 1.37905887 1.37905887 1.68345735 1.37905887 1.37905887\n",
      " 1.37905887 1.37905887 1.26331407 3.14831582 1.37905887 3.14831582\n",
      " 1.37905887 1.68345735 1.68345735 1.37905887 3.14831582 1.37905887\n",
      " 1.37905887 1.37905887 1.68345735 1.37905887]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.998\n",
      "(*) epoch 2, cost 3.236\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.531\n",
      "(*) epoch 2, cost 3.802\n",
      "(*) epoch 3, cost 3.340\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.40472037 0.52527842 0.52527842 0.52527842 0.52527842 0.52527842\n",
      " 0.52527842 0.52527842 0.52527842 1.50812973 1.50812973 0.52527842\n",
      " 1.50812973 0.52527842 0.52527842 1.50812973 0.52527842 0.52527842\n",
      " 0.52527842 0.52527842 0.52527842 0.52527842 0.52527842 0.52527842\n",
      " 0.52527842 0.52527842 0.52527842 0.52527842 1.50812973 0.52527842\n",
      " 0.52527842 0.52527842 1.50812973 3.17014954 0.52527842 1.50812973\n",
      " 0.52527842 0.52527842 0.52527842 1.50812973 1.50812973 0.52527842\n",
      " 1.50812973 0.52527842 0.52527842 1.50812973 1.50812973 0.52527842\n",
      " 0.52527842 0.52527842 0.52527842 1.50812973 0.52527842 0.52527842\n",
      " 0.52527842 0.52527842 1.50812973 0.52527842 1.50812973 0.52527842\n",
      " 0.52527842 1.40472037 0.52527842 0.52527842 1.50812973 1.50812973\n",
      " 1.50812973 1.50812973 0.52527842 0.52527842 0.52527842 0.52527842\n",
      " 1.50812973 0.52527842 0.52527842 0.52527842 1.50812973 1.50812973\n",
      " 1.50812973 1.50812973 3.17014954 1.50812973 1.50812973 0.52527842\n",
      " 1.50812973 0.52527842 0.52527842 0.52527842 0.52527842 1.50812973\n",
      " 1.50812973 1.50812973 0.52527842 2.38757168 0.52527842 0.52527842\n",
      " 1.50812973 1.50812973 0.52527842 0.52527842]\n",
      "all_sum is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.384\n",
      "(*) epoch 2, cost 0.789\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.393\n",
      "(*) epoch 2, cost 2.045\n",
      "(*) epoch 3, cost 1.756\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.93010539 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.72232317 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.72232317 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11921287 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.203\n",
      "(*) epoch 2, cost 0.703\n",
      "(*) epoch 3, cost 0.379\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.631\n",
      "(*) epoch 2, cost 0.738\n",
      "(*) epoch 3, cost 0.502\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.75009507 1.75009507 0.93305278 1.75009507 0.93305278 0.8170423\n",
      " 1.00868313 0.93305278 0.93305278 0.93305278 0.93305278 1.75009507\n",
      " 0.         1.75009507 0.93305278 1.75009507 1.75009507 0.8170423\n",
      " 0.93305278 1.93763622 0.93305278 0.93305278 2.48087486 0.93305278\n",
      " 0.8170423  1.75009507 1.00868313 0.8170423  0.93305278 1.75009507\n",
      " 0.93305278 1.75009507 0.93305278 0.93305278 1.75009507 0.93305278\n",
      " 0.8170423  1.75009507 0.8170423  1.75009507 0.8170423  1.75009507\n",
      " 0.93305278 1.75009507 0.         1.75009507 0.8170423  0.93305278\n",
      " 0.93305278 0.93305278 0.93305278 1.75009507 0.         0.8170423\n",
      " 1.75009507 1.75009507 1.75009507 0.93305278 0.93305278 0.8170423\n",
      " 0.8170423  1.75009507 1.75009507 0.93305278 0.         0.93305278\n",
      " 1.75009507 0.8170423  1.75009507 0.93305278 0.         1.75009507\n",
      " 0.93305278 1.75009507 1.75009507 1.75009507 0.8170423  1.54782208\n",
      " 0.93305278 0.         0.93305278 0.8170423  0.         0.93305278\n",
      " 1.75009507 1.75009507 1.75009507 0.93305278 0.93305278 2.75467852\n",
      " 0.         1.82162574 1.75009507 1.75009507 0.93305278 0.93305278\n",
      " 1.75009507 0.93305278 0.93305278 1.75009507]\n",
      "all_sum is: [2.85011417 1.71882012 1.71882012 1.71882012 1.35277202 1.71882012\n",
      " 0.36604809 1.71882012 1.35277202 1.71882012 1.71882012 1.71882012\n",
      " 1.35277202 0.36604809 1.35277202 1.35277202 1.71882012 1.71882012\n",
      " 2.85011417 1.35277202 1.71882012 1.71882012 2.85011417 2.48406608\n",
      " 1.71882012 1.71882012 1.71882012 1.71882012 1.71882012 1.35277202\n",
      " 1.71882012 1.71882012 1.71882012 1.71882012 1.35277202 1.71882012\n",
      " 1.35277202 1.71882012 1.71882012 1.71882012 2.85011417 1.71882012\n",
      " 1.71882012 1.71882012 1.71882012 2.48406608 2.85011417 1.71882012\n",
      " 1.71882012 1.71882012 1.35277202 1.71882012 1.71882012 1.35277202\n",
      " 1.71882012 1.35277202 0.36604809 1.71882012 1.71882012 1.71882012\n",
      " 2.85011417 1.71882012 2.85011417 1.71882012 0.36604809 2.48406608\n",
      " 1.35277202 1.71882012 2.85011417 1.71882012 1.71882012 1.35277202\n",
      " 0.36604809 1.71882012 1.71882012 1.71882012 2.48406608 1.71882012\n",
      " 1.71882012 1.71882012 1.71882012 1.71882012 1.71882012 1.71882012\n",
      " 1.71882012 1.71882012 1.71882012 1.71882012 1.71882012 1.35277202\n",
      " 1.35277202 1.71882012 1.71882012 1.35277202 1.71882012 2.85011417\n",
      " 1.71882012 1.71882012 1.71882012 1.71882012]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.725\n",
      "(*) epoch 2, cost 1.781\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.177\n",
      "(*) epoch 2, cost 2.818\n",
      "(*) epoch 3, cost 2.574\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.99064337 2.99064337 3.77448593 2.99064337 2.99064337 3.77448593\n",
      " 2.99064337 2.99064337 0.82757479 2.99064337 2.99064337 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337\n",
      " 2.99064337 3.77448593 2.99064337 2.99064337 2.99064337 3.77448593\n",
      " 2.99064337 2.99064337 2.99064337 3.77448593 2.99064337 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337 3.70688293\n",
      " 2.99064337 3.77448593 2.99064337 2.99064337 3.77448593 3.77448593\n",
      " 2.99064337 2.99064337 2.99064337 2.99064337 1.54381435 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 0.82757479 2.99064337 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337 2.99064337\n",
      " 2.99064337 3.77448593 3.89994116 0.82757479 2.99064337 2.99064337\n",
      " 3.77448593 2.99064337 0.82757479 2.99064337 3.77448593 2.99064337\n",
      " 3.70688293 0.82757479 2.99064337 2.99064337 2.99064337 2.99064337\n",
      " 2.99064337 2.99064337 2.99064337 3.77448593 2.99064337 0.82757479\n",
      " 2.99064337 2.99064337 2.99064337 3.77448593 2.99064337 3.77448593\n",
      " 3.89994116 3.77448593 3.77448593 2.99064337]\n",
      "all_sum is: [0.         1.41520924 0.         0.         0.         0.\n",
      " 0.         1.41520924 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.41520924 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.41520924 0.         0.\n",
      " 0.         0.         1.41520924 0.         0.         0.\n",
      " 1.41520924 0.         0.         1.41520924 1.41520924 1.41520924\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.41520924 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.41520924 0.\n",
      " 0.         0.         1.41520924 0.         0.         0.\n",
      " 0.         1.41520924 0.         1.41520924 0.         1.41520924\n",
      " 0.         0.         1.41520924 0.         0.         0.\n",
      " 0.         0.         0.         0.         1.41520924 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.41520924 0.         0.         0.         1.41520924\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.731\n",
      "(*) epoch 2, cost 2.212\n",
      "(*) epoch 3, cost 2.020\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.716\n",
      "(*) epoch 2, cost 1.148\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.54822519 1.53971389 0.         1.53971389 0.1640911  0.1640911\n",
      " 0.         0.1640911  1.53971389 0.         0.1640911  0.1640911\n",
      " 0.1640911  1.54822519 1.3756228  0.1640911  1.53971389 0.1640911\n",
      " 0.1640911  2.92384799 0.1640911  0.         1.53971389 1.53971389\n",
      " 2.75975689 1.3841341  0.1640911  0.1640911  0.         1.53971389\n",
      " 0.         0.1640911  0.1640911  0.         1.53971389 0.1640911\n",
      " 1.53971389 0.         0.         1.53971389 1.53971389 0.1640911\n",
      " 1.53971389 1.53971389 0.1640911  0.1640911  0.         1.53971389\n",
      " 1.53971389 1.3841341  1.53971389 1.3756228  0.1640911  1.3756228\n",
      " 1.53971389 0.         0.         0.         1.54822519 1.3756228\n",
      " 1.53971389 1.53971389 0.         1.3756228  1.3756228  1.3756228\n",
      " 0.1640911  1.53971389 1.53971389 0.         1.53971389 0.1640911\n",
      " 0.1640911  0.1640911  1.53971389 1.3756228  1.53971389 0.1640911\n",
      " 1.3756228  0.         1.53971389 0.1640911  0.1640911  2.75975689\n",
      " 0.1640911  1.3756228  0.1640911  1.53971389 0.1640911  1.53971389\n",
      " 2.75975689 0.         1.54822519 1.54822519 0.1640911  0.1640911\n",
      " 1.3756228  0.1640911  0.1640911  1.53971389]\n",
      "all_sum is: [0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.09309721 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.68388427 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.09309721 0.01867172 1.27796944 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.68388427 0.01867172 0.01867172 1.27796944 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.68388427 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.68388427 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.68388427 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.68388427 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.68388427 0.01867172 0.01867172\n",
      " 0.01867172 0.01867172 0.01867172 0.01867172]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.648\n",
      "(*) epoch 2, cost 2.133\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.914\n",
      "(*) epoch 2, cost 0.739\n",
      "(*) epoch 3, cost 0.546\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.62387878 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766 1.85431766\n",
      " 1.85431766 1.85431766 1.85431766 1.85431766]\n",
      "all_sum is: [2.91472785 2.91472785 2.91472785 1.7186655  2.91472785 2.91472785\n",
      " 1.7186655  2.91472785 2.91472785 2.82247748 2.91472785 2.91472785\n",
      " 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 1.7186655  2.91472785 2.91472785 2.91472785 1.7186655\n",
      " 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785 1.7186655\n",
      " 2.91472785 2.91472785 4.01853983 4.01853983 2.91472785 2.91472785\n",
      " 2.91472785 2.91472785 2.91472785 2.91472785 2.91472785 4.01853983\n",
      " 4.01853983 1.7186655  4.01853983 2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 4.01853983 2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 1.7186655  2.91472785 2.91472785 2.91472785 4.01853983 2.91472785\n",
      " 2.91472785 2.91472785 1.7186655  2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 1.7186655  2.91472785 2.91472785 2.91472785 2.91472785\n",
      " 2.91472785 2.91472785 2.91472785 1.7186655  2.91472785 2.91472785\n",
      " 2.91472785 2.91472785 2.91472785 2.91472785]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.281\n",
      "(*) epoch 2, cost 2.800\n",
      "(*) epoch 3, cost 2.559\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.711\n",
      "(*) epoch 2, cost 1.798\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.37663729 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099 1.18632099\n",
      " 1.18632099 1.18632099 1.18632099 1.18632099]\n",
      "all_sum is: [1.48719694 2.06204453 1.0164066  1.48719694 0.44155901 1.0164066\n",
      " 2.06204453 2.06204453 0.44155901 2.06204453 0.44155901 0.44155901\n",
      " 1.0164066  0.44155901 2.06204453 1.0164066  1.0164066  0.44155901\n",
      " 1.0164066  2.06204453 2.06204453 2.06204453 0.44155901 1.48719694\n",
      " 1.0164066  2.06204453 1.95509157 1.0164066  1.48719694 1.0164066\n",
      " 2.06204453 1.0164066  1.48719694 2.06204453 2.06204453 0.44155901\n",
      " 1.48719694 2.06204453 2.06204453 1.48719694 1.0164066  1.0164066\n",
      " 2.06204453 2.06204453 0.44155901 0.44155901 2.06204453 2.06204453\n",
      " 1.0164066  1.0164066  1.0164066  1.48719694 1.0164066  0.44155901\n",
      " 2.06204453 2.06204453 1.0164066  2.06204453 1.0164066  1.48719694\n",
      " 1.0164066  1.0164066  2.06204453 1.48719694 1.0164066  2.06204453\n",
      " 0.44155901 0.44155901 2.06204453 0.44155901 1.48719694 1.0164066\n",
      " 2.06204453 2.06204453 1.0164066  1.0164066  1.48719694 2.06204453\n",
      " 2.06204453 1.48719694 1.48719694 1.0164066  2.06204453 1.48719694\n",
      " 0.44155901 0.44155901 1.0164066  1.48719694 1.0164066  1.0164066\n",
      " 2.06204453 0.44155901 2.06204453 0.44155901 1.48719694 1.0164066\n",
      " 2.06204453 0.44155901 0.44155901 0.44155901]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.197\n",
      "(*) epoch 2, cost 0.829\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.003\n",
      "(*) epoch 2, cost 2.331\n",
      "(*) epoch 3, cost 1.968\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966 1.54010966\n",
      " 1.54010966 1.54010966 1.54010966 1.54010966]\n",
      "all_sum is: [1.80725085 1.80725085 1.80725085 4.01038409 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 4.01038409 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 2.016697   1.80725085 1.80725085\n",
      " 4.01038409 1.80725085 1.80725085 4.01038409 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 4.01038409 1.80725085 1.80725085 4.01038409 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 4.01038409 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085 1.80725085\n",
      " 1.80725085 1.80725085 1.80725085 1.80725085]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.735\n",
      "(*) epoch 2, cost 0.796\n",
      "(*) epoch 3, cost 0.498\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.902\n",
      "(*) epoch 2, cost 2.033\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.10286886 0.74955879 2.10286886 0.74955879 0.74955879 0.\n",
      " 0.08142841 0.74955879 0.66813038 0.74955879 0.74955879 0.74955879\n",
      " 0.74955879 0.74955879 0.08142841 0.74955879 0.74955879 0.66813038\n",
      " 0.66813038 0.66813038 0.74955879 2.10286886 0.74955879 0.66813038\n",
      " 0.74955879 0.66813038 0.74955879 0.66813038 0.08142841 0.08142841\n",
      " 0.66813038 0.74955879 0.08142841 1.35331007 0.74955879 0.74955879\n",
      " 0.74955879 0.66813038 0.66813038 0.74955879 0.66813038 0.21769267\n",
      " 0.66813038 2.10286886 0.66813038 0.74955879 0.         1.35331007\n",
      " 0.66813038 2.10286886 0.         0.66813038 0.66813038 0.\n",
      " 0.         0.66813038 0.74955879 0.74955879 2.02144045 2.02144045\n",
      " 2.02144045 2.10286886 0.         0.74955879 0.66813038 0.66813038\n",
      " 0.66813038 0.08142841 2.02144045 2.02144045 0.66813038 0.74955879\n",
      " 0.74955879 0.74955879 0.74955879 0.66813038 0.         0.74955879\n",
      " 0.74955879 0.66813038 0.74955879 0.74955879 0.74955879 2.10286886\n",
      " 0.74955879 0.74955879 0.66813038 0.74955879 2.02144045 0.74955879\n",
      " 0.08142841 2.10286886 0.08142841 0.66813038 0.74955879 0.08142841\n",
      " 0.66813038 0.74955879 0.08142841 0.66813038]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.81449185 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.81449185 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.81449185\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.339\n",
      "(*) epoch 2, cost 4.375\n",
      "(*) epoch 3, cost 3.893\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.889\n",
      "(*) epoch 2, cost 1.608\n",
      "(*) epoch 3, cost 1.013\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.7359658  0.7359658  0.7359658  0.7359658  1.71316896 0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 1.71316896 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  1.08700017 0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  1.08700017 0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658  0.7359658  0.7359658\n",
      " 0.7359658  0.7359658  0.7359658  0.7359658 ]\n",
      "all_sum is: [1.90227899 2.52768173 2.52768173 2.29697202 1.90227899 1.90227899\n",
      " 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899\n",
      " 1.90227899 3.57849619 1.90227899 2.52768173 2.52768173 1.90227899\n",
      " 1.90227899 1.90227899 1.90227899 2.52768173 2.52768173 1.90227899\n",
      " 1.90227899 2.29697202 1.90227899 2.52768173 1.90227899 1.90227899\n",
      " 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899 2.52768173\n",
      " 2.29697202 2.52768173 1.90227899 1.90227899 1.90227899 2.29697202\n",
      " 2.95309345 2.52768173 2.29697202 1.90227899 2.52768173 2.52768173\n",
      " 2.52768173 2.00205016 1.90227899 1.90227899 2.95309345 2.52768173\n",
      " 1.90227899 1.90227899 1.90227899 2.52768173 2.52768173 2.52768173\n",
      " 2.52768173 1.90227899 1.90227899 1.90227899 2.95309345 1.90227899\n",
      " 2.29697202 2.29697202 1.90227899 1.90227899 2.52768173 1.90227899\n",
      " 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899 2.52768173\n",
      " 2.52768173 1.90227899 2.52768173 1.90227899 2.92237476 1.90227899\n",
      " 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899 1.90227899\n",
      " 2.52768173 1.90227899 2.95309345 2.52768173 1.90227899 2.52768173\n",
      " 1.90227899 1.90227899 1.90227899 1.90227899]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.371\n",
      "(*) epoch 2, cost 0.606\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.922\n",
      "(*) epoch 2, cost 2.203\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.72617316 2.0697554  4.09050661 0.93971358 2.09474792 4.72617316\n",
      " 2.79406114 3.16227934 2.0697554  4.00186742 1.23016733 3.71141367\n",
      " 2.79406114 2.79301133 4.72617316 3.16227934 2.22256576 4.09050661\n",
      " 2.0697554  2.0697554  1.93211202 1.95447307 2.5036074  1.95447307\n",
      " 1.95447307 0.29045374 2.0697554  4.00186742 3.59613134 2.8718256\n",
      " 1.93211202 3.78645958 4.72617316 1.95447307 3.16227934 2.0697554\n",
      " 2.9468715  4.00186742 3.88658509 2.22256576 2.9468715  2.0697554\n",
      " 4.00186742 3.16227934 2.79406114 2.0697554  0.29045374 3.08714636\n",
      " 2.8718256  4.72617316 2.22256576 3.16227934 4.00186742 0.29045374\n",
      " 4.43571942 3.16227934 1.66401932 2.0697554  3.88658509 3.16227934\n",
      " 2.77170009 3.71141367 2.0697554  1.23016733 4.00186742 3.16227934\n",
      " 2.22256576 4.00186742 4.00186742 1.23016733 2.0697554  1.66401932\n",
      " 1.77930166 4.00186742 2.98331188 1.13004182 1.95447307 3.59613134\n",
      " 2.0697554  4.72617316 2.0697554  4.02685994 3.06215384 2.22256576\n",
      " 3.16227934 3.88658509 2.0697554  4.00186742 2.8718256  1.23016733\n",
      " 2.15839459 3.06215384 2.0697554  1.23016733 1.80429418 3.16227934\n",
      " 3.88658509 2.9468715  3.16227934 1.23016733]\n",
      "all_sum is: [0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838 0.64361838\n",
      " 0.64361838 0.64361838 0.64361838 0.64361838]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.311\n",
      "(*) epoch 2, cost 3.864\n",
      "(*) epoch 3, cost 3.538\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.168\n",
      "(*) epoch 2, cost 3.243\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.33702146 1.63797161 1.33702146 1.33702146 1.63797161 1.33702146\n",
      " 1.63797161 1.63797161 1.33702146 1.63797161 1.33702146 1.63797161\n",
      " 1.63797161 1.63797161 1.33702146 1.63797161 1.33702146 1.33702146\n",
      " 1.33702146 1.63797161 1.63797161 1.33702146 1.63797161 1.33702146\n",
      " 1.33702146 1.33702146 1.33702146 1.63797161 1.33702146 1.33702146\n",
      " 1.33702146 1.63797161 1.33702146 1.33702146 1.33702146 1.33702146\n",
      " 1.63797161 1.33702146 1.33702146 1.33702146 1.33702146 1.33702146\n",
      " 1.63797161 1.63797161 1.33702146 1.63797161 1.63797161 1.63797161\n",
      " 1.33702146 1.33702146 1.33702146 1.63797161 1.33702146 1.33702146\n",
      " 1.63797161 1.63797161 1.33702146 1.33702146 1.63797161 1.63797161\n",
      " 1.33702146 1.33702146 1.63797161 1.33702146 1.63797161 1.63797161\n",
      " 1.63797161 1.33702146 1.63797161 1.33702146 1.33702146 1.33702146\n",
      " 1.63797161 1.33702146 1.63797161 1.63797161 1.63797161 1.63797161\n",
      " 1.33702146 1.63797161 1.33702146 1.33702146 1.33702146 1.63797161\n",
      " 1.63797161 1.33702146 1.63797161 1.63797161 1.33702146 1.63797161\n",
      " 1.63797161 1.63797161 1.63797161 1.63797161 1.33702146 1.63797161\n",
      " 1.63797161 1.33702146 1.33702146 1.33702146]\n",
      "all_sum is: [4.20299482 2.73176604 2.1160028  4.20299482 4.20299482 3.331865\n",
      " 4.20299482 4.20299482 2.1160028  4.20299482 2.1160028  4.20299482\n",
      " 4.20299482 4.20299482 4.20299482 2.1160028  2.1160028  4.20299482\n",
      " 2.1160028  2.1160028  3.331865   4.20299482 2.1160028  2.1160028\n",
      " 2.1160028  2.1160028  4.20299482 4.20299482 2.1160028  4.20299482\n",
      " 4.20299482 2.1160028  4.20299482 4.20299482 4.20299482 4.20299482\n",
      " 4.20299482 4.20299482 2.1160028  4.20299482 4.20299482 4.20299482\n",
      " 2.1160028  4.20299482 4.20299482 2.1160028  4.20299482 2.1160028\n",
      " 4.20299482 4.20299482 2.1160028  2.1160028  4.20299482 4.20299482\n",
      " 4.20299482 4.20299482 2.1160028  4.20299482 4.20299482 2.1160028\n",
      " 2.1160028  4.20299482 4.20299482 4.20299482 4.20299482 2.1160028\n",
      " 2.1160028  2.1160028  2.1160028  2.1160028  2.1160028  4.20299482\n",
      " 2.1160028  4.20299482 4.20299482 4.20299482 2.1160028  4.20299482\n",
      " 2.1160028  4.20299482 2.1160028  2.1160028  2.1160028  2.1160028\n",
      " 4.20299482 4.20299482 4.20299482 2.1160028  2.1160028  2.1160028\n",
      " 2.1160028  4.20299482 2.1160028  4.20299482 4.20299482 2.1160028\n",
      " 2.1160028  2.1160028  2.1160028  4.20299482]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.709\n",
      "(*) epoch 2, cost 1.973\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.519\n",
      "(*) epoch 2, cost 1.283\n",
      "(*) epoch 3, cost 1.122\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.01537392 0.01537392 0.         0.01537392 0.         0.\n",
      " 0.01537392 0.01537392 0.01537392 0.01537392 0.         0.\n",
      " 0.01537392 0.01537392 0.01537392 0.         0.         0.\n",
      " 0.         0.01537392 0.         0.01537392 0.01537392 0.01537392\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.53813105 0.01537392 0.55350497 0.         0.         0.01537392\n",
      " 0.01537392 0.01537392 0.         0.         0.01537392 0.01537392\n",
      " 0.01537392 0.         0.01537392 0.01537392 0.01537392 0.\n",
      " 0.01537392 0.01537392 0.01537392 0.01537392 0.01537392 0.01537392\n",
      " 0.01537392 0.01537392 0.01537392 0.01537392 0.01537392 0.\n",
      " 0.         0.01537392 0.01537392 0.01537392 0.01537392 0.01537392\n",
      " 0.         0.01537392 0.         0.01537392 0.         0.\n",
      " 0.         0.         0.         0.01537392 0.         0.01537392\n",
      " 0.01537392 0.01537392 0.         0.01537392 0.01537392 0.53813105\n",
      " 0.         0.01537392 0.01537392 0.         0.01537392 0.01537392\n",
      " 0.01537392 0.01537392 0.01537392 0.         0.         0.\n",
      " 0.01537392 0.01537392 0.01537392 0.        ]\n",
      "all_sum is: [3.17920662 3.17920662 2.64062136 3.17920662 2.64062136 2.64062136\n",
      " 3.17920662 3.17920662 2.64062136 3.17920662 2.64062136 3.17920662\n",
      " 3.17920662 3.17920662 3.17920662 0.53858526 2.64062136 2.64062136\n",
      " 3.17920662 0.         3.17920662 2.64062136 3.17920662 2.64062136\n",
      " 3.17920662 3.17920662 2.64062136 3.17920662 3.17920662 3.17920662\n",
      " 2.64062136 2.64062136 3.17920662 0.53858526 2.64062136 2.64062136\n",
      " 3.17920662 3.17920662 3.17920662 3.17920662 3.17920662 3.17920662\n",
      " 3.17920662 3.17920662 2.64062136 2.64062136 0.53858526 3.17920662\n",
      " 2.64062136 3.17920662 3.17920662 3.17920662 3.35048524 0.\n",
      " 3.17920662 2.64062136 2.64062136 2.64062136 2.64062136 3.17920662\n",
      " 0.         3.8890705  2.64062136 2.64062136 2.64062136 3.17920662\n",
      " 2.64062136 0.53858526 0.         2.64062136 2.64062136 2.64062136\n",
      " 2.64062136 3.17920662 2.64062136 3.17920662 3.17920662 2.64062136\n",
      " 3.17920662 2.64062136 3.17920662 0.53858526 0.53858526 3.17920662\n",
      " 2.64062136 3.17920662 2.64062136 2.64062136 3.17920662 2.64062136\n",
      " 2.64062136 3.17920662 2.64062136 2.64062136 3.17920662 3.17920662\n",
      " 3.17920662 2.64062136 0.         2.64062136]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.148\n",
      "(*) epoch 2, cost 1.981\n",
      "(*) epoch 3, cost 1.878\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.279\n",
      "(*) epoch 2, cost 1.420\n",
      "(*) epoch 3, cost 1.258\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.00158388 1.50725965 1.50725965 2.07210128 1.50725965 2.07210128\n",
      " 1.50725965 1.50725965 1.50725965 1.50725965 1.50725965 1.50725965\n",
      " 1.50725965 1.50725965 1.00158388 1.50725965 1.00158388 1.00158388\n",
      " 1.50725965 1.50725965 2.07210128 2.07210128 1.00158388 1.50725965\n",
      " 1.50725965 1.50725965 1.50725965 2.07210128 1.50725965 1.50725965\n",
      " 1.00158388 1.50725965 1.00158388 1.50725965 1.00158388 2.07210128\n",
      " 2.07210128 2.07210128 1.00158388 2.07210128 1.50725965 1.50725965\n",
      " 2.07210128 1.56642551 2.07210128 1.56642551 1.50725965 1.50725965\n",
      " 1.50725965 2.07210128 1.50725965 1.50725965 1.50725965 1.50725965\n",
      " 2.07210128 1.50725965 1.00158388 1.50725965 1.50725965 2.07210128\n",
      " 1.00158388 1.50725965 1.81077047 1.50725965 1.50725965 2.07210128\n",
      " 2.07210128 1.50725965 1.50725965 1.00158388 1.00158388 1.50725965\n",
      " 1.50725965 1.50725965 1.00158388 1.56642551 1.50725965 1.50725965\n",
      " 1.00158388 1.50725965 1.56642551 1.50725965 1.50725965 1.00158388\n",
      " 1.56642551 1.00158388 1.50725965 1.50725965 2.07210128 2.07210128\n",
      " 1.00158388 1.00158388 2.07210128 1.50725965 1.50725965 1.50725965\n",
      " 2.07210128 1.50725965 1.56642551 1.00158388]\n",
      "all_sum is: [1.34546811 1.34546811 0.44128893 1.34546811 0.06908072 0.44128893\n",
      " 0.06908072 1.34546811 1.34546811 0.06908072 0.06908072 1.34546811\n",
      " 1.71767631 0.06908072 0.06908072 1.71767631 0.44128893 1.34546811\n",
      " 1.34546811 1.71767631 0.06908072 1.71767631 1.34546811 1.34546811\n",
      " 0.06908072 1.34546811 1.71767631 1.71767631 1.71767631 0.06908072\n",
      " 0.44128893 1.34546811 0.06908072 1.71767631 0.06908072 0.06908072\n",
      " 1.34546811 1.34546811 1.34546811 1.71767631 1.34546811 0.06908072\n",
      " 1.34546811 0.44128893 0.44128893 0.06908072 0.44128893 0.06908072\n",
      " 0.06908072 0.44128893 1.71767631 1.34546811 0.06908072 0.06908072\n",
      " 1.34546811 1.71767631 1.34546811 0.06908072 1.34546811 1.34546811\n",
      " 0.06908072 1.34546811 1.71767631 0.06908072 0.44128893 1.34546811\n",
      " 1.71767631 0.06908072 0.06908072 0.06908072 1.71767631 0.06908072\n",
      " 1.71767631 0.06908072 0.44128893 0.06908072 0.06908072 1.71767631\n",
      " 0.06908072 0.44128893 0.06908072 1.34546811 0.06908072 1.34546811\n",
      " 1.34546811 1.71767631 0.06908072 0.06908072 0.06908072 0.44128893\n",
      " 0.06908072 1.34546811 0.06908072 0.06908072 1.71767631 1.34546811\n",
      " 0.06908072 0.44128893 0.06908072 0.44128893]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.578\n",
      "(*) epoch 2, cost 2.800\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.691\n",
      "(*) epoch 2, cost 3.181\n",
      "(*) epoch 3, cost 2.658\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.43681413 0.86412571 0.86412571 1.43681413 0.86412571 0.86412571\n",
      " 0.86412571 0.86412571 0.86412571 1.43681413 0.86412571 0.86412571\n",
      " 0.86412571 0.86412571 0.86412571 0.86412571 0.86412571 1.82132077\n",
      " 0.86412571 1.43681413 1.43681413 1.82132077 0.86412571 0.86412571\n",
      " 1.43681413 0.86412571 0.86412571 0.86412571 0.86412571 0.86412571\n",
      " 0.86412571 0.86412571 0.86412571 2.39400919 0.86412571 1.82132077\n",
      " 0.86412571 0.86412571 0.86412571 0.86412571 1.43681413 0.86412571\n",
      " 0.86412571 0.86412571 2.39400919 1.43681413 0.86412571 0.86412571\n",
      " 1.82132077 0.86412571 1.43681413 1.43681413 0.86412571 0.86412571\n",
      " 0.86412571 0.86412571 0.86412571 0.86412571 0.86412571 0.86412571\n",
      " 1.43681413 0.86412571 0.86412571 0.86412571 0.86412571 0.86412571\n",
      " 0.86412571 0.86412571 1.43681413 0.86412571 0.86412571 1.43681413\n",
      " 1.43681413 0.86412571 1.43681413 0.86412571 1.43681413 0.86412571\n",
      " 0.86412571 0.86412571 0.86412571 1.82132077 1.43681413 0.86412571\n",
      " 1.82132077 1.43681413 0.86412571 0.86412571 1.43681413 1.43681413\n",
      " 0.86412571 0.86412571 0.86412571 1.43681413 0.86412571 0.86412571\n",
      " 0.86412571 1.43681413 0.86412571 0.86412571]\n",
      "all_sum is: [2.18223266 2.91745877 2.18223266 2.18223266 2.91745877 2.18223266\n",
      " 2.91745877 2.91745877 2.18223266 2.18223266 2.91745877 2.18223266\n",
      " 2.18223266 2.91745877 2.18223266 2.91745877 2.91745877 2.91745877\n",
      " 2.18223266 2.18223266 2.18223266 2.18223266 2.63986124 2.18223266\n",
      " 2.18223266 2.91745877 2.18223266 2.91745877 2.91745877 2.91745877\n",
      " 2.18223266 2.91745877 2.91745877 2.18223266 2.91745877 2.91745877\n",
      " 2.91745877 2.18223266 2.18223266 2.91745877 2.18223266 2.18223266\n",
      " 2.91745877 2.91745877 3.37508735 2.18223266 2.91745877 2.18223266\n",
      " 2.91745877 2.91745877 2.91745877 2.91745877 2.18223266 2.91745877\n",
      " 2.91745877 2.91745877 2.18223266 2.18223266 2.18223266 2.91745877\n",
      " 3.37508735 0.         2.91745877 1.19285469 2.91745877 2.91745877\n",
      " 2.18223266 2.91745877 2.18223266 2.91745877 2.91745877 2.91745877\n",
      " 2.91745877 2.18223266 2.18223266 0.73522611 2.18223266 2.91745877\n",
      " 2.91745877 2.18223266 2.91745877 2.91745877 2.18223266 2.18223266\n",
      " 2.91745877 2.18223266 2.91745877 2.91745877 2.18223266 2.18223266\n",
      " 2.91745877 0.73522611 2.18223266 2.91745877 2.18223266 2.91745877\n",
      " 2.18223266 3.37508735 2.18223266 2.91745877]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.478\n",
      "(*) epoch 2, cost 2.663\n",
      "(*) epoch 3, cost 2.217\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.258\n",
      "(*) epoch 2, cost 1.691\n",
      "(*) epoch 3, cost 1.489\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.83416575 2.83416575 2.52828799 2.83416575 2.83416575 2.83416575\n",
      " 2.83416575 2.52828799 2.83416575 2.83416575 2.83416575 2.52828799\n",
      " 2.83416575 2.52828799 2.83416575 2.83416575 2.83416575 2.52828799\n",
      " 2.52828799 2.83416575 2.83416575 2.83416575 2.83416575 2.52828799\n",
      " 2.83416575 2.52828799 2.83416575 2.52828799 2.83416575 2.83416575\n",
      " 2.83416575 2.52828799 2.83416575 2.52828799 2.83416575 2.52828799\n",
      " 2.83416575 2.83416575 2.83416575 2.52828799 2.83416575 2.52828799\n",
      " 2.52828799 2.52828799 2.83416575 2.83416575 2.52828799 2.52828799\n",
      " 2.83416575 2.83416575 2.83416575 2.83416575 2.52828799 2.83416575\n",
      " 2.52828799 2.83416575 2.83416575 2.83416575 2.52828799 2.52828799\n",
      " 2.83416575 2.52828799 2.52828799 2.83416575 2.83416575 2.52828799\n",
      " 2.83416575 2.83416575 2.83416575 2.52828799 2.83416575 3.59151172\n",
      " 2.83416575 2.83416575 2.83416575 2.83416575 2.83416575 2.83416575\n",
      " 2.52828799 2.52828799 2.83416575 2.52828799 2.83416575 2.83416575\n",
      " 2.52828799 3.59151172 2.83416575 2.83416575 2.52828799 2.52828799\n",
      " 2.52828799 2.83416575 2.83416575 2.83416575 2.83416575 2.52828799\n",
      " 2.83416575 2.83416575 2.83416575 2.83416575]\n",
      "all_sum is: [0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.33834593 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.         0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025 0.15312025\n",
      " 0.15312025 0.33834593 0.15312025 0.15312025]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.107\n",
      "(*) epoch 2, cost 2.942\n",
      "(*) epoch 3, cost 2.522\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.122\n",
      "(*) epoch 2, cost 1.521\n",
      "(*) epoch 3, cost 1.145\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [6.42590226 5.44130261 6.07443595 4.61869126 2.84860102 5.12695706\n",
      " 5.44130261 5.60329091 4.61869126 5.44130261 4.61869126 4.61869126\n",
      " 2.84860102 5.44130261 5.44130261 4.61869126 5.44130261 5.60329091\n",
      " 4.61869126 4.61869126 5.91244765 4.61869126 6.42590226 5.44130261\n",
      " 4.61869126 5.0898363  5.60329091 5.44130261 4.61869126 4.61869126\n",
      " 5.44130261 2.84860102 5.44130261 4.61869126 4.61869126 3.67121237\n",
      " 4.61869126 4.61869126 4.61869126 4.61869126 3.67121237 4.61869126\n",
      " 4.61869126 2.84860102 6.42590226 4.61869126 6.42590226 4.61869126\n",
      " 5.44130261 6.42590226 4.61869126 4.61869126 4.61869126 4.61869126\n",
      " 5.44130261 4.61869126 3.69463226 4.61869126 5.44130261 5.44130261\n",
      " 4.61869126 4.61869126 5.44130261 4.61869126 5.91244765 6.521296\n",
      " 4.61869126 5.44130261 5.44130261 6.42590226 5.44130261 4.61869126\n",
      " 4.61869126 6.42590226 4.61869126 4.61869126 3.67121237 2.84860102\n",
      " 4.61869126 5.91244765 5.44130261 6.42590226 4.61869126 6.42590226\n",
      " 5.60329091 6.07443595 6.42590226 5.0898363  5.0898363  6.42590226\n",
      " 4.61869126 5.44130261 5.44130261 6.42590226 4.61869126 6.42590226\n",
      " 5.44130261 3.67121237 4.61869126 6.42590226]\n",
      "all_sum is: [2.16493116 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 2.16493116 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 2.16493116 2.00475106 1.60878147 2.16493116 1.60878147\n",
      " 1.60878147 1.60878147 2.16493116 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 2.16493116 2.00475106\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 2.56090075 1.60878147 1.60878147 1.60878147 2.00475106\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 2.00475106 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 2.16493116\n",
      " 1.60878147 2.16493116 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 2.00475106 2.16493116 1.60878147 2.00475106 1.60878147 2.00475106\n",
      " 1.60878147 1.60878147 1.60878147 2.16493116 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147 1.60878147\n",
      " 1.60878147 1.60878147 1.60878147 2.00475106]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 2, cost 3.357\n",
      "(*) epoch 3, cost 2.983\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.059\n",
      "(*) epoch 2, cost 0.920\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.93084719 0.89750186 3.19715837 3.19715837 2.16381303 1.7546159\n",
      " 1.93084719 3.729581   1.93084719 1.0508212  2.16381303 2.08416654\n",
      " 1.93084719 2.16381303 3.729581   1.93084719 0.89750186 2.16381303\n",
      " 0.89750186 1.93084719 2.16381303 0.89750186 1.0508212  1.93084719\n",
      " 0.89750186 2.16381303 0.89750186 1.93084719 2.16381303 3.02092707\n",
      " 1.93084719 2.16381303 1.93084719 0.89750186 2.16381303 1.93084719\n",
      " 1.93084719 1.93084719 0.89750186 2.46326982 2.16381303 1.93084719\n",
      " 1.93084719 3.19715837 3.19715837 0.89750186 3.19715837 3.19715837\n",
      " 0.89750186 1.93084719 3.19715837 4.76292634 3.19715837 4.76292634\n",
      " 1.93084719 3.19715837 1.93084719 0.89750186 1.7546159  1.7546159\n",
      " 3.19715837 0.89750186 4.91624568 1.93084719 1.93084719 0.72127056\n",
      " 3.19715837 1.93084719 1.7546159  1.93084719 3.19715837 1.90793525\n",
      " 2.16381303 2.16381303 0.89750186 0.89750186 0.89750186 1.93084719\n",
      " 1.98758174 3.02092707 1.93084719 0.89750186 0.89750186 3.49661516\n",
      " 3.19715837 3.19715837 2.16381303 1.93084719 0.89750186 1.93084719\n",
      " 2.16381303 0.89750186 0.89750186 0.89750186 3.19715837 1.0508212\n",
      " 1.98758174 3.02092707 1.93084719 0.89750186]\n",
      "all_sum is: [1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 0.16643182 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 2.73377551 1.90444399 1.90444399 1.90444399 1.90444399 2.73377551\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 2.73377551\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 2.73377551\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 2.73377551 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399 1.90444399\n",
      " 1.90444399 1.90444399 1.90444399 1.90444399]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.685\n",
      "(*) epoch 2, cost 2.847\n",
      "(*) epoch 3, cost 2.465\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.694\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.74475239 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.74475239 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.21502502 1.10498049 1.10498049 1.21502502 1.21502502 1.10498049\n",
      " 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049 1.21502502\n",
      " 1.10498049 1.21502502 1.21502502 1.10498049 1.10498049 1.21502502\n",
      " 1.21502502 1.21502502 1.10498049 1.21502502 1.21502502 1.21502502\n",
      " 1.21502502 1.21502502 1.10498049 1.21502502 1.10498049 1.10498049\n",
      " 1.21502502 1.21502502 1.10498049 1.21502502 1.21502502 1.10498049\n",
      " 1.21502502 1.10498049 1.10498049 1.10498049 1.21502502 1.21502502\n",
      " 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049 1.21502502\n",
      " 1.10498049 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049\n",
      " 1.21502502 1.21502502 1.10498049 1.21502502 1.21502502 1.21502502\n",
      " 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049 1.21502502\n",
      " 1.10498049 1.21502502 1.10498049 1.21502502 1.21502502 1.10498049\n",
      " 1.10498049 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049\n",
      " 1.21502502 1.10498049 1.10498049 1.10498049 1.10498049 1.21502502\n",
      " 1.10498049 1.10498049 1.10498049 1.21502502 1.21502502 1.21502502\n",
      " 1.21502502 1.21502502 1.21502502 1.21502502 1.10498049 1.21502502\n",
      " 1.21502502 1.10498049 1.10498049 1.21502502]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.244\n",
      "(*) epoch 2, cost 0.484\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.533\n",
      "(*) epoch 2, cost 1.530\n",
      "(*) epoch 3, cost 1.235\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.77130261 1.77130261 1.11815503 1.11815503 1.11815503 1.11815503\n",
      " 1.91153365 1.11815503 1.11815503 1.11815503 1.11815503 0.977924\n",
      " 1.91153365 1.77130261 1.11815503 0.977924   1.91153365 1.11815503\n",
      " 0.977924   0.977924   1.91153365 0.977924   1.11815503 0.977924\n",
      " 1.77130261 0.977924   1.11815503 1.11815503 1.77130261 0.977924\n",
      " 1.11815503 1.91153365 0.977924   1.11815503 0.977924   1.11815503\n",
      " 1.11815503 1.11815503 1.11815503 1.11815503 1.77130261 1.11815503\n",
      " 1.11815503 1.77130261 0.977924   1.11815503 1.91153365 0.977924\n",
      " 1.11815503 1.11815503 0.977924   1.77130261 1.11815503 1.11815503\n",
      " 1.91153365 1.11815503 0.977924   1.11815503 1.11815503 1.11815503\n",
      " 0.977924   0.977924   1.11815503 0.977924   1.91153365 1.11815503\n",
      " 1.91153365 1.91153365 1.77130261 1.11815503 1.11815503 1.77130261\n",
      " 1.91153365 1.91153365 1.11815503 0.977924   1.11815503 0.977924\n",
      " 0.977924   1.91153365 0.977924   0.977924   1.77130261 1.11815503\n",
      " 1.11815503 1.11815503 1.91153365 1.77130261 1.11815503 1.11815503\n",
      " 1.77130261 1.11815503 0.977924   1.91153365 1.11815503 1.11815503\n",
      " 0.977924   0.977924   1.91153365 1.11815503]\n",
      "all_sum is: [1.22743563 2.99075946 2.53153005 1.41927833 2.53153005 2.89055653\n",
      " 1.41927833 2.53153005 3.01541839 1.22743563 1.22743563 1.22743563\n",
      " 1.41927833 1.22743563 2.53153005 4.12767011 2.72337276 4.12767011\n",
      " 3.01541839 1.22743563 2.53153005 2.72337276 2.69871382 4.12767011\n",
      " 1.41927833 2.53153005 1.22743563 1.41927833 1.41927833 1.41927833\n",
      " 1.41927833 1.22743563 1.41927833 2.53153005 1.41927833 1.22743563\n",
      " 2.53153005 1.30688674 2.72337276 2.72337276 2.82357569 2.72337276\n",
      " 2.82357569 1.22743563 1.41927833 1.22743563 1.5864621  1.49872944\n",
      " 1.3946194  1.22743563 3.01541839 1.22743563 1.22743563 2.53153005\n",
      " 2.53153005 2.53153005 2.69871382 1.22743563 4.29485388 2.72337276\n",
      " 1.3946194  1.22743563 2.53153005 1.41927833 2.53153005 1.5864621\n",
      " 1.41927833 2.53153005 2.72337276 2.53153005 2.53153005 1.41927833\n",
      " 2.72337276 1.22743563 1.41927833 2.72337276 2.53153005 2.53153005\n",
      " 2.72337276 2.53153005 2.72337276 2.72337276 1.22743563 2.53153005\n",
      " 2.72337276 2.69871382 2.53153005 2.53153005 1.41927833 2.72337276\n",
      " 1.41927833 1.3946194  1.41927833 2.72337276 1.41927833 1.22743563\n",
      " 1.22743563 1.22743563 2.53153005 2.53153005]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.180\n",
      "(*) epoch 2, cost 3.226\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.355\n",
      "(*) epoch 2, cost 2.808\n",
      "(*) epoch 3, cost 2.542\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.48027257 2.19685674 1.48027257 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257 2.19685674\n",
      " 2.19685674 2.19685674 1.48027257 1.48027257 1.48027257 2.19685674\n",
      " 1.48027257 1.48027257 1.48027257 2.19685674 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 2.19685674 1.48027257\n",
      " 1.48027257 2.19685674 1.48027257 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257 2.19685674\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 2.19685674 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257\n",
      " 2.19685674 1.48027257 2.19685674 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 2.19685674 1.48027257 1.48027257 2.19685674\n",
      " 1.48027257 1.48027257 2.19685674 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 2.19685674 1.48027257 2.19685674 1.48027257 1.48027257\n",
      " 2.19685674 1.48027257 2.19685674 1.48027257 1.48027257 1.48027257\n",
      " 1.48027257 1.48027257 1.48027257 1.48027257]\n",
      "all_sum is: [0.         0.         0.08504365 0.08504365 0.08504365 0.08504365\n",
      " 0.10849164 0.         0.08504365 0.08504365 0.         0.19353528\n",
      " 0.         0.08504365 0.08504365 0.08504365 0.19353528 0.08504365\n",
      " 0.08504365 0.         0.08504365 0.08504365 0.44884665 0.19353528\n",
      " 0.08504365 0.         0.08504365 0.08504365 0.19353528 0.08504365\n",
      " 0.         0.         0.58610347 0.08504365 0.         0.08504365\n",
      " 0.         0.19353528 0.08504365 0.08504365 0.19353528 0.08504365\n",
      " 0.         0.08504365 0.08504365 0.08504365 0.08504365 0.08504365\n",
      " 0.08504365 0.08504365 0.08504365 0.         0.08504365 0.\n",
      " 0.         0.08504365 0.08504365 0.08504365 0.         0.08504365\n",
      " 0.08504365 0.08504365 0.08504365 0.         0.         0.\n",
      " 0.19353528 0.08504365 0.08504365 0.10849164 0.08504365 0.08504365\n",
      " 0.         0.08504365 0.08504365 0.08504365 0.08504365 0.08504365\n",
      " 0.08504365 0.08504365 0.         0.08504365 0.         0.19353528\n",
      " 0.         0.08504365 0.08504365 0.         0.08504365 0.08504365\n",
      " 0.08504365 0.08504365 0.         0.         0.08504365 0.08504365\n",
      " 0.58610347 0.         0.08504365 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.016\n",
      "(*) epoch 2, cost 1.510\n",
      "(*) epoch 3, cost 1.275\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.253\n",
      "(*) epoch 2, cost 2.146\n",
      "(*) epoch 3, cost 1.846\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.46883207 0.46883207 0.22132013 0.46883207 0.46883207 0.46883207\n",
      " 0.46883207 0.46883207 0.46883207 0.22132013 0.46883207 0.46883207\n",
      " 0.46883207 0.46883207 0.46883207 0.22132013 0.46883207 0.46883207\n",
      " 0.22132013 0.46883207 0.46883207 0.22132013 0.22132013 0.46883207\n",
      " 0.22132013 0.46883207 0.46883207 0.46883207 0.22132013 0.22132013\n",
      " 0.46883207 0.46883207 0.22132013 0.22132013 0.46883207 0.22132013\n",
      " 0.22132013 0.22132013 0.46883207 0.46883207 0.46883207 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207 0.46883207 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207 0.46883207 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207 0.46883207 0.46883207\n",
      " 0.22132013 0.46883207 0.46883207 0.22132013 0.46883207 0.22132013\n",
      " 0.46883207 0.22132013 0.46883207 0.46883207 0.22132013 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207 0.22132013 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207 0.22132013 0.22132013\n",
      " 0.22132013 0.46883207 0.22132013 0.46883207 0.46883207 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.22132013 0.46883207 0.22132013\n",
      " 0.46883207 0.46883207 0.46883207 0.46883207]\n",
      "all_sum is: [0.30518571 0.30518571 0.30518571 0.30518571 0.         0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.         0.30518571\n",
      " 0.         0.30518571 0.         0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.         0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.         0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.         0.30518571 0.30518571\n",
      " 0.         0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571 0.30518571\n",
      " 0.30518571 0.30518571 0.30518571 0.30518571]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.816\n",
      "(*) epoch 2, cost 2.563\n",
      "(*) epoch 3, cost 2.160\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.452\n",
      "(*) epoch 2, cost 2.521\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.93459021 0.25630933 1.25869356 1.85154903 1.85154903 1.85154903\n",
      " 0.25630933 0.84916479 1.25869356 0.93459021 0.84916479 0.84916479\n",
      " 0.25630933 0.84916479 1.25869356 1.25869356 0.93459021 1.85154903\n",
      " 0.25630933 0.25630933 0.84916479 1.85154903 0.25630933 1.25869356\n",
      " 0.34173475 0.84916479 0.25630933 0.25630933 1.34411898 0.34173475\n",
      " 0.25630933 0.25630933 0.93459021 1.85154903 0.34173475 0.84916479\n",
      " 1.25869356 0.84916479 0.84916479 1.34411898 0.25630933 1.34411898\n",
      " 0.25630933 0.84916479 1.34411898 0.25630933 1.25869356 0.84916479\n",
      " 0.93459021 0.84916479 1.85154903 0.84916479 1.25869356 0.25630933\n",
      " 0.25630933 1.85154903 0.84916479 0.25630933 1.25869356 1.25869356\n",
      " 0.25630933 1.25869356 0.84916479 1.85154903 1.85154903 0.84916479\n",
      " 1.25869356 0.25630933 0.84916479 1.25869356 0.84916479 0.25630933\n",
      " 1.25869356 0.84916479 1.85154903 0.84916479 0.34173475 0.25630933\n",
      " 0.84916479 0.84916479 1.85154903 0.93459021 0.84916479 0.84916479\n",
      " 0.25630933 1.25869356 0.25630933 0.93459021 0.25630933 2.1955524\n",
      " 0.84916479 0.25630933 0.25630933 0.25630933 1.25869356 0.25630933\n",
      " 0.25630933 0.25630933 0.25630933 0.25630933]\n",
      "all_sum is: [0.82430623 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.8651992  2.06428105 2.8651992  2.06428105 2.8651992  2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 1.62522438 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.8651992  2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.8651992  2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.8651992  2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.8651992  2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.8651992  2.8651992  2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105 2.06428105\n",
      " 2.06428105 2.06428105 2.06428105 2.06428105]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.378\n",
      "(*) epoch 2, cost 3.314\n",
      "(*) epoch 3, cost 3.046\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.523\n",
      "(*) epoch 2, cost 2.053\n",
      "(*) epoch 3, cost 1.740\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.83857396 1.4841378  1.83857396 1.83857396 1.83857396 1.83857396\n",
      " 1.83857396 1.4841378  1.4841378  1.83857396 1.4841378  1.4841378\n",
      " 1.83857396 1.83857396 1.83857396 1.83857396 1.4841378  1.4841378\n",
      " 1.4841378  1.4841378  1.83857396 1.4841378  1.83857396 1.83857396\n",
      " 1.83857396 1.83857396 1.4841378  1.4841378  1.4841378  1.83857396\n",
      " 1.4841378  1.83857396 1.83857396 1.83857396 1.4841378  1.83857396\n",
      " 1.83857396 1.4841378  1.83857396 1.83857396 1.83857396 1.4841378\n",
      " 1.83857396 1.4841378  1.4841378  1.83857396 1.83857396 1.4841378\n",
      " 1.83857396 1.4841378  1.4841378  1.83857396 1.4841378  1.83857396\n",
      " 1.4841378  1.4841378  1.83857396 1.83857396 1.83857396 1.83857396\n",
      " 1.83857396 1.4841378  1.4841378  1.4841378  1.83857396 1.83857396\n",
      " 1.83857396 1.4841378  1.83857396 1.83857396 1.4841378  1.83857396\n",
      " 1.4841378  1.83857396 1.4841378  1.83857396 1.4841378  1.4841378\n",
      " 1.83857396 1.4841378  1.4841378  1.83857396 1.4841378  1.83857396\n",
      " 1.4841378  1.83857396 1.83857396 1.4841378  1.83857396 1.83857396\n",
      " 1.83857396 1.83857396 1.83857396 1.4841378  1.83857396 1.83857396\n",
      " 1.83857396 1.83857396 1.83857396 1.83857396]\n",
      "all_sum is: [1.12146415 1.12146415 1.12146415 1.12146415 1.12146415 1.12146415\n",
      " 2.06722351 1.12146415 2.06722351 1.12146415 1.12146415 2.06722351\n",
      " 2.06722351 1.12146415 1.12146415 2.06722351 1.12146415 2.06722351\n",
      " 1.12146415 1.12146415 1.12146415 1.12146415 1.12146415 2.06722351\n",
      " 1.12146415 2.06722351 1.12146415 1.12146415 1.12146415 1.12146415\n",
      " 1.12146415 2.06722351 1.12146415 2.06722351 1.12146415 1.12146415\n",
      " 2.06722351 2.06722351 2.06722351 1.12146415 1.12146415 1.12146415\n",
      " 1.12146415 1.12146415 1.12146415 2.06722351 1.12146415 1.12146415\n",
      " 2.06722351 1.12146415 1.12146415 1.12146415 1.12146415 1.12146415\n",
      " 2.06722351 1.12146415 1.12146415 1.12146415 2.06722351 1.12146415\n",
      " 1.12146415 1.12146415 1.12146415 1.12146415 1.12146415 1.12146415\n",
      " 1.12146415 2.06722351 1.12146415 2.06722351 1.12146415 2.06722351\n",
      " 1.12146415 2.06722351 1.12146415 2.06722351 1.12146415 1.12146415\n",
      " 1.12146415 1.12146415 1.12146415 2.06722351 1.12146415 1.12146415\n",
      " 2.06722351 1.12146415 2.06722351 1.12146415 2.06722351 1.12146415\n",
      " 1.12146415 2.06722351 1.12146415 1.12146415 1.12146415 1.12146415\n",
      " 1.12146415 1.12146415 1.12146415 2.06722351]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.816\n",
      "(*) epoch 2, cost 2.621\n",
      "(*) epoch 3, cost 2.263\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.637\n",
      "(*) epoch 2, cost 2.381\n",
      "(*) epoch 3, cost 2.004\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.89329133 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052 0.02750052\n",
      " 0.02750052 0.02750052 0.02750052 0.02750052]\n",
      "all_sum is: [1.47357866 1.47357866 1.92685497 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.92685497 1.47357866\n",
      " 1.47357866 1.47357866 1.92685497 1.92685497 1.92685497 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.92685497\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.92685497 1.92685497 1.92685497 1.47357866 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.92685497 1.47357866 1.47357866 1.92685497 1.47357866\n",
      " 1.92685497 1.47357866 1.47357866 1.47357866 1.47357866 1.92685497\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 0.89285909\n",
      " 1.47357866 1.47357866 1.47357866 1.92685497 1.47357866 1.92685497\n",
      " 1.47357866 1.92685497 1.47357866 1.47357866 1.47357866 1.92685497\n",
      " 1.47357866 1.92685497 1.92685497 1.92685497 1.47357866 1.92685497\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866 1.47357866\n",
      " 1.47357866 1.47357866 1.47357866 1.47357866]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.038\n",
      "(*) epoch 2, cost 0.568\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.852\n",
      "(*) epoch 2, cost 1.662\n",
      "(*) epoch 3, cost 1.323\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.225677   1.58444181 1.58444181 1.58444181 1.225677   1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.225677   1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.225677   1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.225677   1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.96311047 1.58444181 1.225677   1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.225677   1.58444181\n",
      " 1.58444181 1.58444181 1.225677   1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.225677   1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.225677   1.58444181 1.225677   1.225677\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181 1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.96311047 1.225677   1.58444181\n",
      " 1.58444181 1.58444181 1.58444181 1.58444181]\n",
      "all_sum is: [3.82807357 3.82807357 3.82807357 3.82807357 3.82807357 3.97840846\n",
      " 3.97840846 3.82807357 3.82807357 3.97840846 3.82807357 3.82807357\n",
      " 3.82807357 4.4230004  3.82807357 3.97840846 3.97840846 3.97840846\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357 3.97840846 3.82807357\n",
      " 3.82807357 3.82807357 3.97840846 3.82807357 3.82807357 3.82807357\n",
      " 3.82807357 3.97840846 3.82807357 3.82807357 3.82807357 3.97840846\n",
      " 3.82807357 3.97840846 3.97840846 3.82807357 3.97840846 3.82807357\n",
      " 3.82807357 3.82807357 3.97840846 3.82807357 3.82807357 3.97840846\n",
      " 3.82807357 3.97840846 3.97840846 3.82807357 3.82807357 3.82807357\n",
      " 3.97840846 3.82807357 3.82807357 3.82807357 3.97840846 3.82807357\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357 4.27266551 3.82807357\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357\n",
      " 3.82807357 3.97840846 3.82807357 3.97840846 3.82807357 3.82807357\n",
      " 3.82807357 3.97840846 3.82807357 3.82807357 3.82807357 3.97840846\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357 3.82807357\n",
      " 3.82807357 3.82807357 3.82807357 3.82807357]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.002\n",
      "(*) epoch 2, cost 3.121\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.889\n",
      "(*) epoch 2, cost 2.911\n",
      "(*) epoch 3, cost 2.693\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.10032198 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.10032198 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.10032198 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.10032198\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.99275936\n",
      " 0.10032198 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.10032198 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.99275936 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.10032198 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.10032198 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.10032198 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844 0.50817844\n",
      " 0.50817844 0.50817844 0.50817844 0.50817844]\n",
      "all_sum is: [1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.83823066 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 0.66541338 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 0.66541338 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 0.66541338 1.31632719 1.31632719\n",
      " 1.31632719 1.31632719 1.31632719 1.31632719]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.204\n",
      "(*) epoch 2, cost 0.822\n",
      "(*) epoch 3, cost 0.509\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.040\n",
      "(*) epoch 2, cost 1.426\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.16467483 4.16467483 3.20210249 4.16467483 3.20210249 4.16467483\n",
      " 3.20210249 4.16467483 2.02351555 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 3.20210249 4.16467483 4.16467483 2.02351555 4.16467483\n",
      " 2.02351555 3.20210249 4.16467483 4.16467483 1.06094321 2.02351555\n",
      " 3.20210249 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 1.06094321 2.02351555 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 2.02351555 4.16467483 4.16467483 3.20210249 4.16467483\n",
      " 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 4.16467483 2.02351555 4.16467483 4.16467483 2.02351555\n",
      " 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 2.02351555 4.16467483 4.16467483 4.16467483 4.16467483 3.20210249\n",
      " 4.16467483 4.16467483 4.16467483 4.16467483 2.02351555 4.16467483\n",
      " 2.02351555 2.02351555 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 4.16467483 2.02351555 2.02351555 4.16467483 4.16467483\n",
      " 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483 4.16467483\n",
      " 4.16467483 3.20210249 2.02351555 4.16467483]\n",
      "all_sum is: [2.94100089 3.10573984 2.94100089 2.94100089 2.94100089 3.10573984\n",
      " 1.7642582  3.10573984 2.94100089 1.7642582  3.10573984 3.10573984\n",
      " 2.33197996 3.10573984 2.94100089 4.85113825 2.94100089 1.59951925\n",
      " 2.94100089 1.7642582  2.94100089 2.94100089 3.10573984 2.94100089\n",
      " 2.94100089 1.59951925 2.94100089 3.10573984 3.10573984 3.10573984\n",
      " 3.10573984 3.10573984 2.94100089 2.94100089 3.10573984 3.10573984\n",
      " 3.10573984 3.10573984 2.94100089 3.10573984 2.94100089 2.33197996\n",
      " 3.10573984 3.10573984 4.44815544 2.94100089 3.10573984 2.94100089\n",
      " 3.10573984 1.7642582  3.10573984 2.94100089 3.10573984 2.94100089\n",
      " 3.10573984 2.94100089 4.44815544 2.94100089 3.10573984 3.10573984\n",
      " 3.10573984 3.10573984 2.94100089 3.6734616  2.94100089 3.10573984\n",
      " 3.10573984 3.10573984 3.10573984 3.10573984 3.10573984 2.94100089\n",
      " 3.10573984 2.94100089 3.10573984 3.10573984 2.94100089 3.10573984\n",
      " 3.10573984 3.10573984 3.10573984 3.10573984 3.10573984 3.10573984\n",
      " 1.7642582  2.94100089 2.94100089 3.10573984 1.7642582  3.10573984\n",
      " 3.10573984 3.10573984 2.94100089 1.7642582  1.7642582  2.94100089\n",
      " 2.94100089 3.10573984 4.44815544 3.10573984]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.763\n",
      "(*) epoch 2, cost 0.924\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 10\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.355\n",
      "(*) epoch 2, cost 3.581\n",
      "(*) epoch 3, cost 3.170\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.17905309 1.24047203 1.67293983 2.61152089 1.24047203 1.24047203\n",
      " 1.67293983 1.24047203 1.24047203 1.24047203 2.17905309 1.67293983\n",
      " 1.67293983 1.24047203 2.17905309 1.67293983 2.17905309 2.17905309\n",
      " 2.17905309 1.67293983 1.67293983 2.17905309 2.17905309 1.67293983\n",
      " 1.24047203 2.78677897 1.67293983 1.67293983 1.24047203 2.61152089\n",
      " 2.17905309 1.67293983 1.67293983 2.17905309 1.24047203 1.67293983\n",
      " 1.24047203 1.67293983 2.17905309 2.61152089 1.67293983 1.24047203\n",
      " 1.24047203 1.67293983 2.17905309 2.17905309 1.24047203 2.17905309\n",
      " 2.17905309 1.24047203 2.17905309 1.67293983 1.24047203 1.24047203\n",
      " 2.17905309 2.17905309 1.24047203 1.8481979  1.24047203 1.67293983\n",
      " 1.24047203 1.24047203 1.24047203 2.17905309 1.24047203 1.24047203\n",
      " 1.24047203 2.17905309 1.24047203 1.67293983 1.24047203 1.24047203\n",
      " 2.17905309 1.24047203 2.61152089 1.67293983 1.67293983 1.24047203\n",
      " 1.67293983 2.17905309 1.24047203 1.24047203 1.67293983 1.4157301\n",
      " 1.24047203 1.24047203 1.24047203 2.17905309 2.17905309 1.67293983\n",
      " 1.24047203 2.17905309 2.17905309 1.24047203 2.17905309 1.24047203\n",
      " 1.67293983 1.67293983 2.17905309 1.24047203]\n",
      "all_sum is: [1.61055168 0.85145697 0.61434153 1.61055168 1.37343624 1.37343624\n",
      " 1.37343624 2.09142807 2.09142807 0.61434153 0.85145697 2.09142807\n",
      " 1.61055168 1.61055168 1.61055168 1.61055168 2.09142807 0.61434153\n",
      " 1.61055168 1.33233335 1.85431262 0.85145697 0.85145697 1.61055168\n",
      " 1.85431262 0.85145697 1.61055168 1.61055168 1.85431262 1.61055168\n",
      " 1.61055168 1.61055168 1.61055168 0.85145697 1.61055168 0.61434153\n",
      " 1.61055168 1.37343624 1.37343624 1.61055168 1.61055168 1.61055168\n",
      " 1.61055168 0.85145697 2.09142807 0.85145697 1.61055168 1.37343624\n",
      " 1.61055168 1.61055168 1.37343624 1.37343624 0.85145697 1.37343624\n",
      " 1.37343624 2.09142807 1.61055168 1.61055168 1.85431262 1.37343624\n",
      " 1.37343624 2.09142807 1.37343624 0.61434153 0.61434153 0.85145697\n",
      " 1.61055168 1.37343624 2.09142807 0.61434153 1.37343624 2.09142807\n",
      " 0.61434153 1.61055168 1.61055168 1.61055168 1.85431262 2.09142807\n",
      " 1.61055168 0.61434153 2.09142807 1.85431262 1.61055168 1.61055168\n",
      " 1.61055168 0.85145697 2.09142807 1.37343624 2.09142807 1.33233335\n",
      " 1.61055168 2.09142807 0.61434153 1.37343624 1.61055168 2.09142807\n",
      " 1.37343624 1.37343624 1.61055168 1.61055168]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.814\n",
      "(*) epoch 2, cost 2.325\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.207\n",
      "(*) epoch 2, cost 3.519\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 3.99623764\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264 2.67867264\n",
      " 2.67867264 2.67867264 2.67867264 2.67867264]\n",
      "all_sum is: [1.06337048 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 2.90437492 1.06337048 1.06337048\n",
      " 2.90437492 1.06337048 1.06337048 2.90437492 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 2.90437492 3.5425262  2.90437492 2.90437492\n",
      " 1.06337048 2.90437492 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 2.90437492 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 1.06337048 2.90437492 2.90437492\n",
      " 1.06337048 1.06337048 1.06337048 2.90437492 1.06337048 1.06337048\n",
      " 2.90437492 1.06337048 1.70152176 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 1.06337048 2.90437492 1.06337048\n",
      " 2.90437492 2.90437492 2.90437492 1.06337048 2.90437492 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 2.90437492 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 1.70152176 2.90437492 1.06337048 5.14837757\n",
      " 2.90437492 2.90437492 1.06337048 1.06337048 1.06337048 1.06337048\n",
      " 1.06337048 1.06337048 2.90437492 2.90437492]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.022\n",
      "(*) epoch 2, cost 1.047\n",
      "(*) epoch 3, cost 0.768\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.495\n",
      "(*) epoch 2, cost 2.286\n",
      "(*) epoch 3, cost 1.968\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.70204732 1.15567861 0.70204732 0.70204732 0.70204732 1.15567861\n",
      " 1.15567861 0.70204732 1.15567861 1.15567861 0.70204732 0.70204732\n",
      " 1.15567861 1.15567861 0.70204732 1.15567861 0.70204732 0.70204732\n",
      " 1.15567861 1.15567861 1.15567861 0.70204732 0.70204732 0.70204732\n",
      " 0.70204732 1.15567861 0.70204732 0.70204732 0.70204732 1.15567861\n",
      " 0.70204732 0.70204732 0.70204732 1.15567861 0.70204732 1.15567861\n",
      " 0.70204732 1.15567861 0.70204732 1.15567861 1.15567861 0.70204732\n",
      " 0.70204732 1.15567861 1.15567861 0.70204732 0.70204732 0.70204732\n",
      " 0.70204732 1.15567861 0.70204732 0.70204732 1.15567861 1.15567861\n",
      " 1.15567861 1.15567861 1.15567861 1.15567861 1.15567861 1.15567861\n",
      " 0.70204732 1.15567861 1.15567861 0.70204732 0.70204732 1.15567861\n",
      " 1.15567861 1.15567861 0.70204732 0.70204732 1.15567861 0.70204732\n",
      " 0.70204732 0.70204732 0.70204732 1.15567861 1.15567861 0.70204732\n",
      " 0.70204732 0.70204732 1.15567861 0.70204732 1.15567861 1.15567861\n",
      " 1.15567861 0.70204732 1.15567861 1.15567861 0.70204732 0.70204732\n",
      " 0.70204732 1.15567861 0.70204732 0.70204732 0.70204732 0.70204732\n",
      " 0.70204732 0.70204732 0.70204732 0.70204732]\n",
      "all_sum is: [1.05270389 0.58169122 1.05270389 1.05270389 0.58169122 1.05270389\n",
      " 1.05270389 0.58169122 0.58169122 1.05270389 1.05270389 0.58169122\n",
      " 0.58169122 1.05270389 0.58169122 1.05270389 1.05270389 0.58169122\n",
      " 0.58169122 0.58169122 1.05270389 1.05270389 0.58169122 0.58169122\n",
      " 0.58169122 0.58169122 0.58169122 0.58169122 0.58169122 1.05270389\n",
      " 1.05270389 1.05270389 0.58169122 1.05270389 1.05270389 0.58169122\n",
      " 0.58169122 0.58169122 0.58169122 1.05270389 1.05270389 0.58169122\n",
      " 1.05270389 1.05270389 0.58169122 0.58169122 1.05270389 0.58169122\n",
      " 1.05270389 0.58169122 1.05270389 1.05270389 0.58169122 1.05270389\n",
      " 0.58169122 0.58169122 1.05270389 0.58169122 0.58169122 1.05270389\n",
      " 1.05270389 0.58169122 1.05270389 1.05270389 0.58169122 0.58169122\n",
      " 1.05270389 0.58169122 1.05270389 1.05270389 1.05270389 1.05270389\n",
      " 1.05270389 0.58169122 0.58169122 1.05270389 1.05270389 0.58169122\n",
      " 1.05270389 0.58169122 1.05270389 0.58169122 0.58169122 1.05270389\n",
      " 1.05270389 1.05270389 1.05270389 1.05270389 1.05270389 0.58169122\n",
      " 0.58169122 0.58169122 1.05270389 0.58169122 0.58169122 0.58169122\n",
      " 0.58169122 0.58169122 1.05270389 0.58169122]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.747\n",
      "(*) epoch 2, cost 1.942\n",
      "(*) epoch 3, cost 1.610\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.898\n",
      "(*) epoch 2, cost 1.742\n",
      "(*) epoch 3, cost 1.293\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.10527421 1.23466576 1.23466576 1.9714464  1.9714464  1.9714464\n",
      " 2.10527421 1.23466576 1.9714464  1.36849358 1.36849358 2.10527421\n",
      " 1.36849358 2.10527421 1.05845817 1.9714464  2.10527421 1.36849358\n",
      " 1.36849358 1.19228598 2.10527421 0.96062949 0.22384886 1.36849358\n",
      " 2.10527421 2.10527421 1.23466576 1.23466576 1.9714464  2.10527421\n",
      " 2.10527421 1.36849358 1.9714464  2.20310289 0.96062949 0.22384886\n",
      " 1.23466576 2.10527421 1.46632226 1.36849358 2.06927508 2.10527421\n",
      " 1.23466576 2.10527421 1.36849358 1.36849358 0.22384886 2.10527421\n",
      " 1.9714464  2.10527421 0.45550535 1.36849358 2.10527421 1.36849358\n",
      " 1.23466576 2.10527421 1.23466576 2.10527421 1.46632226 2.10527421\n",
      " 1.36849358 1.23466576 1.36849358 1.36849358 0.22384886 1.0944573\n",
      " 2.10527421 1.36849358 2.10527421 1.23466576 1.23466576 1.9714464\n",
      " 2.20310289 1.36849358 1.23466576 1.36849358 1.36849358 1.9714464\n",
      " 1.36849358 1.0944573  1.36849358 1.36849358 1.9714464  1.23466576\n",
      " 1.23466576 1.36849358 1.36849358 1.46632226 1.9714464  0.96062949\n",
      " 1.19228598 1.9714464  1.23466576 1.36849358 2.10527421 1.9714464\n",
      " 1.9714464  2.10527421 1.9714464  1.36849358]\n",
      "all_sum is: [2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 3.72278914 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 3.19766809\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 3.72278914 2.89131554 2.89131554 2.89131554 3.19766809\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 3.19766809 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 3.72278914 2.89131554 2.89131554 2.89131554\n",
      " 2.89131554 2.89131554 2.89131554 2.89131554]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.972\n",
      "(*) epoch 2, cost 4.209\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.590\n",
      "(*) epoch 2, cost 1.852\n",
      "(*) epoch 3, cost 1.610\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  4.28997892 2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.26428987 2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.26428987\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  4.28997892 2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  4.28997892 2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334  2.9373334  2.9373334\n",
      " 2.9373334  2.9373334  2.9373334  2.9373334 ]\n",
      "all_sum is: [2.01227935 3.76931695 2.01227935 2.01227935 2.01227935 2.01227935\n",
      " 2.01227935 2.01227935 2.01227935 3.76931695 3.76931695 2.01227935\n",
      " 3.76931695 3.76931695 2.01227935 2.01227935 2.01227935 3.76931695\n",
      " 2.01227935 3.76931695 2.01227935 2.64114557 3.76931695 3.76931695\n",
      " 2.64114557 3.76931695 3.76931695 3.76931695 2.01227935 2.01227935\n",
      " 1.88570475 3.76931695 3.76931695 3.76931695 2.01227935 2.01227935\n",
      " 2.01227935 3.76931695 2.01227935 2.01227935 2.01227935 2.01227935\n",
      " 4.39818316 3.76931695 3.76931695 3.76931695 3.76931695 2.01227935\n",
      " 2.01227935 2.01227935 3.76931695 2.01227935 3.76931695 2.01227935\n",
      " 2.01227935 2.01227935 3.76931695 2.01227935 2.01227935 2.01227935\n",
      " 2.01227935 3.64274235 2.01227935 3.76931695 2.01227935 2.01227935\n",
      " 2.01227935 3.76931695 2.01227935 2.01227935 2.01227935 2.01227935\n",
      " 2.01227935 3.76931695 3.76931695 2.01227935 2.01227935 2.01227935\n",
      " 2.01227935 2.01227935 2.01227935 2.01227935 3.76931695 2.01227935\n",
      " 2.01227935 2.01227935 3.76931695 3.76931695 2.01227935 2.01227935\n",
      " 4.39818316 3.76931695 2.01227935 3.76931695 2.01227935 2.01227935\n",
      " 3.76931695 2.01227935 3.76931695 2.01227935]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.702\n",
      "(*) epoch 2, cost 1.445\n",
      "(*) epoch 3, cost 0.988\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.957\n",
      "(*) epoch 2, cost 1.676\n",
      "(*) epoch 3, cost 1.362\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 1.26684554 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 4.33425334 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 4.33425334\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 1.26684554 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 4.33425334 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 1.26684554 2.36034574\n",
      " 2.36034574 2.36034574 1.26684554 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 4.33425334\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574 1.26684554 2.36034574\n",
      " 3.24075314 2.36034574 2.36034574 2.36034574 2.36034574 2.36034574\n",
      " 2.36034574 2.36034574 2.36034574 2.36034574]\n",
      "all_sum is: [3.24096084 3.24096084 1.73854711 2.3088192  3.24096084 3.24096084\n",
      " 2.3088192  2.3088192  2.3088192  2.3088192  2.3088192  2.3088192\n",
      " 3.24096084 2.3088192  2.3088192  2.3088192  2.3088192  2.3088192\n",
      " 2.3088192  3.24096084 3.24096084 3.24096084 2.3088192  3.24096084\n",
      " 2.3088192  3.24096084 2.3088192  2.3088192  3.24096084 2.3088192\n",
      " 2.3088192  3.24096084 2.3088192  2.3088192  2.3088192  1.73854711\n",
      " 3.24096084 2.3088192  2.3088192  2.3088192  3.24096084 2.3088192\n",
      " 2.73323461 3.24096084 2.3088192  2.3088192  2.3088192  2.3088192\n",
      " 3.24096084 2.3088192  2.3088192  2.3088192  2.3088192  2.3088192\n",
      " 3.24096084 3.24096084 2.3088192  2.3088192  3.24096084 2.3088192\n",
      " 2.3088192  3.24096084 3.24096084 2.3088192  2.3088192  2.3088192\n",
      " 2.3088192  2.3088192  3.24096084 2.3088192  3.24096084 2.73323461\n",
      " 3.24096084 2.3088192  3.24096084 2.3088192  3.24096084 2.3088192\n",
      " 3.24096084 2.3088192  2.3088192  2.3088192  3.24096084 2.3088192\n",
      " 2.3088192  2.3088192  2.3088192  2.3088192  2.3088192  2.3088192\n",
      " 3.24096084 3.66537625 2.3088192  3.24096084 2.3088192  2.3088192\n",
      " 3.24096084 3.24096084 2.3088192  2.3088192 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.655\n",
      "(*) epoch 2, cost 2.320\n",
      "(*) epoch 3, cost 2.069\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.804\n",
      "(*) epoch 2, cost 2.009\n",
      "(*) epoch 3, cost 1.651\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.92213944 2.92213944 2.92213944 3.89648614 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 3.89648614 3.89648614 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944 3.89648614\n",
      " 2.92213944 3.89648614 3.89648614 2.92213944 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 3.89648614 2.92213944 2.92213944 3.89648614\n",
      " 2.92213944 3.89648614 2.92213944 2.92213944 2.92213944 2.92213944\n",
      " 3.89648614 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 2.92213944 4.5184331  3.89648614 3.89648614\n",
      " 2.92213944 2.92213944 2.92213944 2.92213944 3.89648614 2.92213944\n",
      " 3.89648614 2.92213944 2.92213944 2.92213944 3.89648614 2.92213944\n",
      " 2.92213944 2.92213944 3.89648614 2.92213944 2.14425923 2.92213944\n",
      " 2.92213944 3.89648614 2.92213944 2.92213944 5.4927798  2.92213944\n",
      " 2.92213944 2.92213944 3.89648614 3.74055289 2.92213944 2.92213944\n",
      " 3.89648614 2.92213944 2.92213944 2.92213944 2.92213944 2.92213944\n",
      " 2.92213944 2.92213944 2.92213944 3.89648614]\n",
      "all_sum is: [0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   1.2620125\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 1.2620125  0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   0.935791   0.935791   0.935791   0.935791\n",
      " 0.935791   0.935791   1.2620125  0.935791   0.935791   0.935791\n",
      " 0.935791   1.2620125  0.935791   0.935791   0.935791   0.37898099\n",
      " 0.935791   0.935791   0.935791   0.935791  ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.373\n",
      "(*) epoch 2, cost 0.867\n",
      "(*) epoch 3, cost 0.596\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.934\n",
      "(*) epoch 2, cost 2.474\n",
      "(*) epoch 3, cost 2.121\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.2711671\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.18215708 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         2.18215708 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.18215708 0.         0.         0.         0.\n",
      " 0.         0.2711671  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 3.66612593 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 3.66612593 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399 2.57916399\n",
      " 2.57916399 2.57916399 2.57916399 2.57916399]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.016\n",
      "(*) epoch 2, cost 0.468\n",
      "(*) epoch 3, cost 0.337\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.964\n",
      "(*) epoch 2, cost 1.589\n",
      "(*) epoch 3, cost 1.154\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.3253121  2.00686011 1.64035587 1.64035587 1.64035587 1.64035587\n",
      " 1.64035587 2.3253121  2.17359687 2.17359687 2.17359687 2.8585531\n",
      " 2.17359687 1.64035587 2.17359687 2.54010111 2.17359687 2.17359687\n",
      " 3.100082   1.64035587 2.54010111 2.17359687 1.64035587 2.3253121\n",
      " 2.25033006 2.17359687 2.17359687 2.17359687 2.17359687 2.8585531\n",
      " 1.64035587 2.78357106 2.17359687 2.00686011 2.93528629 2.17359687\n",
      " 2.17359687 2.17359687 1.64035587 2.17359687 2.78357106 2.8585531\n",
      " 1.64035587 1.64035587 2.17359687 2.3253121  2.17359687 2.8585531\n",
      " 3.22505734 2.17359687 1.64035587 2.17359687 2.17359687 2.17359687\n",
      " 3.02509996 1.64035587 1.64035587 2.17359687 2.8585531  2.41512577\n",
      " 1.64035587 2.17359687 2.17359687 1.64035587 2.17359687 2.61683431\n",
      " 2.17359687 2.3253121  2.8585531  1.64035587 2.3253121  2.17359687\n",
      " 1.64035587 2.78357106 2.17359687 1.64035587 3.1500753  2.17359687\n",
      " 1.64035587 1.64035587 2.8585531  2.3253121  1.64035587 1.64035587\n",
      " 2.17359687 1.64035587 2.3253121  1.64035587 2.78357106 2.78357106\n",
      " 2.17359687 2.3253121  2.8585531  1.64035587 1.64035587 2.3253121\n",
      " 2.3253121  1.64035587 1.64035587 3.46852729]\n",
      "all_sum is: [0.10063594 0.3809997  0.10063594 0.3809997  0.10063594 0.3809997\n",
      " 0.76896581 0.3809997  0.10063594 0.10063594 0.10063594 0.10063594\n",
      " 0.76896581 0.3809997  0.76896581 0.10063594 0.3809997  0.10063594\n",
      " 0.10063594 0.10063594 0.76896581 0.10063594 0.3809997  0.10063594\n",
      " 0.3809997  0.10063594 0.10063594 0.10063594 0.10063594 0.10063594\n",
      " 0.3809997  1.04932957 0.10063594 0.3809997  0.10063594 0.76896581\n",
      " 0.10063594 0.3809997  0.10063594 0.10063594 0.3809997  0.76896581\n",
      " 0.76896581 0.10063594 0.3809997  0.10063594 0.10063594 0.3809997\n",
      " 0.3809997  0.76896581 0.76896581 0.10063594 0.10063594 0.10063594\n",
      " 0.76896581 0.76896581 0.10063594 0.10063594 0.10063594 0.3809997\n",
      " 0.76896581 0.10063594 0.10063594 0.3809997  0.10063594 0.76896581\n",
      " 0.10063594 0.76896581 0.10063594 0.3809997  0.76896581 0.10063594\n",
      " 0.10063594 0.76896581 0.10063594 0.10063594 0.76896581 1.04932957\n",
      " 0.10063594 0.10063594 0.10063594 0.10063594 0.76896581 0.10063594\n",
      " 0.10063594 0.10063594 0.76896581 0.10063594 0.3809997  0.76896581\n",
      " 0.3809997  0.10063594 0.10063594 0.76896581 0.10063594 0.10063594\n",
      " 0.76896581 0.10063594 0.3809997  0.10063594]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.007\n",
      "(*) epoch 2, cost 2.020\n",
      "(*) epoch 3, cost 1.838\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.087\n",
      "(*) epoch 2, cost 1.657\n",
      "(*) epoch 3, cost 1.280\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 3.30332021 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629 2.60193629\n",
      " 2.60193629 2.60193629 2.60193629 2.60193629]\n",
      "all_sum is: [0.         0.         0.37058795 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.37058795 0.         0.         0.         0.37058795 0.37058795\n",
      " 0.         0.         0.         0.         0.37058795 0.\n",
      " 0.37058795 0.         0.         0.         0.         0.\n",
      " 0.37058795 0.         0.         0.37058795 0.         0.\n",
      " 0.         0.         0.         0.37058795 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.37058795 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.37058795 0.37058795\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.37058795 0.         0.         0.\n",
      " 0.         0.37058795 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.748\n",
      "(*) epoch 2, cost 0.880\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.772\n",
      "(*) epoch 2, cost 2.712\n",
      "(*) epoch 3, cost 2.487\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.30472822 1.30472822 1.30472822 1.30472822 1.5927188  1.5927188\n",
      " 1.5927188  1.30472822 1.5927188  1.30472822 1.5927188  1.30472822\n",
      " 1.30472822 1.30472822 1.30472822 1.5927188  1.30472822 1.30472822\n",
      " 1.30472822 1.30472822 1.30472822 1.30472822 1.5927188  1.30472822\n",
      " 1.5927188  1.30472822 1.30472822 1.30472822 1.30472822 1.30472822\n",
      " 1.30472822 1.30472822 1.5927188  1.30472822 1.30472822 1.5927188\n",
      " 1.30472822 1.5927188  1.5927188  1.30472822 1.30472822 1.30472822\n",
      " 1.30472822 1.30472822 1.5927188  1.5927188  1.5927188  1.30472822\n",
      " 1.30472822 1.30472822 1.30472822 1.5927188  1.30472822 1.5927188\n",
      " 1.5927188  1.30472822 1.30472822 1.30472822 1.30472822 1.30472822\n",
      " 1.30472822 1.30472822 1.30472822 1.30472822 1.30472822 1.5927188\n",
      " 1.30472822 1.30472822 1.30472822 1.30472822 1.30472822 1.5927188\n",
      " 1.30472822 1.30472822 1.30472822 1.5927188  1.30472822 1.30472822\n",
      " 1.30472822 1.30472822 1.5927188  1.30472822 1.5927188  1.5927188\n",
      " 1.5927188  1.30472822 1.5927188  1.30472822 1.5927188  1.5927188\n",
      " 1.30472822 1.30472822 1.5927188  1.30472822 1.30472822 1.30472822\n",
      " 1.30472822 1.5927188  1.30472822 1.30472822]\n",
      "all_sum is: [3.74334731 2.77464465 3.74334731 3.74334731 3.74334731 3.05732833\n",
      " 2.08862567 3.05732833 3.74334731 2.08862567 2.77464465 3.05732833\n",
      " 3.05732833 3.74334731 3.05732833 3.74334731 3.05732833 4.44888615\n",
      " 3.05732833 3.74334731 3.05732833 2.08862567 3.05732833 3.05732833\n",
      " 3.05732833 2.08862567 3.05732833 3.05732833 3.74334731 3.05732833\n",
      " 3.05732833 2.77464465 3.05732833 3.05732833 3.05732833 2.08862567\n",
      " 2.08862567 3.74334731 3.05732833 3.05732833 2.08862567 3.05732833\n",
      " 3.05732833 3.74334731 2.08862567 3.05732833 3.05732833 3.05732833\n",
      " 3.05732833 3.05732833 3.05732833 3.74334731 3.74334731 3.05732833\n",
      " 3.05732833 3.74334731 3.05732833 5.13490513 3.05732833 2.08862567\n",
      " 2.77464465 3.05732833 3.05732833 3.74334731 3.74334731 3.05732833\n",
      " 3.74334731 3.05732833 3.05732833 3.05732833 3.05732833 3.05732833\n",
      " 3.74334731 3.05732833 3.74334731 3.74334731 4.24207131 3.74334731\n",
      " 3.74334731 2.08862567 3.05732833 3.74334731 2.08862567 3.05732833\n",
      " 3.05732833 3.05732833 2.08862567 3.05732833 3.05732833 3.05732833\n",
      " 3.05732833 4.24207131 3.05732833 3.05732833 3.05732833 3.05732833\n",
      " 3.05732833 3.74334731 3.74334731 3.05732833]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.139\n",
      "(*) epoch 2, cost 2.005\n",
      "(*) epoch 3, cost 1.754\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.003\n",
      "(*) epoch 2, cost 2.903\n",
      "(*) epoch 3, cost 2.376\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70410591 0.70410591 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70410591 0.         0.70410591\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.70410591 0.         0.         0.         0.         0.\n",
      " 0.         0.70410591 0.         0.70410591 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70410591 0.         0.\n",
      " 0.         0.         0.         0.70410591 0.81451037 0.\n",
      " 0.         0.         0.         0.70410591 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.70410591 0.70410591 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.70410591 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.70410591 0.        ]\n",
      "all_sum is: [1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.45824585 1.45824585 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.45824585 1.45824585 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.45824585 1.45824585 1.45824585 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.45824585 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.45824585\n",
      " 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971 1.12599971\n",
      " 1.12599971 1.12599971 1.12599971 1.45824585 1.12599971 1.12599971\n",
      " 1.45824585 1.12599971 1.12599971 1.12599971]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.122\n",
      "(*) epoch 2, cost 1.233\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.227\n",
      "(*) epoch 2, cost 0.568\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.30160383 0.30160383 0.30160383 0.30160383 0.29857072 0.59569083\n",
      " 0.30160383 0.59569083 0.59569083 0.30160383 0.59569083 0.30160383\n",
      " 0.59569083 0.30160383 0.51419043 0.00448372 0.59569083 3.02778886\n",
      " 0.30160383 0.30160383 0.30160383 0.30160383 0.59569083 0.80827742\n",
      " 0.59569083 0.30160383 0.30160383 0.59569083 0.59569083 0.30160383\n",
      " 0.00448372 0.30160383 0.29857072 0.00448372 0.30160383 0.30160383\n",
      " 0.59569083 0.30160383 0.30160383 0.30160383 0.30160383 0.30160383\n",
      " 0.30160383 0.59569083 0.30160383 0.30160383 0.30160383 0.30160383\n",
      " 0.59569083 0.51419043 0.59569083 0.59569083 0.30160383 0.30160383\n",
      " 3.02778886 0.30160383 0.59569083 0.59569083 0.30160383 0.30160383\n",
      " 0.29857072 0.30160383 0.30160383 0.30160383 0.51419043 0.30160383\n",
      " 0.30160383 0.30160383 0.30160383 0.59569083 0.29857072 0.59569083\n",
      " 0.30160383 0.30160383 0.59569083 0.30160383 0.59569083 0.30160383\n",
      " 0.30160383 0.59569083 0.59569083 0.59569083 0.80827742 0.59569083\n",
      " 0.51419043 0.30160383 0.30160383 0.30160383 3.02778886 0.30160383\n",
      " 0.30160383 0.00448372 0.30160383 0.29857072 0.30160383 0.30160383\n",
      " 0.59569083 0.30160383 0.30160383 0.59569083]\n",
      "all_sum is: [1.4507047  1.75457601 0.8451877  1.75457601 1.4507047  1.75457601\n",
      " 1.75457601 1.14905901 1.75457601 1.75457601 1.75457601 0.8451877\n",
      " 1.75457601 1.4507047  1.75457601 1.75457601 1.14905901 1.4507047\n",
      " 1.75457601 1.4507047  1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 1.75457601 1.75457601 1.75457601 1.4507047  1.75457601 1.75457601\n",
      " 1.11417155 1.75457601 1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 0.50865455 1.14905901 1.75457601 1.75457601 1.75457601 1.4507047\n",
      " 1.75457601 0.8451877  1.11417155 1.75457601 1.75457601 1.75457601\n",
      " 1.75457601 1.75457601 1.14905901 0.50865455 1.75457601 1.75457601\n",
      " 1.75457601 1.4507047  1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 1.4507047  1.75457601 1.4507047  1.75457601 1.75457601 1.75457601\n",
      " 1.14905901 1.75457601 1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 1.14905901 1.75457601 1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 1.75457601 1.4507047  1.4507047  1.14905901 1.4507047  1.75457601\n",
      " 2.82288878 1.75457601 1.75457601 1.75457601 1.75457601 1.75457601\n",
      " 1.75457601 1.75457601 1.75457601 1.4507047  0.8451877  1.11417155\n",
      " 0.20478324 1.75457601 1.75457601 1.75457601]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.451\n",
      "(*) epoch 2, cost 1.650\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.873\n",
      "(*) epoch 2, cost 2.827\n",
      "(*) epoch 3, cost 2.500\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.30257634 3.76335444 3.76335444 3.76335444 3.30257634 6.450435\n",
      " 3.30257634 6.450435   3.30257634 3.30257634 3.76335444 4.6231524\n",
      " 3.30257634 3.30257634 3.30257634 3.30257634 3.30257634 3.30257634\n",
      " 3.30257634 3.30257634 3.30257634 3.76335444 3.76335444 3.30257634\n",
      " 3.30257634 3.30257634 3.76335444 3.30257634 3.30257634 3.30257634\n",
      " 3.30257634 3.76335444 3.76335444 3.76335444 6.450435   6.450435\n",
      " 3.30257634 3.45452891 3.30257634 3.30257634 6.9112131  3.30257634\n",
      " 3.30257634 3.30257634 3.76335444 3.30257634 3.30257634 3.76335444\n",
      " 3.76335444 3.30257634 3.30257634 3.30257634 1.84474235 3.30257634\n",
      " 3.30257634 3.30257634 3.30257634 3.76335444 3.30257634 6.450435\n",
      " 3.30257634 6.450435   3.30257634 3.30257634 3.30257634 4.6231524\n",
      " 3.30257634 3.30257634 3.30257634 6.9112131  3.76335444 6.450435\n",
      " 3.30257634 3.30257634 3.30257634 6.450435   3.76335444 6.450435\n",
      " 3.30257634 3.76335444 3.30257634 3.76335444 3.30257634 3.30257634\n",
      " 3.30257634 3.30257634 6.450435   3.30257634 3.76335444 3.30257634\n",
      " 3.76335444 3.76335444 3.30257634 3.30257634 3.76335444 3.30257634\n",
      " 3.30257634 3.30257634 6.450435   3.30257634]\n",
      "all_sum is: [3.23331793 2.07022754 2.07022754 1.42572397 2.07022754 2.07022754\n",
      " 1.42572397 2.07022754 2.07022754 2.07022754 1.42572397 3.52262541\n",
      " 2.35953502 1.42572397 2.07022754 2.58881436 3.00403859 1.42572397\n",
      " 2.54992802 1.42572397 1.42572397 2.07022754 2.07022754 1.42572397\n",
      " 1.42572397 1.42572397 1.42572397 2.07022754 2.07022754 1.42572397\n",
      " 1.42572397 2.07022754 2.07022754 1.42572397 2.35953502 1.42572397\n",
      " 1.42572397 1.42572397 2.07022754 1.42572397 1.42572397 3.00403859\n",
      " 4.16712898 2.07022754 1.42572397 2.07022754 1.42572397 2.78680132\n",
      " 2.58881436 3.00403859 1.42572397 3.00403859 2.07022754 2.07022754\n",
      " 2.35953502 1.42572397 2.07022754 1.42572397 2.35953502 1.42572397\n",
      " 3.23331793 2.07022754 2.07022754 1.42572397 1.42572397 2.07022754\n",
      " 2.58881436 3.23331793 2.07022754 1.42572397 2.07022754 2.07022754\n",
      " 2.58881436 3.30538814 1.42572397 1.42572397 2.58881436 3.52262541\n",
      " 2.07022754 2.07022754 1.42572397 2.35953502 2.07022754 1.42572397\n",
      " 2.58881436 2.07022754 1.42572397 3.00403859 3.05154285 2.07022754\n",
      " 1.42572397 1.42572397 2.07022754 3.23331793 3.23331793 1.42572397\n",
      " 1.42572397 2.07022754 1.42572397 1.42572397]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.424\n",
      "(*) epoch 2, cost 2.147\n",
      "(*) epoch 3, cost 1.980\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.341\n",
      "(*) epoch 2, cost 3.559\n",
      "(*) epoch 3, cost 3.319\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [6.13176586e-04 2.25290610e+00 2.25290610e+00 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 2.25290610e+00 6.13176586e-04\n",
      " 2.25290610e+00 2.25290610e+00 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 2.25290610e+00 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 2.47792001e+00 6.13176586e-04 2.25290610e+00 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 2.25290610e+00 6.13176586e-04\n",
      " 2.25290610e+00 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 2.25290610e+00 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 2.25627091e-01 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 2.25627091e-01 2.25290610e+00 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 2.25290610e+00 2.47792001e+00 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 2.25290610e+00 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 2.25290610e+00 6.13176586e-04 6.13176586e-04\n",
      " 2.25290610e+00 6.13176586e-04 2.25290610e+00 2.25290610e+00\n",
      " 2.25290610e+00 6.13176586e-04 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 2.25290610e+00 6.13176586e-04\n",
      " 2.25290610e+00 2.25290610e+00 6.13176586e-04 6.13176586e-04\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 2.25290610e+00\n",
      " 6.13176586e-04 6.13176586e-04 6.13176586e-04 2.25290610e+00\n",
      " 6.13176586e-04 2.25290610e+00 6.13176586e-04 2.25290610e+00\n",
      " 2.25290610e+00 6.13176586e-04 2.25290610e+00 2.25627091e-01]\n",
      "all_sum is: [2.68648952 2.68648952 2.68648952 2.57178299 2.68648952 2.68648952\n",
      " 2.68648952 2.57178299 2.68648952 2.68648952 2.68648952 2.57178299\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.57178299\n",
      " 2.68648952 2.57178299 2.68648952 2.57178299 2.68648952 2.57178299\n",
      " 2.68648952 2.57178299 0.46158722 2.68648952 2.68648952 2.68648952\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.57178299\n",
      " 2.57178299 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952\n",
      " 2.57178299 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.57178299\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952\n",
      " 2.68648952 2.68648952 2.57178299 2.68648952 2.57178299 2.68648952\n",
      " 2.68648952 2.57178299 2.68648952 2.68648952 2.68648952 2.57178299\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952\n",
      " 2.57178299 2.57178299 2.68648952 2.57178299 2.68648952 2.68648952\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952\n",
      " 2.68648952 2.68648952 2.68648952 2.68648952 2.68648952 0.46158722\n",
      " 2.68648952 2.68648952 2.68648952 2.57178299]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.002\n",
      "(*) epoch 2, cost 0.975\n",
      "(*) epoch 3, cost 0.557\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.142\n",
      "(*) epoch 2, cost 0.646\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.28352731 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 0.28352731 0.28352731 0.8153072  0.28352731\n",
      " 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731 0.57672346\n",
      " 0.28352731 0.28352731 0.57672346 0.28352731 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 0.93702759 0.28352731 0.28352731 0.28352731\n",
      " 0.57672346 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 3.98038016 0.57672346 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 0.28352731 0.57672346 0.28352731 0.28352731\n",
      " 0.28352731 0.57672346 0.28352731 0.57672346 0.28352731 0.28352731\n",
      " 0.57672346 0.28352731 0.57672346 0.28352731 0.28352731 0.28352731\n",
      " 0.57672346 0.28352731 0.57672346 0.57672346 0.57672346 0.28352731\n",
      " 0.57672346 0.57672346 0.28352731 0.57672346 0.57672346 0.28352731\n",
      " 3.32687989 0.28352731 0.28352731 0.28352731 0.57672346 0.28352731\n",
      " 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 0.57672346 0.28352731 0.28352731 0.28352731\n",
      " 0.28352731 0.28352731 0.28352731 0.28352731 0.28352731 3.32687989\n",
      " 0.57672346 0.28352731 0.28352731 0.28352731]\n",
      "all_sum is: [0.73576695 0.73576695 0.85698121 0.73576695 0.73576695 0.73576695\n",
      " 0.73576695 0.85698121 0.85698121 0.73576695 0.73576695 0.73576695\n",
      " 0.73576695 0.73576695 0.85698121 0.73576695 0.85698121 0.73576695\n",
      " 0.85698121 0.85698121 0.73576695 0.73576695 0.73576695 0.85698121\n",
      " 0.73576695 0.85698121 0.73576695 0.85698121 0.85698121 0.85698121\n",
      " 0.73576695 0.73576695 0.85698121 0.73576695 0.73576695 0.73576695\n",
      " 0.85698121 0.73576695 0.85698121 0.73576695 0.85698121 0.73576695\n",
      " 0.85698121 0.73576695 0.85698121 0.85698121 0.73576695 0.85698121\n",
      " 0.73576695 0.85698121 0.85698121 0.85698121 0.73576695 0.73576695\n",
      " 0.73576695 0.85698121 0.85698121 0.73576695 0.85698121 0.85698121\n",
      " 0.73576695 0.85698121 0.85698121 0.73576695 0.85698121 0.73576695\n",
      " 0.85698121 0.73576695 0.85698121 0.73576695 0.73576695 0.85698121\n",
      " 0.73576695 0.73576695 0.85698121 0.73373188 0.73576695 0.73576695\n",
      " 0.73576695 0.85698121 0.85698121 0.85698121 0.85698121 0.85698121\n",
      " 0.85698121 0.73576695 0.85698121 0.85698121 0.85698121 0.73576695\n",
      " 0.85698121 0.85698121 0.73576695 0.85698121 0.85494614 0.85698121\n",
      " 0.73576695 0.73576695 0.85698121 0.85698121]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.222\n",
      "(*) epoch 2, cost 0.671\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.861\n",
      "(*) epoch 2, cost 2.254\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.65207937 1.45149538 1.45149538\n",
      " 1.45149538 1.65207937 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 2.30891033\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538 1.45149538\n",
      " 1.45149538 1.45149538 1.45149538 1.45149538]\n",
      "all_sum is: [4.20103576 4.20103576 4.3550363  2.31013415 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 4.20103576 4.3550363\n",
      " 4.20103576 4.20103576 4.20103576 4.3550363  4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 4.20103576 4.20103576\n",
      " 4.3550363  4.20103576 4.20103576 4.20103576 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 2.31013415 4.20103576 2.31013415\n",
      " 4.20103576 4.3550363  4.3550363  3.51134517 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 2.31013415 4.20103576\n",
      " 4.20103576 4.20103576 2.31013415 4.3550363  4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 4.3550363  4.3550363\n",
      " 4.20103576 4.3550363  4.3550363  4.20103576 4.20103576 4.3550363\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 2.31013415 2.31013415\n",
      " 4.20103576 2.46413469 4.20103576 4.20103576 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 2.31013415 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576 4.20103576 4.20103576\n",
      " 4.20103576 4.20103576 4.3550363  4.20103576 4.20103576 2.31013415\n",
      " 4.20103576 4.20103576 4.20103576 4.20103576]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.328\n",
      "(*) epoch 2, cost 0.555\n",
      "(*) epoch 3, cost 0.381\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 11\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.928\n",
      "(*) epoch 2, cost 4.648\n",
      "(*) epoch 3, cost 4.392\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.95942864 1.95942864 1.95942864 2.17562642 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 3.77950196 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.06473792 1.95942864 1.95942864 1.06473792 1.06473792\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.06473792\n",
      " 1.95942864 1.06473792 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 2.13219241 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.06473792 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.06473792 1.06473792\n",
      " 1.95942864 1.95942864 1.06473792 1.06473792 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.06473792 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864 1.95942864\n",
      " 1.95942864 1.95942864 1.95942864 1.95942864]\n",
      "all_sum is: [0.14984231 0.14984231 0.14984231 1.28999427 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 1.28999427 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 1.28999427 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 1.1685546  0.14984231 0.14984231 0.14984231 1.28999427\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 1.28999427 1.28999427 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 2.30870656 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231 0.14984231\n",
      " 0.14984231 0.14984231 0.14984231 0.14984231]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.568\n",
      "(*) epoch 2, cost 1.224\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.282\n",
      "(*) epoch 2, cost 0.624\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.18209278 1.92853288 1.92853288 1.18209278 1.92853288 1.92853288\n",
      " 1.92853288 1.92853288 2.01000338 1.18209278 1.92853288 1.18209278\n",
      " 1.92853288 1.92853288 1.18209278 1.92853288 1.92853288 1.92853288\n",
      " 1.92853288 1.92853288 1.92853288 1.0820235  1.92853288 1.92853288\n",
      " 1.92853288 1.92853288 1.92853288 1.26356328 1.92853288 1.26356328\n",
      " 1.92853288 1.92853288 1.92853288 1.18209278 2.01000338 1.92853288\n",
      " 1.18209278 1.18209278 1.26356328 1.92853288 1.92853288 2.01000338\n",
      " 1.92853288 2.01000338 1.92853288 1.18209278 1.18209278 1.18209278\n",
      " 1.92853288 1.18209278 1.18209278 1.18209278 1.18209278 1.18209278\n",
      " 1.18209278 1.18209278 1.18209278 1.92853288 1.92853288 1.92853288\n",
      " 1.18209278 1.18209278 1.18209278 2.01000338 1.18209278 1.18209278\n",
      " 1.26356328 2.01000338 1.26356328 1.18209278 1.92853288 1.92853288\n",
      " 1.18209278 1.18209278 1.92853288 1.18209278 1.92853288 1.92853288\n",
      " 1.26356328 1.92853288 1.92853288 1.92853288 2.01000338 1.92853288\n",
      " 1.92853288 1.92853288 1.92853288 1.92853288 1.18209278 1.92853288\n",
      " 1.92853288 1.92853288 1.92853288 1.92853288 1.0820235  1.92853288\n",
      " 1.18209278 1.92853288 2.01000338 1.92853288]\n",
      "all_sum is: [1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741 1.53580741\n",
      " 1.53580741 1.53580741 1.53580741 1.53580741]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.676\n",
      "(*) epoch 2, cost 2.274\n",
      "(*) epoch 3, cost 1.906\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.782\n",
      "(*) epoch 2, cost 2.903\n",
      "(*) epoch 3, cost 2.639\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.19873371 0.19873371 0.19873371 0.19873371 0.19873371 0.81169458\n",
      " 0.19873371 0.19873371 0.33450228 0.81169458 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371 0.81169458\n",
      " 0.81169458 0.19873371 0.81169458 0.19873371 0.81169458 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.81169458 0.81169458 0.81169458\n",
      " 0.81169458 0.81169458 0.19873371 0.81169458 0.19873371 0.81169458\n",
      " 0.19873371 0.19873371 0.19873371 0.19873371 0.81169458 0.19873371\n",
      " 0.81169458 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.81169458 0.19873371 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.81169458 0.19873371 0.81169458 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.81169458 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.81169458 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.81169458 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371 0.19873371\n",
      " 0.19873371 0.81169458 0.19873371 0.19873371]\n",
      "all_sum is: [1.72404817 1.72404817 1.72404817 2.00916656 2.00916656 1.72404817\n",
      " 2.00916656 1.72404817 2.00916656 2.00916656 1.72404817 1.72404817\n",
      " 2.00916656 1.72404817 1.72404817 2.00916656 2.00916656 1.72404817\n",
      " 1.72404817 2.00916656 2.00916656 1.72404817 1.72404817 2.00916656\n",
      " 1.72404817 1.72404817 2.00916656 1.72404817 2.00916656 2.00916656\n",
      " 2.00916656 1.72404817 2.00916656 2.00916656 1.72404817 1.72404817\n",
      " 1.72404817 1.72404817 2.00916656 2.00916656 1.72404817 2.00916656\n",
      " 1.72404817 2.00916656 1.72404817 2.00916656 2.00916656 2.00916656\n",
      " 2.00916656 1.72404817 1.72404817 2.00916656 2.00916656 2.00916656\n",
      " 1.72404817 2.00916656 2.00916656 2.00916656 2.00916656 2.00916656\n",
      " 1.72404817 1.72404817 2.00916656 2.00916656 2.00916656 2.00916656\n",
      " 2.00916656 1.72404817 2.00916656 2.00916656 2.00916656 2.00916656\n",
      " 1.72404817 2.00916656 2.00916656 1.72404817 1.72404817 2.00916656\n",
      " 1.72404817 1.72404817 1.72404817 2.00916656 1.72404817 1.72404817\n",
      " 1.72404817 2.00916656 1.72404817 2.00916656 2.00916656 1.72404817\n",
      " 1.72404817 1.72404817 1.72404817 2.00916656 1.72404817 1.72404817\n",
      " 1.72404817 2.00916656 1.72404817 1.72404817]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.474\n",
      "(*) epoch 2, cost 1.707\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.270\n",
      "(*) epoch 2, cost 1.774\n",
      "(*) epoch 3, cost 1.379\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.35375246 2.00736166 0.79769849 2.00736166 2.02306064 2.00736166\n",
      " 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849\n",
      " 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849\n",
      " 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849\n",
      " 0.79769849 2.00736166 2.00736166 2.54771666 0.79769849 1.33805349\n",
      " 0.79769849 2.00736166 0.79769849 0.79769849 0.81339746 2.02306064\n",
      " 0.81339746 2.00736166 0.79769849 0.79769849 0.79769849 0.79769849\n",
      " 2.00736166 0.79769849 1.33805349 0.79769849 0.79769849 0.79769849\n",
      " 0.79769849 2.00736166 2.54771666 0.79769849 2.00736166 1.35375246\n",
      " 0.79769849 0.81339746 2.3525381  0.81339746 0.79769849 2.00736166\n",
      " 0.81339746 0.79769849 0.79769849 0.79769849 0.79769849 0.79769849\n",
      " 2.00736166 0.79769849 0.79769849 2.00736166 0.79769849 1.33805349\n",
      " 0.79769849 0.79769849 2.00736166 0.79769849 0.79769849 0.79769849\n",
      " 1.33805349 0.79769849 2.54771666 0.79769849 0.79769849 2.54771666\n",
      " 0.81339746 0.79769849 0.79769849 0.79769849 0.79769849 1.33805349\n",
      " 0.81339746 2.00736166 2.00736166 2.00736166 2.00736166 1.35375246\n",
      " 0.79769849 0.81339746 0.79769849 0.79769849]\n",
      "all_sum is: [1.91979616 3.29631872 3.29631872 1.91979616 3.29631872 4.15523437\n",
      " 4.15523437 1.91979616 4.15523437 1.91979616 3.29631872 1.37652257\n",
      " 3.29631872 3.29631872 1.91979616 1.91979616 1.37652257 3.29631872\n",
      " 3.29631872 3.29631872 3.29631872 3.29631872 1.91979616 3.29631872\n",
      " 1.91979616 3.29631872 3.29631872 1.91979616 0.         3.29631872\n",
      " 3.29631872 1.91979616 3.29631872 3.29631872 3.29631872 3.29631872\n",
      " 1.91979616 3.29631872 3.29631872 3.29631872 3.29631872 3.29631872\n",
      " 1.91979616 1.91979616 1.91979616 3.29631872 3.29631872 3.29631872\n",
      " 3.29631872 1.91979616 3.29631872 1.91979616 1.91979616 2.23543821\n",
      " 3.29631872 3.29631872 3.29631872 3.29631872 3.29631872 3.29631872\n",
      " 3.29631872 1.91979616 3.29631872 3.29631872 3.29631872 3.29631872\n",
      " 1.37652257 3.29631872 3.29631872 3.29631872 1.91979616 0.\n",
      " 3.29631872 1.91979616 1.91979616 3.29631872 0.         3.29631872\n",
      " 3.29631872 3.29631872 3.29631872 3.29631872 1.91979616 3.29631872\n",
      " 3.29631872 3.29631872 1.91979616 3.29631872 3.29631872 3.29631872\n",
      " 1.91979616 3.29631872 1.91979616 1.37652257 1.37652257 3.29631872\n",
      " 3.29631872 3.29631872 1.91979616 1.91979616]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.815\n",
      "(*) epoch 2, cost 1.977\n",
      "(*) epoch 3, cost 1.707\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.188\n",
      "(*) epoch 2, cost 1.837\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.43817417 3.43817417 4.300968   3.43817417 3.43817417 3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 3.43817417 3.43817417 3.43817417\n",
      " 3.43817417 4.300968   3.43817417 3.43817417 3.43817417 3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 4.300968   3.43817417 3.43817417\n",
      " 3.43817417 4.300968   3.43817417 4.300968   3.43817417 3.43817417\n",
      " 4.300968   3.43817417 4.300968   4.300968   3.43817417 4.300968\n",
      " 3.43817417 3.43817417 3.43817417 4.300968   3.43817417 4.300968\n",
      " 3.43817417 3.43817417 3.43817417 4.300968   4.300968   3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 3.43817417 3.43817417 3.43817417\n",
      " 3.43817417 4.300968   3.43817417 4.300968   3.43817417 4.300968\n",
      " 3.43817417 4.300968   3.43817417 3.48657023 3.43817417 3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 4.300968   4.300968   3.43817417\n",
      " 3.43817417 4.300968   3.43817417 4.300968   4.300968   3.43817417\n",
      " 4.300968   3.43817417 2.6237764  3.43817417 3.43817417 3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 4.300968   4.300968   3.43817417\n",
      " 3.43817417 3.43817417 3.43817417 3.43817417 4.300968   3.43817417\n",
      " 3.43817417 3.43817417 4.300968   3.43817417]\n",
      "all_sum is: [1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 2.19648366 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 2.19648366 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581 1.47158581\n",
      " 1.47158581 1.47158581 1.47158581 1.47158581]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.703\n",
      "(*) epoch 2, cost 3.643\n",
      "(*) epoch 3, cost 3.420\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.646\n",
      "(*) epoch 2, cost 2.096\n",
      "(*) epoch 3, cost 1.741\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.63201857 2.63201857 2.01493919 2.63201857 2.01493919 2.01493919\n",
      " 2.63201857 2.63201857 2.63201857 2.63201857 2.01493919 2.63201857\n",
      " 2.01493919 2.01493919 2.63201857 2.01493919 2.63201857 2.01493919\n",
      " 2.01493919 2.63201857 2.01493919 2.01493919 2.63201857 2.63201857\n",
      " 2.01493919 2.63201857 2.01493919 2.01493919 2.01493919 2.63201857\n",
      " 2.01493919 2.01493919 2.63201857 2.01493919 2.01493919 2.63201857\n",
      " 2.63201857 2.01493919 2.63201857 2.01493919 2.01493919 2.01493919\n",
      " 2.01493919 2.01493919 2.63201857 2.01493919 2.01493919 2.63201857\n",
      " 2.63201857 2.01493919 2.01493919 2.63201857 2.01493919 2.01493919\n",
      " 2.63201857 2.01493919 2.01493919 2.01493919 2.63201857 2.63201857\n",
      " 2.01493919 2.01493919 2.01493919 2.63201857 2.01493919 2.01493919\n",
      " 2.01493919 2.01493919 2.01493919 2.63201857 2.63201857 2.01493919\n",
      " 2.63201857 2.01493919 2.01493919 2.01493919 2.63201857 2.01493919\n",
      " 2.01493919 2.01493919 2.01493919 2.01493919 2.63201857 2.63201857\n",
      " 2.63201857 2.01493919 2.01493919 2.01493919 2.01493919 2.01493919\n",
      " 2.01493919 2.63201857 2.63201857 2.63201857 2.63201857 2.01493919\n",
      " 2.63201857 2.63201857 2.01493919 2.01493919]\n",
      "all_sum is: [0.81509437 0.97376587 0.97376587 0.97376587 0.81509437 0.81509437\n",
      " 0.81509437 0.81509437 2.61362056 0.81509437 0.81509437 0.81509437\n",
      " 0.81509437 0.97376587 0.97376587 0.97376587 0.81509437 0.81509437\n",
      " 0.81509437 0.61813731 0.81509437 0.97376587 0.81509437 0.45946581\n",
      " 0.45946581 0.97376587 0.81509437 0.81509437 2.61362056 0.81509437\n",
      " 0.81509437 0.81509437 0.97376587 0.97376587 0.55014926 0.81509437\n",
      " 0.97376587 0.81509437 0.61813731 0.81509437 0.97376587 0.81509437\n",
      " 0.81509437 0.81509437 0.45946581 0.97376587 0.55014926 0.97376587\n",
      " 0.81509437 0.81509437 0.97376587 0.39147777 0.45946581 0.81509437\n",
      " 0.81509437 0.45946581 0.81509437 0.39147777 0.81509437 0.81509437\n",
      " 0.81509437 0.97376587 0.81509437 0.81509437 0.81509437 0.97376587\n",
      " 0.81509437 0.97376587 0.55014926 0.97376587 0.03584921 0.81509437\n",
      " 0.97376587 0.81509437 0.81509437 0.81509437 0.97376587 0.81509437\n",
      " 0.45946581 0.81509437 0.97376587 0.45946581 0.97376587 0.45946581\n",
      " 0.81509437 0.97376587 0.81509437 0.97376587 0.45946581 0.81509437\n",
      " 0.81509437 0.97376587 0.97376587 0.81509437 0.45946581 0.81509437\n",
      " 0.81509437 0.81509437 0.97376587 0.81509437]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.487\n",
      "(*) epoch 2, cost 3.062\n",
      "(*) epoch 3, cost 2.535\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.012\n",
      "(*) epoch 2, cost 1.634\n",
      "(*) epoch 3, cost 1.447\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.19047402 3.19047402 3.19047402 3.19047402 3.19047402 0.98621227\n",
      " 3.19047402 3.19047402 3.19047402 0.98621227 3.19047402 3.19047402\n",
      " 0.98621227 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402\n",
      " 0.98621227 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 3.19047402 3.50910002 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402\n",
      " 3.19047402 0.98621227 3.19047402 3.19047402 0.98621227 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402\n",
      " 3.50910002 3.19047402 3.19047402 3.19047402 3.50910002 3.19047402\n",
      " 3.19047402 0.98621227 0.98621227 3.19047402 3.19047402 3.19047402\n",
      " 0.98621227 3.19047402 0.98621227 3.19047402 3.19047402 3.19047402\n",
      " 3.19047402 3.19047402 0.98621227 3.19047402 3.19047402 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 3.19047402 3.19047402 0.98621227\n",
      " 3.19047402 3.19047402 0.98621227 3.19047402 0.98621227 3.19047402\n",
      " 3.19047402 3.19047402 3.19047402 0.98621227 3.19047402 3.19047402\n",
      " 3.19047402 0.98621227 3.19047402 0.98621227]\n",
      "all_sum is: [0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 1.25582011\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 1.25582011 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 1.25582011 0.41059472 0.41059472 0.41059472 0.41059472 1.25582011\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472 0.41059472\n",
      " 0.41059472 0.41059472 0.41059472 0.41059472]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.237\n",
      "(*) epoch 2, cost 1.746\n",
      "(*) epoch 3, cost 1.492\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.877\n",
      "(*) epoch 2, cost 1.043\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.24748323 0.05217465 1.59345488 0.24748323 2.13493657 1.59345488\n",
      " 0.05217465 1.78876346 0.24748323 0.24748323 0.39834775 0.24748323\n",
      " 0.05217465 0.24748323 0.05217465 0.05217465 0.05217465 0.05217465\n",
      " 1.93962798 1.59345488 0.24748323 0.05217465 0.05217465 0.05217465\n",
      " 0.05217465 0.24748323 0.39834775 1.59345488 0.24748323 1.59345488\n",
      " 0.05217465 2.0365652  1.59345488 0.24748323 0.05217465 0.24748323\n",
      " 0.59365633 0.05217465 0.24748323 0.05217465 0.05217465 1.59345488\n",
      " 0.24748323 1.78876346 0.24748323 0.24748323 0.24748323 0.24748323\n",
      " 1.78876346 1.78876346 0.24748323 0.05217465 0.05217465 1.78876346\n",
      " 1.59345488 0.24748323 0.24748323 0.24748323 1.78876346 1.78876346\n",
      " 1.93962798 0.39834775 0.05217465 0.24748323 0.05217465 0.05217465\n",
      " 0.59365633 1.59345488 0.19530858 0.05217465 0.39834775 0.59365633\n",
      " 1.59345488 0.24748323 0.05217465 0.05217465 0.05217465 0.24748323\n",
      " 1.59345488 0.05217465 0.24748323 0.05217465 0.05217465 1.78876346\n",
      " 0.05217465 0.24748323 1.78876346 0.05217465 0.24748323 1.59345488\n",
      " 0.24748323 0.05217465 1.59345488 0.39834775 0.39834775 0.24748323\n",
      " 1.93962798 1.59345488 1.78876346 0.24748323]\n",
      "all_sum is: [3.74955746 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.74955746 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.74955746 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676 3.18969676\n",
      " 3.18969676 3.18969676 3.18969676 3.18969676]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.626\n",
      "(*) epoch 2, cost 2.628\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.017\n",
      "(*) epoch 2, cost 0.792\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.69376051 2.21366721 0.69376051 0.69376051 0.69376051 0.69376051\n",
      " 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721 0.69376051\n",
      " 2.21366721 2.21366721 0.69376051 2.21366721 0.69376051 2.21366721\n",
      " 0.69376051 0.69376051 2.21366721 0.69376051 0.69376051 2.21366721\n",
      " 2.21366721 0.69376051 0.69376051 0.69376051 0.69376051 0.69376051\n",
      " 0.69376051 2.21366721 0.69376051 0.69376051 0.69376051 0.69376051\n",
      " 0.69376051 2.21366721 0.69376051 0.69376051 0.69376051 1.70364283\n",
      " 0.69376051 2.21366721 0.69376051 0.69376051 2.21366721 2.21366721\n",
      " 0.69376051 0.69376051 0.69376051 2.21366721 0.69376051 2.21366721\n",
      " 0.69376051 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721\n",
      " 0.69376051 2.21366721 0.69376051 0.69376051 0.69376051 0.69376051\n",
      " 0.69376051 0.69376051 0.69376051 1.0783232  2.21366721 2.21366721\n",
      " 0.69376051 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721\n",
      " 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721 0.69376051\n",
      " 2.21366721 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721\n",
      " 0.69376051 0.69376051 0.69376051 0.69376051 2.21366721 0.69376051\n",
      " 0.69376051 0.69376051 1.0783232  0.69376051]\n",
      "all_sum is: [0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209 0.49836209\n",
      " 0.49836209 0.49836209 0.49836209 0.49836209]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.280\n",
      "(*) epoch 2, cost 1.260\n",
      "(*) epoch 3, cost 1.008\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.531\n",
      "(*) epoch 2, cost 2.453\n",
      "(*) epoch 3, cost 2.001\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 0.95582751 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 0.95582751 0.95582751\n",
      " 1.61754041 1.61754041 0.95582751 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 0.95582751 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 0.95582751\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 0.95582751 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041 1.61754041\n",
      " 1.61754041 1.61754041 1.61754041 0.95582751 1.61754041 0.95582751\n",
      " 1.61754041 0.95582751 1.61754041 1.61754041]\n",
      "all_sum is: [0.64542782 0.64542782 0.64542782 1.01788386 0.64542782 0.64542782\n",
      " 0.64542782 1.66331168 1.61743042 2.63531428 0.64542782 0.64542782\n",
      " 1.61743042 1.61743042 1.61743042 1.61743042 1.61743042 1.61743042\n",
      " 0.64542782 2.63531428 0.64542782 1.66331168 2.63531428 0.64542782\n",
      " 2.63531428 0.64542782 1.61743042 0.64542782 1.61743042 0.64542782\n",
      " 1.66331168 1.66331168 0.         1.61743042 0.64542782 1.66331168\n",
      " 0.64542782 2.63531428 2.63531428 2.63531428 0.64542782 2.63531428\n",
      " 0.64542782 0.64542782 0.64542782 0.64542782 1.61743042 0.64542782\n",
      " 0.97200261 0.         1.01788386 0.64542782 1.61743042 0.\n",
      " 2.63531428 0.64542782 0.         0.64542782 1.66331168 1.66331168\n",
      " 0.64542782 1.66331168 0.64542782 1.01788386 0.64542782 1.61743042\n",
      " 1.66331168 0.97200261 1.66331168 1.61743042 2.63531428 1.98988646\n",
      " 2.63531428 1.98988646 1.61743042 0.64542782 0.64542782 2.63531428\n",
      " 1.61743042 0.64542782 0.64542782 1.61743042 0.64542782 0.64542782\n",
      " 1.66331168 0.64542782 2.63531428 0.64542782 1.61743042 0.97200261\n",
      " 1.61743042 0.64542782 2.63531428 1.66331168 1.98988646 0.64542782\n",
      " 1.66331168 0.64542782 0.64542782 2.63531428]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.799\n",
      "(*) epoch 2, cost 0.802\n",
      "(*) epoch 3, cost 0.616\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.798\n",
      "(*) epoch 2, cost 2.296\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.42065573 0.99994255 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.99994255 0.42065573 0.42065573 0.42065573 0.99994255 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.99994255 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.99994255 0.42065573 0.99994255 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.99994255 0.42065573 0.42065573 0.99994255 0.42065573 0.42065573\n",
      " 2.42685557 0.42065573 0.42065573 0.42065573 0.         0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.99994255 0.42065573 0.42065573 0.42065573 0.42065573\n",
      " 0.42065573 0.42065573 0.42065573 0.42065573 0.42065573 0.99994255\n",
      " 0.99994255 0.99994255 0.42065573 0.42065573]\n",
      "all_sum is: [3.82713384 3.82713384 3.82713384 3.82713384 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 3.82713384 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 3.81705121 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 1.84380563 3.82713384 3.82713384\n",
      " 3.82713384 3.81705121 3.81705121 3.81705121 3.82713384 3.81705121\n",
      " 3.82713384 3.82713384 3.82713384 3.81705121 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 3.81705121 3.82713384 3.81705121\n",
      " 3.82713384 3.82713384 3.81705121 3.82713384 3.81705121 3.82713384\n",
      " 3.82713384 3.81705121 3.82713384 3.82713384 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 1.84380563 3.82713384 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 1.84380563 3.82713384 3.82713384\n",
      " 3.81705121 3.82713384 3.82713384 3.82713384 3.81705121 3.82713384\n",
      " 3.82713384 3.81705121 1.84380563 3.82713384 3.81705121 3.81705121\n",
      " 3.82713384 1.84380563 3.82713384 3.82713384 3.81705121 1.84380563\n",
      " 3.82713384 3.82713384 3.82713384 3.82713384 3.82713384 3.82713384\n",
      " 3.82713384 3.82713384 3.82713384 3.81705121 3.82713384 3.82713384\n",
      " 1.84380563 3.81705121 3.81705121 3.82713384]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.916\n",
      "(*) epoch 2, cost 3.522\n",
      "(*) epoch 3, cost 3.290\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.136\n",
      "(*) epoch 2, cost 0.773\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 3.01262441 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 3.0443433  2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 3.01262441 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537 2.44710537\n",
      " 2.44710537 2.44710537 2.44710537 2.44710537]\n",
      "all_sum is: [0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.         0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.         0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.         0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762 0.55607762\n",
      " 0.55607762 0.55607762 0.55607762 0.55607762]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.805\n",
      "(*) epoch 2, cost 2.390\n",
      "(*) epoch 3, cost 2.096\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.450\n",
      "(*) epoch 2, cost 1.864\n",
      "(*) epoch 3, cost 1.561\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.64579987 3.37297931 1.64579987 1.64579987 1.64579987 1.64579987\n",
      " 1.64579987 3.24137619 3.60290678 1.64579987 1.64579987 3.01144871\n",
      " 3.01144871 1.64579987 3.01144871 1.64579987 3.24137619 1.64579987\n",
      " 3.01144871 3.01144871 3.01144871 1.64579987 1.64579987 3.01144871\n",
      " 3.07619919 1.64579987 1.64579987 1.64579987 1.64579987 1.64579987\n",
      " 1.64579987 1.64579987 1.64579987 1.71055034 1.64579987 1.64579987\n",
      " 2.00733047 1.64579987 1.64579987 1.64579987 1.64579987 1.64579987\n",
      " 1.64579987 1.64579987 1.64579987 2.18196276 3.01144871 1.64579987\n",
      " 1.64579987 1.64579987 3.01144871 1.64579987 3.24137619 1.64579987\n",
      " 3.01144871 3.01144871 1.64579987 1.64579987 1.64579987 3.24137619\n",
      " 1.64579987 1.64579987 3.01144871 3.01144871 4.60702503 1.64579987\n",
      " 3.01144871 1.64579987 3.01144871 1.64579987 1.64579987 3.24137619\n",
      " 3.01144871 1.64579987 3.24137619 2.00733047 1.64579987 1.64579987\n",
      " 1.64579987 3.24137619 3.24137619 1.64579987 1.64579987 1.64579987\n",
      " 1.64579987 1.64579987 1.64579987 3.01144871 1.64579987 1.64579987\n",
      " 4.60702503 1.64579987 3.01144871 1.64579987 2.00733047 3.01144871\n",
      " 2.00733047 1.64579987 1.64579987 1.64579987]\n",
      "all_sum is: [0.37946444 0.         0.37946444 0.37946444 0.37946444 0.\n",
      " 0.37946444 0.         0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.         0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 2.18687675 1.83745259 0.37946444 2.18687675 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 2.18687675 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.         0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 2.18687675\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 2.18687675\n",
      " 0.37946444 0.         0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444 0.37946444\n",
      " 0.37946444 0.37946444 0.37946444 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.575\n",
      "(*) epoch 2, cost 3.398\n",
      "(*) epoch 3, cost 3.276\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.277\n",
      "(*) epoch 2, cost 2.921\n",
      "(*) epoch 3, cost 2.532\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 3.52403021 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 3.52403021 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 3.52403021 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 3.52403021 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781 1.32983781\n",
      " 1.32983781 1.32983781 1.32983781 1.32983781]\n",
      "all_sum is: [1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 2.90507815 1.95190262 1.95190262 1.95190262 1.95190262 2.11343896\n",
      " 2.11343896 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 2.11343896 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 2.90507815 2.11343896 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 2.11343896 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 2.11343896 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 2.11343896 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262 1.95190262\n",
      " 1.95190262 1.95190262 1.95190262 1.95190262]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.601\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.675\n",
      "(*) epoch 2, cost 0.913\n",
      "(*) epoch 3, cost 0.645\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.91798512 0.29640412 0.91798512 0.29640412 0.91798512 0.29640412\n",
      " 0.91798512 0.91798512 0.91798512 0.91798512 0.29640412 0.91798512\n",
      " 0.29640412 0.29640412 0.91798512 0.91798512 0.91798512 0.91798512\n",
      " 0.91798512 0.29640412 0.29640412 0.29640412 0.91798512 0.91798512\n",
      " 0.29640412 0.91798512 0.91798512 0.91798512 0.29640412 0.91798512\n",
      " 0.29640412 0.29640412 0.29640412 0.29640412 0.91798512 0.91798512\n",
      " 0.91798512 0.91798512 0.91798512 0.91798512 0.29640412 0.29640412\n",
      " 0.91798512 0.91798512 0.91798512 0.29640412 0.91798512 0.29640412\n",
      " 0.29640412 0.91798512 0.91798512 0.91798512 0.29640412 0.91798512\n",
      " 0.91798512 0.91798512 0.91798512 0.29640412 0.29640412 0.91798512\n",
      " 0.91798512 0.91798512 0.91798512 0.91798512 0.29640412 0.91798512\n",
      " 0.29640412 0.91798512 0.91798512 0.91798512 0.91798512 0.91798512\n",
      " 0.29640412 0.29640412 0.91798512 0.91798512 0.91798512 0.29640412\n",
      " 0.91798512 0.29640412 0.29640412 0.29640412 0.91798512 0.91798512\n",
      " 0.91798512 0.91798512 0.29640412 0.29640412 0.91798512 0.29640412\n",
      " 0.91798512 0.29640412 0.91798512 0.91798512 0.29640412 0.91798512\n",
      " 0.91798512 0.29640412 0.91798512 0.91798512]\n",
      "all_sum is: [0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 2.16468757\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 1.0141167  0.53080877 1.0141167  0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 1.0141167  0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 1.0141167  0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 1.0141167  0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 1.0141167  0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 2.6479955  0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 2.16468757 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877 0.53080877\n",
      " 0.53080877 1.0141167  0.53080877 0.         0.53080877 0.53080877\n",
      " 0.53080877 0.53080877 0.53080877 0.53080877]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.021\n",
      "(*) epoch 2, cost 1.479\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.915\n",
      "(*) epoch 2, cost 1.407\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372 1.31473372\n",
      " 1.31473372 1.31473372 1.31473372 1.31473372]\n",
      "all_sum is: [3.9922152  2.19173415 2.58077441 3.9922152  3.9922152  4.27395956\n",
      " 3.9922152  3.9922152  3.9922152  3.9922152  4.27395956 3.9922152\n",
      " 4.38125546 3.9922152  4.38125546 4.38125546 4.66299982 3.9922152\n",
      " 2.58077441 3.9922152  2.19173415 3.9922152  2.19173415 3.9922152\n",
      " 3.9922152  3.9922152  4.38125546 4.38125546 2.19173415 3.9922152\n",
      " 2.58077441 4.27395956 2.19173415 2.19173415 4.66299982 4.38125546\n",
      " 3.9922152  3.9922152  3.9922152  3.9922152  3.9922152  3.9922152\n",
      " 3.9922152  2.19173415 3.9922152  3.9922152  3.9922152  2.6247282\n",
      " 4.38125546 3.9922152  3.9922152  3.46061492 4.38125546 4.38125546\n",
      " 3.9922152  3.9922152  3.9922152  3.9922152  4.38125546 3.9922152\n",
      " 2.58077441 3.9922152  4.38125546 3.9922152  3.9922152  4.38125546\n",
      " 4.38125546 4.38125546 4.38125546 2.19173415 3.01376847 2.19173415\n",
      " 2.19173415 2.19173415 3.9922152  4.38125546 3.9922152  4.38125546\n",
      " 2.90647257 3.9922152  3.9922152  4.38125546 3.9922152  4.38125546\n",
      " 3.9922152  3.9922152  3.9922152  3.9922152  3.9922152  3.9922152\n",
      " 2.6247282  4.38125546 4.38125546 3.9922152  2.19173415 3.9922152\n",
      " 2.19173415 3.84965518 4.38125546 3.9922152 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.853\n",
      "(*) epoch 2, cost 2.171\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.865\n",
      "(*) epoch 2, cost 3.408\n",
      "(*) epoch 3, cost 3.167\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.94874206 2.85222117 2.85222117 2.85222117 2.18553878 0.76320328\n",
      " 0.09652089 0.         2.85222117 2.85222117 0.76320328 2.18553878\n",
      " 2.18553878 2.85222117 2.94874206 2.94874206 2.85222117 2.18553878\n",
      " 2.28205967 2.28205967 2.85222117 2.85222117 2.28205967 2.85222117\n",
      " 2.85222117 2.85222117 2.85222117 2.18553878 2.18553878 2.85222117\n",
      " 2.18553878 0.66668239 2.94874206 2.18553878 0.66668239 0.66668239\n",
      " 2.94874206 2.18553878 2.85222117 0.76320328 2.18553878 2.85222117\n",
      " 0.         0.66668239 0.         0.09652089 2.85222117 0.66668239\n",
      " 2.94874206 2.85222117 0.66668239 0.76320328 2.85222117 2.85222117\n",
      " 2.85222117 0.         2.85222117 2.85222117 0.66668239 2.85222117\n",
      " 0.66668239 0.         2.85222117 2.85222117 0.66668239 2.85222117\n",
      " 2.94874206 0.09652089 0.66668239 0.66668239 2.94874206 2.85222117\n",
      " 2.85222117 2.85222117 0.66668239 0.         2.85222117 0.\n",
      " 2.18553878 2.85222117 2.85222117 0.09652089 2.85222117 2.28205967\n",
      " 2.18553878 0.66668239 0.66668239 2.85222117 0.76320328 0.66668239\n",
      " 2.85222117 2.18553878 0.76320328 2.85222117 0.         2.94874206\n",
      " 2.85222117 2.85222117 2.85222117 0.66668239]\n",
      "all_sum is: [1.40523771 1.16509336 1.40523771 1.16509336 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.16509336 1.40523771 1.40523771\n",
      " 1.16509336 1.16509336 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.16509336 1.40523771 1.40523771 1.16509336\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.16509336 1.16509336 1.40523771 1.40523771 1.16509336 1.16509336\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.16509336 1.40523771 1.40523771 1.16509336 1.16509336\n",
      " 1.16509336 1.40523771 1.16509336 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771 1.40523771\n",
      " 1.40523771 1.40523771 1.40523771 1.16509336 1.40523771 1.16509336\n",
      " 1.40523771 1.40523771 1.40523771 1.40523771]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.397\n",
      "(*) epoch 2, cost 1.905\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.937\n",
      "(*) epoch 2, cost 2.376\n",
      "(*) epoch 3, cost 2.067\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 3.24723984 0.51095329 0.51095329 0.51095329 0.51095329 0.51095329\n",
      " 0.51095329 0.51095329 0.51095329 0.51095329]\n",
      "all_sum is: [1.2616626  1.2616626  2.80959749 2.80959749 2.80959749 2.80959749\n",
      " 1.2616626  1.2616626  2.80959749 1.2616626  1.2616626  2.80959749\n",
      " 1.2616626  2.80959749 2.80959749 2.80959749 1.2616626  1.2616626\n",
      " 1.2616626  2.80959749 2.80959749 1.2616626  1.2616626  1.2616626\n",
      " 2.80959749 1.2616626  2.80959749 2.80959749 1.54793489 2.80959749\n",
      " 2.80959749 1.2616626  1.2616626  1.2616626  1.2616626  1.2616626\n",
      " 1.2616626  2.80959749 2.80959749 1.2616626  2.80959749 1.2616626\n",
      " 2.80959749 2.80959749 1.2616626  1.2616626  2.80959749 2.80959749\n",
      " 2.80959749 2.80959749 2.80959749 1.2616626  1.2616626  2.80959749\n",
      " 2.80959749 2.80959749 2.80959749 1.2616626  1.2616626  1.2616626\n",
      " 1.2616626  2.80959749 2.80959749 2.80959749 2.80959749 1.2616626\n",
      " 1.2616626  1.2616626  2.88570366 2.80959749 2.80959749 2.80959749\n",
      " 1.2616626  1.2616626  2.80959749 1.2616626  2.80959749 2.80959749\n",
      " 2.80959749 2.80959749 2.80959749 1.2616626  1.2616626  2.80959749\n",
      " 1.2616626  2.80959749 1.2616626  1.2616626  2.80959749 1.2616626\n",
      " 2.80959749 2.80959749 1.2616626  2.80959749 1.2616626  2.80959749\n",
      " 1.2616626  2.80959749 2.79458482 1.2616626 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.352\n",
      "(*) epoch 2, cost 0.619\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.873\n",
      "(*) epoch 2, cost 1.827\n",
      "(*) epoch 3, cost 1.575\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.49749315 1.72147328 1.72147328 1.72147328 1.40594512 2.81302131\n",
      " 1.46736023 2.81302131 1.72147328 1.72147328 1.72147328 2.81302131\n",
      " 1.08207662 1.00383164 2.81302131 1.00383164 1.72147328 1.72147328\n",
      " 2.81302131 2.09537967 1.40594512 2.12358676 1.00383164 1.72147328\n",
      " 1.46736023 2.09537967 1.86947371 1.72147328 1.72147328 2.43210746\n",
      " 2.81302131 2.81302131 1.00383164 1.00383164 1.72147328 1.00383164\n",
      " 2.09537967 1.72147328 0.76654846 1.72147328 1.00383164 2.09537967\n",
      " 0.74971859 1.00383164 1.72147328 1.72147328 1.72147328 2.12358676\n",
      " 2.17362465 2.09537967 2.12358676 3.21513479 2.12358676 2.81302131\n",
      " 1.72147328 1.72147328 1.00383164 1.72147328 2.81302131 0.36443498\n",
      " 1.72147328 1.71446581 1.00383164 1.72147328 1.00383164 2.49749315\n",
      " 1.00383164 1.00383164 2.12358676 1.00383164 2.81302131 1.72147328\n",
      " 1.72147328 2.81302131 1.72147328 1.00383164 1.72147328 3.21513479\n",
      " 1.00383164 2.81302131 1.00383164 1.4841901  1.15183207 2.09537967\n",
      " 1.00383164 1.72147328 3.20812733 1.72147328 2.09537967 1.00383164\n",
      " 1.72147328 2.81302131 2.81302131 1.00383164 1.72147328 1.72147328\n",
      " 2.83422094 1.00383164 1.08207662 2.81302131]\n",
      "all_sum is: [1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.82420721 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366 1.71562366\n",
      " 1.71562366 1.71562366 1.71562366 1.71562366]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.890\n",
      "(*) epoch 2, cost 2.366\n",
      "(*) epoch 3, cost 2.103\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.285\n",
      "(*) epoch 2, cost 0.561\n",
      "(*) epoch 3, cost 0.291\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.57989818 2.29340532 0.57989818 0.57989818 0.         0.57989818\n",
      " 0.         0.57989818 0.57989818 0.57989818 0.57989818 2.29340532\n",
      " 0.         0.57989818 0.57989818 0.57989818 0.57989818 0.57989818\n",
      " 0.57989818 0.         0.         0.57989818 0.         0.57989818\n",
      " 0.57989818 0.57989818 0.57989818 0.57989818 0.57989818 2.29340532\n",
      " 0.         0.57989818 0.57989818 0.         0.         0.57989818\n",
      " 0.57989818 0.57989818 0.         2.29340532 0.         0.57989818\n",
      " 0.         0.         0.57989818 0.         0.         0.57989818\n",
      " 0.         0.57989818 0.         0.         0.57989818 0.\n",
      " 0.57989818 0.57989818 0.57989818 0.57989818 0.         0.57989818\n",
      " 0.         0.57989818 0.         0.         0.57989818 0.\n",
      " 0.57989818 2.29340532 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.57989818 0.57989818\n",
      " 0.         0.57989818 0.         1.71350713 0.         0.\n",
      " 0.57989818 0.57989818 1.71350713 0.57989818 0.57989818 0.57989818\n",
      " 0.         0.         0.         0.         0.57989818 0.57989818\n",
      " 0.         0.57989818 0.         0.        ]\n",
      "all_sum is: [1.40061003 1.40061003 1.95701001 1.40061003 1.40061003 1.51824252\n",
      " 1.40061003 3.56296567 1.40061003 1.51824252 1.40061003 1.83937752\n",
      " 1.40061003 1.40061003 1.40061003 1.83937752 1.40061003 1.83937752\n",
      " 1.95701001 1.83937752 1.83937752 1.83937752 1.51824252 1.95701001\n",
      " 1.51824252 1.51824252 1.40061003 1.95701001 1.40061003 3.56296567\n",
      " 1.51824252 1.95701001 1.51824252 1.95701001 1.51824252 1.40061003\n",
      " 1.40061003 1.83937752 1.83937752 1.95701001 3.12419818 1.40061003\n",
      " 1.40061003 1.51824252 1.51824252 1.40061003 1.51824252 3.24183067\n",
      " 1.51824252 1.83937752 1.51824252 1.83937752 1.40061003 1.83937752\n",
      " 1.40061003 1.40061003 1.51824252 1.51824252 1.83937752 1.40061003\n",
      " 1.95701001 1.40061003 1.51824252 3.12419818 1.40061003 1.51824252\n",
      " 1.83937752 1.95701001 1.51824252 1.95701001 1.95701001 1.95701001\n",
      " 1.95701001 1.40061003 1.40061003 1.83937752 1.83937752 3.56296567\n",
      " 3.12419818 1.51824252 1.40061003 1.40061003 1.83937752 1.95701001\n",
      " 1.95701001 1.95701001 1.40061003 1.40061003 1.40061003 1.51824252\n",
      " 1.40061003 1.51824252 1.40061003 1.95701001 1.95701001 1.83937752\n",
      " 1.51824252 1.51824252 1.83937752 1.40061003]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.298\n",
      "(*) epoch 2, cost 1.401\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.193\n",
      "(*) epoch 2, cost 3.335\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 1.93581056 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295 0.94435295\n",
      " 0.94435295 0.94435295 0.94435295 0.94435295]\n",
      "all_sum is: [0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172 0.07560172\n",
      " 0.07560172 0.07560172 0.07560172 0.07560172]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.968\n",
      "(*) epoch 2, cost 1.378\n",
      "(*) epoch 3, cost 1.222\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.971\n",
      "(*) epoch 2, cost 0.651\n",
      "(*) epoch 3, cost 0.560\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.39807818 1.445294   1.94595112 1.445294   1.445294   1.39807818\n",
      " 1.445294   1.40993641 1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.40993641\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.40993641\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.8988952  1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.40993641 1.445294   1.445294\n",
      " 1.445294   1.39807818 1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.94595112 1.445294   1.40993641 2.31133955 1.445294   1.445294\n",
      " 1.445294   1.445294   1.94595112 1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.39807818 1.445294   1.445294\n",
      " 1.445294   1.445294   1.445294   1.445294   1.445294   1.445294\n",
      " 1.445294   1.445294   2.31133955 1.39807818]\n",
      "all_sum is: [1.61611805 1.61611805 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 1.61611805\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 1.61611805 1.61611805 0.11431573 0.11431573 0.11431573\n",
      " 1.61611805 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 1.61611805 0.11431573 0.11431573 1.61611805 0.17572313\n",
      " 0.11431573 1.61611805 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 1.61611805 0.11431573 1.61611805 0.11431573 0.11431573\n",
      " 0.17572313 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 1.61611805 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573\n",
      " 0.11431573 0.11431573 0.11431573 0.11431573 0.11431573 1.61611805\n",
      " 0.11431573 1.61611805 0.11431573 0.11431573]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.049\n",
      "(*) epoch 2, cost 1.229\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.097\n",
      "(*) epoch 2, cost 1.378\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.05762937 0.93342873 0.93342873 0.08409705 0.93342873 0.93342873\n",
      " 3.48371881 0.         1.51602918 0.93342873 1.01752577 0.93342873\n",
      " 0.93342873 2.55029008 0.93342873 3.48371881 0.08409705 0.\n",
      " 0.         0.08409705 0.08409705 3.48371881 1.07777042 1.97353232\n",
      " 0.93342873 1.12420064 2.47203573 1.04010359 3.48371881 0.\n",
      " 0.49850341 0.93342873 0.93342873 0.93342873 0.         0.93342873\n",
      " 0.93342873 1.07777042 0.93342873 3.48371881 0.         2.55029008\n",
      " 0.         0.         0.         2.05762937 1.04010359 1.97353232\n",
      " 0.         0.93342873 0.93342873 0.         0.93342873 2.55029008\n",
      " 0.         0.93342873 1.538607   4.5238224  0.         0.\n",
      " 0.93342873 0.         0.93342873 0.         0.         0.93342873\n",
      " 0.93342873 0.         0.93342873 0.93342873 0.93342873 0.93342873\n",
      " 1.01752577 0.93342873 0.         0.93342873 1.97353232 0.\n",
      " 0.         0.93342873 0.93342873 3.48371881 1.04010359 0.\n",
      " 1.97353232 0.93342873 0.         2.63438712 1.97353232 0.93342873\n",
      " 1.04010359 0.         3.48371881 3.48371881 0.93342873 0.49850341\n",
      " 0.93342873 1.97353232 3.48371881 1.97353232]\n",
      "all_sum is: [0.02478975 0.02478975 0.02478975 0.02478975 0.02478975 0.27448967\n",
      " 0.02478975 0.02478975 0.55746279 0.02478975 0.02478975 0.02478975\n",
      " 0.02478975 0.02478975 0.27448967 0.02478975 0.27448967 0.02478975\n",
      " 0.02478975 0.02478975 0.02478975 0.55746279 0.02478975 0.02478975\n",
      " 0.55746279 0.55746279 0.02478975 0.02478975 0.02478975 0.02478975\n",
      " 0.02478975 0.02478975 0.02478975 0.02478975 0.02478975 0.55746279\n",
      " 0.02478975 0.         0.02478975 0.53267304 0.55746279 0.02478975\n",
      " 0.02478975 0.80716271 0.02478975 0.02478975 0.27448967 0.27448967\n",
      " 0.55746279 0.02478975 0.02478975 0.02478975 0.02478975 0.02478975\n",
      " 0.55746279 0.02478975 0.02478975 0.55746279 0.55746279 0.02478975\n",
      " 0.55746279 0.02478975 0.         0.27448967 0.02478975 0.02478975\n",
      " 0.02478975 0.02478975 0.55746279 0.02478975 0.02478975 0.02478975\n",
      " 0.         0.02478975 0.02478975 0.55746279 0.02478975 0.55746279\n",
      " 0.02478975 0.53267304 0.02478975 0.02478975 0.         0.53267304\n",
      " 0.02478975 0.02478975 0.02478975 0.27448967 0.55746279 0.02478975\n",
      " 0.02478975 0.02478975 0.55746279 0.02478975 0.02478975 0.02478975\n",
      " 0.02478975 0.55746279 0.02478975 0.02478975]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.596\n",
      "(*) epoch 2, cost 2.561\n",
      "(*) epoch 3, cost 2.447\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.388\n",
      "(*) epoch 2, cost 1.571\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601 0.17462601\n",
      " 0.17462601 0.17462601 0.17462601 0.17462601]\n",
      "all_sum is: [0.53064571 0.53064571 0.         0.53064571 0.         0.53064571\n",
      " 0.53064571 0.         0.53064571 0.         0.53064571 0.53064571\n",
      " 0.53064571 0.         0.53064571 0.53064571 0.53064571 0.\n",
      " 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.53064571 0.         0.         0.53064571\n",
      " 0.53064571 0.         0.         0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.         0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.         0.53064571 0.53064571 0.53064571\n",
      " 0.         0.99722651 0.         0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.         0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.         0.         0.         0.53064571 0.53064571 0.53064571\n",
      " 0.         0.         0.53064571 0.53064571 0.53064571 0.53064571\n",
      " 0.53064571 0.53064571 0.         0.53064571]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.980\n",
      "(*) epoch 2, cost 1.385\n",
      "(*) epoch 3, cost 1.142\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.526\n",
      "(*) epoch 2, cost 2.220\n",
      "(*) epoch 3, cost 1.860\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.84095436 0.84095436 1.63192584 1.75995372 1.63192584 0.84095436\n",
      " 0.71520213 1.18942443 0.84095436 0.39845296 1.63192584 1.63192584\n",
      " 1.63192584 1.63192584 0.39845296 0.84095436 0.84095436 1.63192584\n",
      " 1.63192584 1.75995372 1.63192584 1.63192584 1.63192584 0.71520213\n",
      " 1.63192584 1.63192584 1.63192584 0.84095436 1.63192584 1.63192584\n",
      " 1.50617361 1.63192584 0.39845296 1.63192584 0.71520213 0.84095436\n",
      " 1.0636722  0.84095436 1.63192584 0.71520213 0.71520213 0.84095436\n",
      " 0.84095436 0.27270072 1.63192584 1.18942443 0.84095436 0.84095436\n",
      " 1.63192584 1.63192584 1.63661366 0.84095436 0.84095436 0.84095436\n",
      " 0.84095436 1.63192584 1.63192584 1.63192584 1.63192584 0.84095436\n",
      " 0.84095436 0.84095436 0.84095436 1.63192584 0.84095436 0.84095436\n",
      " 0.84095436 1.50617361 0.84095436 1.18942443 1.63192584 1.50617361\n",
      " 1.69089733 0.84095436 1.18942443 0.84095436 1.63192584 1.63192584\n",
      " 0.84095436 1.63192584 0.84095436 1.63192584 1.63192584 0.84095436\n",
      " 1.63192584 1.50617361 1.63192584 1.63192584 0.71520213 1.63192584\n",
      " 1.0636722  1.50617361 0.39845296 1.18942443 0.71520213 1.63192584\n",
      " 0.96898224 0.84095436 0.84095436 0.84095436]\n",
      "all_sum is: [2.48943468 5.0568731  6.36768259 5.0568731  2.48943468 5.0568731\n",
      " 3.80024417 5.0568731  5.0568731  6.36768259 5.0568731  6.36768259\n",
      " 2.48943468 2.48943468 5.0568731  6.36768259 3.80024417 3.80024417\n",
      " 5.0568731  6.36768259 3.80024417 6.36768259 6.36768259 5.0568731\n",
      " 6.36768259 6.36768259 5.0568731  3.80024417 6.36768259 6.36768259\n",
      " 5.0568731  3.80024417 3.80024417 2.48943468 3.80024417 5.0568731\n",
      " 3.80024417 6.36768259 6.36768259 6.36768259 6.36768259 6.36768259\n",
      " 6.36768259 2.48943468 6.36768259 5.0568731  5.0568731  3.80024417\n",
      " 6.36768259 6.36768259 2.48943468 6.36768259 6.36768259 5.0568731\n",
      " 2.48943468 2.48943468 3.80024417 6.36768259 6.36768259 2.48943468\n",
      " 5.0568731  3.80024417 6.36768259 6.36768259 3.80024417 5.0568731\n",
      " 6.36768259 2.48943468 3.80024417 5.0568731  3.80024417 3.80024417\n",
      " 6.36768259 5.0568731  2.48943468 5.0568731  5.0568731  6.36768259\n",
      " 5.0568731  6.36768259 5.0568731  5.0568731  3.80024417 3.80024417\n",
      " 2.48943468 2.48943468 5.0568731  5.0568731  3.80024417 6.36768259\n",
      " 2.48943468 2.48943468 2.48943468 6.36768259 2.48943468 6.36768259\n",
      " 5.0568731  5.0568731  6.36768259 5.0568731 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.932\n",
      "(*) epoch 2, cost 3.028\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.468\n",
      "(*) epoch 2, cost 2.995\n",
      "(*) epoch 3, cost 2.688\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.35391367 0.35391367 0.35391367 0.35391367 0.35391367 0.89623506\n",
      " 0.35391367 0.35391367 0.35391367 0.35396066 0.35391367 0.85049251\n",
      " 0.35391367 0.35391367 0.35391367 0.35396066 0.89623506 0.35391367\n",
      " 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35391367 0.35391367 0.35391367 0.35396066 0.35391367 0.97688559\n",
      " 0.35396066 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35391367 0.89623506 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35391367 0.85049251 0.35391367 0.35391367 0.35396066 0.35391367\n",
      " 0.35391367 0.89623506 0.35391367 0.35391367 0.97688559 0.35391367\n",
      " 0.97688559 0.35391367 0.35391367 0.35396066 0.35391367 0.97688559\n",
      " 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367 0.85049251\n",
      " 0.35396066 0.35391367 0.35391367 0.35391367 0.35391367 0.35396066\n",
      " 0.35391367 0.35396066 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35396066 0.35396066 0.35391367 0.35391367 0.35391367 0.35391367\n",
      " 0.35391367 0.35391367 0.35391367 0.35396066 0.35396066 0.35391367\n",
      " 0.97688559 0.35391367 0.35391367 0.35391367]\n",
      "all_sum is: [0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 1.25759198\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 1.25759198 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 1.25759198 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 1.25759198 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 1.25759198 0.17174211 0.17174211 0.17174211\n",
      " 0.17174211 0.17174211 0.17174211 0.17174211]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.592\n",
      "(*) epoch 2, cost 1.256\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.863\n",
      "(*) epoch 2, cost 2.443\n",
      "(*) epoch 3, cost 2.141\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.3350013  3.2457857  1.93701181 2.30666288 2.30666288 1.93701181\n",
      " 2.36524825 3.99388897 1.93701181 1.93701181 2.36524825 1.93701181\n",
      " 2.30666288 2.36524825 2.30666288 1.93701181 1.93701181 3.33520162\n",
      " 1.93701181 1.93701181 1.93701181 2.30666288 2.36524825 2.30666288\n",
      " 2.30666288 2.30666288 3.3350013  2.30666288 2.30666288 2.30666288\n",
      " 2.30666288 2.30666288 2.30666288 2.30666288 2.30666288 2.30666288\n",
      " 3.27661625 2.30666288 2.30666288 1.99559719 2.30666288 3.27661625\n",
      " 2.30666288 2.30666288 1.53249416 2.30666288 1.93701181 3.9353036\n",
      " 2.30666288 1.99559719 1.93701181 3.61543677 3.27661625 2.30666288\n",
      " 1.93701181 1.99559719 2.30666288 2.30666288 2.30666288 3.27661625\n",
      " 3.27661625 2.30666288 0.8327541  2.30666288 3.27661625 2.36524825\n",
      " 2.30666288 2.30666288 2.30666288 1.53249416 3.27661625 2.30666288\n",
      " 1.99559719 2.30666288 3.61543677 2.30666288 1.93701181 3.27661625\n",
      " 1.93701181 1.99559719 2.30666288 2.90696518 1.99559719 1.93701181\n",
      " 2.30666288 3.27661625 1.99559719 2.96535023 1.93701181 2.36524825\n",
      " 2.36524825 1.93701181 1.93701181 3.33520162 2.50244752 2.30666288\n",
      " 2.56083258 3.27661625 1.93701181 2.30666288]\n",
      "all_sum is: [1.09221361 1.09221361 1.67633161 1.09221361 1.09221361 1.09221361\n",
      " 1.09221361 1.67633161 2.33606855 1.09221361 1.09221361 2.33606855\n",
      " 2.33606855 1.09221361 1.09221361 1.67633161 1.09221361 1.09221361\n",
      " 1.09221361 1.67633161 1.67633161 1.09221361 2.33606855 1.52623801\n",
      " 1.09221361 1.09221361 2.92018655 1.09221361 1.67633161 1.09221361\n",
      " 1.09221361 1.09221361 1.09221361 1.09221361 1.67633161 1.09221361\n",
      " 2.33606855 1.09221361 1.09221361 1.09221361 1.09221361 1.09221361\n",
      " 2.33606855 1.09221361 2.33606855 1.09221361 1.09221361 1.09221361\n",
      " 2.92018655 1.09221361 2.92018655 1.09221361 1.09221361 1.67633161\n",
      " 2.33606855 1.09221361 1.09221361 2.33606855 1.09221361 1.09221361\n",
      " 1.09221361 1.09221361 2.33606855 1.09221361 1.67633161 1.09221361\n",
      " 1.09221361 1.67633161 1.67633161 1.67633161 1.09221361 1.09221361\n",
      " 1.09221361 1.09221361 1.67633161 1.09221361 1.09221361 1.67633161\n",
      " 1.09221361 1.09221361 1.09221361 1.09221361 1.09221361 1.67633161\n",
      " 1.09221361 1.09221361 1.09221361 1.67633161 1.09221361 1.09221361\n",
      " 1.09221361 2.33606855 2.33606855 1.09221361 1.67633161 1.09221361\n",
      " 1.67633161 1.67633161 1.09221361 1.09221361]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.634\n",
      "(*) epoch 2, cost 3.576\n",
      "(*) epoch 3, cost 3.509\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.766\n",
      "(*) epoch 2, cost 1.377\n",
      "(*) epoch 3, cost 1.128\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.1168983  2.78574682 3.1168983  2.78574682 2.78574682 3.81813685\n",
      " 2.78574682 2.78574682 3.48662546 2.78574682 2.78574682 2.78574682\n",
      " 3.1168983  3.1168983  3.1168983  2.78574682 2.78574682 2.78574682\n",
      " 2.78574682 2.78574682 3.1168983  3.1168983  3.1168983  3.1168983\n",
      " 2.78574682 2.78574682 3.1168983  3.1168983  3.1168983  2.78574682\n",
      " 2.78574682 3.1168983  2.78574682 2.78574682 3.1168983  2.78574682\n",
      " 3.1168983  3.1168983  3.1168983  3.1168983  3.1168983  2.04550862\n",
      " 3.1168983  3.1168983  2.78574682 2.78574682 2.78574682 2.78574682\n",
      " 3.1168983  2.78574682 3.1168983  2.78574682 2.78574682 2.78574682\n",
      " 3.1168983  3.1168983  2.78574682 2.78574682 3.81777694 3.1168983\n",
      " 3.1168983  2.78574682 1.71435714 3.1168983  3.1168983  2.78574682\n",
      " 2.78574682 3.81777694 2.78574682 2.78574682 3.1168983  3.1168983\n",
      " 2.78574682 2.78574682 3.1168983  2.78574682 3.1168983  2.78574682\n",
      " 3.1168983  2.78574682 2.78574682 3.1168983  3.1168983  2.78574682\n",
      " 2.78574682 2.78574682 3.1168983  3.1168983  2.78574682 2.78574682\n",
      " 3.1168983  3.1168983  2.78574682 3.1168983  2.78574682 2.78574682\n",
      " 2.78574682 3.1168983  3.48662546 2.78574682]\n",
      "all_sum is: [0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719 0.66786719\n",
      " 0.66786719 0.66786719 0.66786719 0.66786719]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.664\n",
      "(*) epoch 2, cost 3.677\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.445\n",
      "(*) epoch 2, cost 1.302\n",
      "(*) epoch 3, cost 0.964\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.45672972 4.34180819 3.45672972 4.41566951 3.45672972 4.41566951\n",
      " 3.45672972 3.45672972 3.45672972 4.34180819 4.41566951 3.45672972\n",
      " 3.45672972 4.41566951 4.41566951 4.41566951 3.45672972 3.45672972\n",
      " 3.45672972 3.45672972 3.45672972 4.34180819 4.41566951 3.45672972\n",
      " 3.45672972 3.45672972 4.41566951 3.45672972 3.45672972 3.45672972\n",
      " 3.45672972 3.45672972 3.45672972 4.41566951 3.45672972 4.41566951\n",
      " 3.45672972 3.45672972 5.30074799 3.45672972 3.45672972 4.41566951\n",
      " 3.45672972 3.45672972 3.45672972 4.34180819 3.45672972 3.45672972\n",
      " 3.45672972 3.45672972 3.45672972 3.45672972 3.45672972 4.41566951\n",
      " 4.34180819 3.45672972 4.41566951 3.45672972 3.45672972 3.45672972\n",
      " 3.45672972 3.45672972 3.45672972 3.45672972 4.41566951 3.45672972\n",
      " 3.45672972 3.45672972 4.34180819 3.45672972 4.41566951 4.41566951\n",
      " 3.45672972 3.45672972 4.41566951 4.34180819 3.45672972 3.45672972\n",
      " 4.41566951 3.45672972 4.41566951 4.41566951 3.45672972 3.45672972\n",
      " 3.45672972 4.34180819 3.45672972 3.45672972 3.45672972 4.41566951\n",
      " 3.45672972 3.45672972 4.34180819 4.34180819 5.30074799 3.45672972\n",
      " 4.34180819 3.45672972 3.45672972 3.45672972]\n",
      "all_sum is: [3.04719948 3.04719948 3.04719948 3.25580086 3.04719948 3.04719948\n",
      " 3.04719948 3.04719948 3.95661197 3.04719948 3.04719948 3.04719948\n",
      " 4.16521335 3.04719948 3.25580086 3.04719948 3.04719948 3.25580086\n",
      " 3.04719948 3.04719948 3.25580086 3.04719948 3.04719948 3.04719948\n",
      " 3.04719948 3.25580086 3.25580086 4.16521335 3.04719948 3.25580086\n",
      " 3.25580086 3.25580086 3.25580086 3.25580086 3.04719948 3.25580086\n",
      " 3.25580086 3.95661197 3.04719948 3.04719948 3.04719948 3.04719948\n",
      " 3.04719948 3.04719948 3.04719948 3.04719948 3.04719948 3.25580086\n",
      " 3.25580086 3.04719948 3.25580086 3.25580086 3.25580086 3.25580086\n",
      " 3.04719948 3.04719948 3.04719948 3.04719948 3.95661197 3.04719948\n",
      " 3.04719948 3.04719948 3.04719948 3.04719948 3.04719948 3.25580086\n",
      " 3.04719948 3.25580086 3.25580086 3.04719948 3.04719948 3.25580086\n",
      " 3.04719948 3.25580086 3.25580086 3.04719948 3.04719948 3.04719948\n",
      " 3.25580086 3.25580086 3.25580086 3.25580086 3.04719948 3.04719948\n",
      " 3.04719948 3.25580086 3.04719948 3.04719948 3.25580086 3.25580086\n",
      " 3.04719948 3.04719948 4.16521335 3.25580086 3.25580086 3.25580086\n",
      " 3.04719948 3.25580086 3.04719948 3.95661197]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.453\n",
      "(*) epoch 2, cost 2.367\n",
      "(*) epoch 3, cost 2.046\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.894\n",
      "(*) epoch 2, cost 3.270\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.99808917 2.99808917 3.27181541 2.25023685 4.25203489 3.27181541\n",
      " 2.99808917 2.99808917 2.99808917 2.25023685 2.99808917 2.38053432\n",
      " 2.99808917 2.99808917 2.99808917 3.27181541 2.99808917 2.99808917\n",
      " 3.27181541 2.99808917 2.25023685 2.99808917 2.99808917 2.25023685\n",
      " 2.25023685 2.99808917 2.25023685 2.99808917 3.27181541 2.99808917\n",
      " 3.27181541 3.12838664 2.25023685 2.52396309 2.99808917 3.27181541\n",
      " 3.27181541 2.99808917 2.99808917 2.99808917 3.27181541 2.52396309\n",
      " 2.52396309 2.25023685 2.25023685 2.99808917 3.27181541 3.27181541\n",
      " 2.25023685 2.25023685 3.27181541 2.99808917 2.99808917 2.52396309\n",
      " 2.99808917 2.65426056 2.99808917 2.25023685 3.12838664 2.52396309\n",
      " 3.27181541 3.27181541 3.27181541 2.52396309 2.99808917 2.99808917\n",
      " 2.99808917 3.27181541 2.99808917 2.25023685 2.25023685 2.99808917\n",
      " 3.27181541 3.27181541 3.27181541 2.25023685 2.99808917 2.99808917\n",
      " 2.99808917 2.99808917 2.99808917 3.27181541 2.25023685 2.99808917\n",
      " 3.27181541 2.99808917 2.99808917 2.99808917 2.25023685 3.27181541\n",
      " 2.25023685 3.27181541 2.99808917 3.27181541 3.27181541 2.25023685\n",
      " 2.25023685 3.97830865 2.99808917 2.25023685]\n",
      "all_sum is: [2.52956746 2.52956746 2.52956746 2.52956746 2.61530936 2.52956746\n",
      " 2.61530936 2.52956746 2.52956746 2.52956746 2.52956746 2.69088704\n",
      " 2.52956746 2.40314297 2.52956746 2.52956746 2.69088704 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.69088704 2.61530936 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.69088704 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.69088704 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.61530936 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 3.24660192 2.52956746\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746 2.52956746\n",
      " 2.52956746 2.69088704 2.52956746 2.69088704 2.52956746 2.52956746\n",
      " 2.52956746 2.52956746 2.61530936 2.52956746 2.61530936 2.61530936\n",
      " 2.52956746 2.52956746 2.52956746 2.52956746]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.226\n",
      "(*) epoch 2, cost 2.516\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.487\n",
      "(*) epoch 2, cost 2.891\n",
      "(*) epoch 3, cost 2.667\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 1.53998592 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 3.62471176 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 1.53998592 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937 2.40550937\n",
      " 2.40550937 2.40550937 1.53998592 2.40550937]\n",
      "all_sum is: [1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 2.02724198\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 2.02724198 0.60033873 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 2.02724198 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 2.02724198 1.37147881 2.02724198 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 2.02724198\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 0.60033873\n",
      " 2.02724198 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 0.60033873\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 0.60033873\n",
      " 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.76609474 1.37147881 1.37147881 2.02724198 2.02724198\n",
      " 1.37147881 1.37147881 1.37147881 2.02724198 1.37147881 1.37147881\n",
      " 1.37147881 1.37147881 2.02724198 1.37147881 1.37147881 1.37147881\n",
      " 1.37147881 1.11033156 1.37147881 1.37147881]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.705\n",
      "(*) epoch 2, cost 1.490\n",
      "(*) epoch 3, cost 1.370\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.819\n",
      "(*) epoch 2, cost 2.563\n",
      "(*) epoch 3, cost 2.360\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  2.02640701 0.7768927  2.02640701 0.7768927  0.7768927\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  2.02640701 2.02640701\n",
      " 2.02640701 2.02640701 0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 2.02640701 2.02640701 0.7768927  0.7768927  2.02640701 0.7768927\n",
      " 0.7768927  2.02640701 2.02640701 0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  0.7768927\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  0.7768927\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  0.7768927\n",
      " 2.02640701 0.7768927  2.02640701 0.7768927  0.7768927  0.7768927\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  2.02640701\n",
      " 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927  0.7768927\n",
      " 2.02640701 0.7768927  0.7768927  0.7768927  0.7768927  0.7768927\n",
      " 0.7768927  0.7768927  2.02640701 0.7768927 ]\n",
      "all_sum is: [2.78451342 2.08919086 2.78451342 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.78451342 4.21374324 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 4.21374324 2.78451342 2.08919086 2.08919086\n",
      " 2.78451342 2.78451342 2.08919086 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.78451342 2.78451342 4.21374324 2.08919086\n",
      " 2.78451342 2.78451342 2.78451342 2.78451342 2.08919086 2.78451342\n",
      " 2.08919086 2.78451342 2.78451342 2.78451342 2.78451342 4.21374324\n",
      " 2.08919086 4.21374324 2.78451342 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.78451342 2.78451342 2.78451342 2.78451342\n",
      " 2.08919086 2.08919086 2.78451342 4.21374324 2.08919086 2.78451342\n",
      " 2.08919086 3.51842068 2.08919086 2.78451342 2.08919086 2.08919086\n",
      " 2.78451342 2.08919086 2.78451342 2.78451342 2.78451342 2.08919086\n",
      " 2.78451342 4.21374324 2.78451342 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.78451342 2.08919086 2.08919086 2.78451342\n",
      " 2.08919086 2.78451342 4.21374324 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.08919086 2.78451342 2.78451342 2.78451342\n",
      " 2.78451342 2.78451342 2.08919086 3.51842068]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.736\n",
      "(*) epoch 2, cost 2.334\n",
      "(*) epoch 3, cost 2.015\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.544\n",
      "(*) epoch 2, cost 1.757\n",
      "(*) epoch 3, cost 1.499\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 4.59206229 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 4.59206229 3.20565425 4.67349374 3.20565425\n",
      " 4.59206229 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 4.67349374 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 4.59206229\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425 3.20565425\n",
      " 3.20565425 4.59206229 3.20565425 3.20565425]\n",
      "all_sum is: [1.88463988 0.99178914 0.89285074 0.99178914 1.88463988 0.85520045\n",
      " 0.99178914 0.         0.         1.88463988 1.88463988 1.88463988\n",
      " 1.88463988 0.         1.88463988 1.88463988 1.88463988 1.88463988\n",
      " 0.99178914 0.99178914 0.89285074 2.73984033 1.88463988 1.88463988\n",
      " 0.99178914 1.88463988 1.88463988 0.99178914 1.88463988 0.99178914\n",
      " 1.88463988 0.         1.88463988 0.99178914 1.88463988 1.88463988\n",
      " 1.88463988 1.88463988 0.99178914 0.99178914 0.         0.99178914\n",
      " 1.88463988 0.99178914 0.89285074 1.88463988 0.99178914 0.99178914\n",
      " 1.88463988 1.88463988 1.88463988 1.88463988 0.85520045 0.\n",
      " 0.         1.88463988 1.88463988 0.99178914 0.99178914 0.99178914\n",
      " 1.88463988 1.88463988 1.88463988 1.88463988 0.99178914 0.99178914\n",
      " 1.88463988 0.         1.88463988 1.88463988 1.88463988 1.88463988\n",
      " 0.89285074 1.88463988 0.         1.88463988 0.99178914 0.99178914\n",
      " 0.         0.89285074 0.89285074 0.         1.88463988 0.99178914\n",
      " 1.88463988 1.88463988 1.88463988 1.88463988 0.99178914 0.\n",
      " 0.89285074 0.99178914 0.89285074 0.89285074 1.84698958 0.99178914\n",
      " 0.99178914 0.89285074 0.         0.99178914]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.207\n",
      "(*) epoch 2, cost 1.644\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.141\n",
      "(*) epoch 2, cost 3.540\n",
      "(*) epoch 3, cost 3.140\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.02923752 1.02923752 0.         1.02923752 1.02923752 0.\n",
      " 1.02923752 1.02923752 1.73120324 1.02923752 1.02923752 0.\n",
      " 0.         0.         0.         0.         1.02923752 1.02923752\n",
      " 1.02923752 1.02923752 1.02923752 1.02923752 0.         1.02923752\n",
      " 1.02923752 1.02923752 1.02923752 0.         0.         0.\n",
      " 0.         0.         1.02923752 1.02923752 0.         0.\n",
      " 0.         0.         1.02923752 0.         0.         1.02923752\n",
      " 1.02923752 1.02923752 1.02923752 0.         0.         1.02923752\n",
      " 1.30324497 1.02923752 1.02923752 1.02923752 0.         1.02923752\n",
      " 0.         1.02923752 1.02923752 1.02923752 0.         1.02923752\n",
      " 1.02923752 0.         0.         1.02923752 0.         1.02923752\n",
      " 1.02923752 0.         1.02923752 1.02923752 1.02923752 1.02923752\n",
      " 0.         0.         0.         0.         1.02923752 0.\n",
      " 0.         0.         0.         0.         1.02923752 1.02923752\n",
      " 1.02923752 1.02923752 0.         0.         0.         1.02923752\n",
      " 0.         1.02923752 1.02923752 1.02923752 0.         0.\n",
      " 0.         1.02923752 1.02923752 0.        ]\n",
      "all_sum is: [0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  2.04860222 0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349  0.8730349  0.8730349\n",
      " 0.8730349  0.8730349  0.8730349  0.8730349 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.929\n",
      "(*) epoch 2, cost 1.760\n",
      "(*) epoch 3, cost 1.311\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.456\n",
      "(*) epoch 2, cost 0.420\n",
      "(*) epoch 3, cost 0.282\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.99265589 4.99265589 4.99265589 4.36278181 4.99265589 4.9688078\n",
      " 4.99265589 4.36278181 4.99265589 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.99265589 4.99265589 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.36278181 4.99265589 4.99265589 4.99265589\n",
      " 4.36278181 4.99265589 4.9688078  4.99265589 4.36278181 4.99265589\n",
      " 4.36278181 4.99265589 4.36278181 4.99265589 4.99265589 4.36278181\n",
      " 4.36278181 4.36278181 5.59868188 4.99265589 4.36278181 4.36278181\n",
      " 4.99265589 4.99265589 4.99265589 4.99265589 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.99265589 4.36278181 4.99265589 4.36278181\n",
      " 4.36278181 4.99265589 4.36278181 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.36278181 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.36278181 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.99265589 4.36278181 4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.99265589 4.9688078  4.36278181 4.99265589\n",
      " 4.99265589 4.36278181 4.99265589 4.36278181 4.9688078  4.36278181\n",
      " 4.36278181 4.36278181 4.36278181 5.7620361  4.36278181 4.36278181\n",
      " 4.36278181 4.36278181 4.36278181 4.99265589]\n",
      "all_sum is: [1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  0.20123101 1.0228958  1.0228958  0.20123101 1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 0.20123101 1.0228958  0.20123101 1.0228958  1.0228958  0.20123101\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  0.20123101 1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  0.20123101 1.0228958\n",
      " 0.20123101 1.0228958  1.0228958  1.0228958  0.20123101 1.0228958\n",
      " 0.20123101 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  0.20123101 1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  1.0228958\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958  1.0228958  0.20123101\n",
      " 1.0228958  1.0228958  1.0228958  1.0228958 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.170\n",
      "(*) epoch 2, cost 2.474\n",
      "(*) epoch 3, cost 2.151\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.849\n",
      "(*) epoch 2, cost 0.980\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.30853117 4.14848126 1.07741952 2.30853117 2.76826046 2.30853117\n",
      " 2.91736961 1.07741952 2.30853117 2.91736961 2.30853117 1.07741952\n",
      " 2.91736961 5.8393222  2.30853117 1.07741952 2.76826046 2.30853117\n",
      " 2.91736961 2.30853117 1.07741952 1.07741952 1.07741952 2.30853117\n",
      " 3.99937211 1.07741952 2.30853117 3.99937211 2.30853117 3.99937211\n",
      " 4.60821055 4.14848126 2.30853117 5.8393222  2.91736961 2.30853117\n",
      " 2.30853117 2.30853117 4.14848126 4.14848126 1.07741952 2.91736961\n",
      " 5.8393222  4.14848126 2.91736961 2.30853117 2.30853117 1.07741952\n",
      " 1.07741952 1.07741952 2.91736961 2.76826046 2.30853117 1.07741952\n",
      " 5.8393222  1.07741952 4.14848126 3.99937211 2.30853117 2.30853117\n",
      " 4.14848126 2.30853117 4.14848126 2.91736961 2.91736961 2.30853117\n",
      " 2.76826046 2.91736961 4.14848126 1.07741952 2.91736961 4.14848126\n",
      " 2.30853117 1.07741952 3.99937211 3.13517204 4.60821055 5.8393222\n",
      " 2.30853117 1.07741952 2.30853117 2.30853117 1.07741952 1.07741952\n",
      " 1.07741952 4.14848126 2.76826046 2.30853117 4.14848126 3.99937211\n",
      " 2.76826046 2.30853117 2.30853117 2.30853117 3.99937211 2.76826046\n",
      " 2.30853117 3.99937211 2.30853117 1.07741952]\n",
      "all_sum is: [2.3577683  3.04236335 2.6060634  3.04236335 2.6060634  2.3577683\n",
      " 2.3577683  2.6060634  3.04236335 3.04236335 2.3577683  3.04236335\n",
      " 2.6060634  2.3577683  3.04236335 3.29065846 2.3577683  2.3577683\n",
      " 3.04236335 2.3577683  2.3577683  2.3577683  2.6060634  2.3577683\n",
      " 2.3577683  2.3577683  3.04236335 3.04236335 2.3577683  2.3577683\n",
      " 3.04236335 4.01651659 2.3577683  3.29065846 2.6060634  2.3577683\n",
      " 3.04236335 2.3577683  2.6060634  2.3577683  2.6060634  2.3577683\n",
      " 3.04236335 3.04236335 3.04236335 2.3577683  3.04236335 2.3577683\n",
      " 2.6060634  2.3577683  3.04236335 2.3577683  2.3577683  3.04236335\n",
      " 3.04236335 2.3577683  2.3577683  2.3577683  2.3577683  3.04236335\n",
      " 2.3577683  3.04236335 2.3577683  3.29065846 2.3577683  3.04236335\n",
      " 2.3577683  2.3577683  2.6060634  2.3577683  2.3577683  2.3577683\n",
      " 2.3577683  3.04236335 2.3577683  3.04236335 2.3577683  2.3577683\n",
      " 3.04236335 2.3577683  3.04236335 2.3577683  2.3577683  3.04236335\n",
      " 2.3577683  2.3577683  3.04236335 2.3577683  3.04236335 3.04236335\n",
      " 3.04236335 2.3577683  2.3577683  3.04236335 2.3577683  2.3577683\n",
      " 2.3577683  2.3577683  2.3577683  3.04236335]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.937\n",
      "(*) epoch 2, cost 3.664\n",
      "(*) epoch 3, cost 3.294\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.545\n",
      "(*) epoch 2, cost 2.252\n",
      "(*) epoch 3, cost 2.021\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.13297371 1.10179683 0.65001757 0.65001757 1.13297371 1.10179683\n",
      " 1.10179683 0.65001757 0.65001757 1.13297371 1.10179683 1.13297371\n",
      " 1.10179683 1.13297371 1.13297371 1.13297371 1.13297371 0.61884068\n",
      " 1.13297371 1.13297371 1.10179683 1.10179683 1.13297371 0.61884068\n",
      " 1.10179683 1.13297371 1.10179683 0.61884068 1.10179683 1.13297371\n",
      " 1.13297371 1.13297371 0.65001757 0.65001757 1.10179683 1.10179683\n",
      " 0.61884068 1.13297371 1.13297371 1.10179683 1.13297371 1.10179683\n",
      " 1.13297371 1.13297371 1.13297371 1.13297371 0.61884068 1.13297371\n",
      " 1.13297371 0.61884068 1.13297371 1.13297371 1.13297371 0.65001757\n",
      " 1.13297371 1.10179683 1.13297371 0.61884068 1.10179683 1.13297371\n",
      " 1.13297371 1.13297371 1.13297371 1.10179683 1.10179683 1.13297371\n",
      " 1.13297371 0.65001757 1.13297371 1.13297371 1.10179683 1.13297371\n",
      " 1.13297371 1.13297371 1.13297371 1.13297371 1.13297371 1.13297371\n",
      " 1.13297371 1.13297371 1.10179683 1.10179683 1.13297371 1.10179683\n",
      " 0.65001757 1.10179683 1.10179683 1.10179683 1.10179683 1.10179683\n",
      " 1.10179683 1.13297371 1.13297371 1.13297371 1.10179683 1.13297371\n",
      " 1.13297371 0.65001757 1.13297371 1.13297371]\n",
      "all_sum is: [2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451 2.58717451\n",
      " 2.58717451 2.58717451 2.58717451 2.58717451]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.342\n",
      "(*) epoch 2, cost 3.273\n",
      "(*) epoch 3, cost 3.032\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.176\n",
      "(*) epoch 2, cost 1.044\n",
      "(*) epoch 3, cost 0.794\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.19533961 1.19533961 1.19533961 1.6935692  1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.6935692\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.98304953 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961 1.19533961\n",
      " 1.19533961 1.19533961 1.19533961 1.19533961]\n",
      "all_sum is: [5.46444951 4.85765258 4.85765258 4.85765258 5.46444951 4.85765258\n",
      " 5.46444951 4.85765258 4.85765258 5.54337476 4.85765258 4.85765258\n",
      " 5.54337476 4.93657783 5.46444951 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 4.85765258 5.46444951 4.85765258 4.85765258\n",
      " 4.85765258 5.46444951 4.85765258 4.85765258 4.85765258 5.46444951\n",
      " 4.85765258 4.85765258 4.85765258 5.46444951 4.85765258 5.46444951\n",
      " 4.85765258 4.85765258 4.85765258 5.46444951 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 5.46444951 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.93657783 5.46444951 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 5.46444951 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 5.46444951 4.85765258 4.85765258 4.93657783\n",
      " 4.93657783 4.85765258 4.85765258 5.46444951 4.85765258 4.85765258\n",
      " 4.85765258 5.46444951 5.46444951 4.85765258 4.85765258 4.85765258\n",
      " 4.85765258 4.85765258 4.85765258 4.85765258]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.686\n",
      "(*) epoch 2, cost 0.716\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.674\n",
      "(*) epoch 2, cost 1.727\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 2.2815584 1.7733336\n",
      " 1.7733336 2.2815584 3.4624963 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 3.4624963 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 0.        1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 0.        1.7733336 1.7733336 1.7733336 1.7733336 3.4624963\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 3.4624963 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 2.2815584 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336 1.7733336\n",
      " 1.7733336 1.7733336]\n",
      "all_sum is: [1.00107976 1.00107976 1.00107976 0.93487613 0.93487613 1.00107976\n",
      " 1.00107976 0.93487613 1.00107976 0.93487613 1.00107976 1.00107976\n",
      " 0.93487613 1.00107976 1.00107976 0.93487613 1.00107976 1.00107976\n",
      " 1.00107976 0.93487613 1.00107976 1.00107976 1.00107976 1.00107976\n",
      " 1.00107976 1.00107976 1.00107976 1.00107976 1.00107976 0.93487613\n",
      " 0.93487613 1.00107976 1.00107976 1.00107976 1.00107976 0.93487613\n",
      " 1.00107976 1.00107976 0.93487613 1.00107976 1.00107976 0.93487613\n",
      " 1.00107976 0.93487613 0.93487613 0.93487613 1.00107976 1.00107976\n",
      " 0.93487613 1.00107976 1.00107976 0.93487613 1.00107976 0.93487613\n",
      " 1.00107976 0.93487613 1.00107976 0.93487613 1.57349637 1.00107976\n",
      " 1.00107976 1.00107976 0.93487613 1.00107976 1.00107976 0.93487613\n",
      " 1.00107976 1.00107976 1.00107976 1.00107976 1.00107976 1.00107976\n",
      " 1.00107976 1.00107976 0.93487613 1.00107976 1.00107976 1.00107976\n",
      " 1.00107976 0.93487613 1.00107976 1.00107976 1.00107976 1.00107976\n",
      " 0.93487613 0.93487613 1.57349637 0.93487613 0.93487613 0.93487613\n",
      " 1.00107976 0.93487613 0.93487613 0.93487613 1.00107976 0.93487613\n",
      " 0.93487613 1.00107976 0.93487613 0.93487613]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.656\n",
      "(*) epoch 2, cost 1.913\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.402\n",
      "(*) epoch 2, cost 1.519\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.23257402 1.23257402 1.23257402 1.23257402 1.23257402 1.23257402\n",
      " 1.03849004 1.23257402 1.23257402 0.15752251 0.         1.03849004\n",
      " 1.39009653 1.23257402 0.         0.         1.23257402 1.23257402\n",
      " 1.23257402 1.23257402 1.23257402 1.23257402 1.23257402 0.\n",
      " 0.         1.23257402 0.         1.23257402 1.46218089 0.\n",
      " 1.23257402 0.         1.03849004 0.         2.27106406 0.\n",
      " 0.         1.23257402 1.23257402 0.15752251 1.46218089 1.39009653\n",
      " 0.         1.23257402 0.         1.23257402 1.23257402 1.03849004\n",
      " 1.23257402 1.23257402 0.         2.27106406 1.23257402 0.\n",
      " 1.23257402 1.46218089 1.23257402 0.         1.23257402 0.\n",
      " 0.         1.23257402 1.23257402 1.23257402 1.23257402 0.15752251\n",
      " 0.22960687 1.23257402 1.23257402 1.03849004 1.23257402 1.23257402\n",
      " 0.         1.23257402 1.23257402 0.         1.23257402 0.\n",
      " 1.23257402 1.23257402 1.23257402 1.23257402 1.23257402 0.\n",
      " 1.23257402 0.         0.         1.23257402 1.23257402 1.23257402\n",
      " 1.23257402 1.23257402 1.23257402 0.         0.         1.23257402\n",
      " 0.         1.23257402 1.23257402 0.22960687]\n",
      "all_sum is: [3.70671126 3.6135704  3.70671126 3.6135704  3.70671126 3.6135704\n",
      " 3.70671126 3.70671126 3.70671126 3.70671126 3.70671126 3.70671126\n",
      " 3.70671126 3.70671126 3.70671126 3.70671126 3.70671126 3.70671126\n",
      " 3.6135704  4.11310916 3.70671126 4.20625002 4.20625002 3.70671126\n",
      " 3.70671126 3.6135704  3.70671126 3.6135704  3.70671126 3.70671126\n",
      " 3.70671126 3.70671126 3.6135704  3.6135704  3.70671126 3.70671126\n",
      " 3.70671126 3.70671126 3.70671126 3.6135704  3.6135704  3.70671126\n",
      " 3.70671126 3.6135704  3.70671126 3.70671126 3.6135704  3.70671126\n",
      " 3.70671126 4.20625002 3.70671126 3.6135704  3.70671126 3.70671126\n",
      " 3.70671126 3.6135704  3.6135704  3.70671126 3.70671126 3.6135704\n",
      " 3.6135704  3.70671126 3.6135704  3.70671126 4.11310916 3.70671126\n",
      " 3.6135704  3.6135704  4.20625002 3.70671126 3.70671126 3.6135704\n",
      " 3.70671126 3.6135704  3.70671126 3.70671126 3.70671126 3.70671126\n",
      " 3.70671126 3.70671126 4.11310916 3.70671126 3.70671126 3.6135704\n",
      " 3.70671126 4.20625002 3.6135704  3.6135704  4.20625002 3.6135704\n",
      " 3.6135704  3.70671126 3.70671126 3.70671126 3.70671126 3.70671126\n",
      " 3.70671126 3.70671126 3.70671126 3.6135704 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.509\n",
      "(*) epoch 2, cost 1.226\n",
      "(*) epoch 3, cost 1.107\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.702\n",
      "(*) epoch 2, cost 3.346\n",
      "(*) epoch 3, cost 2.869\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.46419547 2.11039295 2.11039295 3.10923828 3.70153716 3.10923828\n",
      " 3.10923828 3.10923828 3.10923828 2.70269183 3.10923828 4.05533969\n",
      " 2.46419547 3.10923828 3.70153716 2.11039295 3.10923828 3.70153716\n",
      " 3.10923828 3.10923828 3.10923828 2.11039295 2.11039295 3.10923828\n",
      " 2.70269183 2.11039295 3.10923828 2.11039295 2.11039295 3.10923828\n",
      " 3.10923828 3.10923828 2.11039295 3.10923828 3.10923828 3.10923828\n",
      " 3.10923828 2.11039295 2.11039295 2.11039295 3.10923828 3.10923828\n",
      " 3.10923828 2.11039295 2.11039295 3.10923828 3.10923828 2.11039295\n",
      " 2.11039295 2.11039295 3.10923828 3.10923828 3.10923828 2.11039295\n",
      " 2.70269183 2.11039295 3.10923828 3.10923828 3.10923828 3.10923828\n",
      " 3.10923828 3.10923828 2.70269183 3.10923828 3.10923828 3.10923828\n",
      " 3.10923828 2.11039295 3.10923828 2.11039295 3.70153716 3.70153716\n",
      " 2.11039295 3.10923828 3.10923828 3.10923828 3.10923828 3.4630408\n",
      " 3.10923828 3.10923828 2.11039295 3.10923828 2.11039295 2.11039295\n",
      " 2.11039295 3.4630408  2.11039295 3.10923828 2.11039295 2.11039295\n",
      " 2.11039295 2.11039295 2.11039295 2.11039295 3.10923828 2.11039295\n",
      " 2.11039295 3.10923828 3.10923828 2.11039295]\n",
      "all_sum is: [1.16228623 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.9825556  0.9825556  1.16228623 1.16228623 0.9825556  1.16228623\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.         0.9825556  0.9825556  0.9825556  0.9825556  1.16228623\n",
      " 1.16228623 1.16228623 0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  1.16228623\n",
      " 0.9825556  1.16228623 0.9825556  0.9825556  1.16228623 0.9825556\n",
      " 0.9825556  1.16228623 0.9825556  0.9825556  0.9825556  1.16228623\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 0.         0.9825556  1.16228623 0.9825556  1.16228623 0.9825556\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556  0.9825556  0.9825556\n",
      " 1.16228623 1.16228623 1.16228623 0.9825556  0.9825556  1.16228623\n",
      " 0.9825556  1.16228623 0.9825556  0.9825556  0.9825556  1.16228623\n",
      " 0.9825556  1.16228623 0.9825556  0.9825556  1.16228623 0.9825556\n",
      " 0.9825556  0.9825556  0.9825556  0.9825556 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.733\n",
      "(*) epoch 2, cost 1.696\n",
      "(*) epoch 3, cost 1.486\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.501\n",
      "(*) epoch 2, cost 1.488\n",
      "(*) epoch 3, cost 1.290\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.12500285 1.91510287 1.0945116  0.12500285 1.0945116  1.91510287\n",
      " 0.12500285 1.91510287 1.0945116  0.94559412 0.94559412 1.0945116\n",
      " 0.94559412 0.12500285 0.12500285 0.94559412 0.12500285 1.0420457\n",
      " 0.94559412 0.94559412 0.94559412 0.22145443 0.94559412 0.12500285\n",
      " 0.94559412 1.91510287 0.12500285 0.12500285 0.94559412 1.0945116\n",
      " 1.19096318 1.0945116  0.12500285 1.91510287 0.94559412 0.94559412\n",
      " 0.12500285 0.94559412 0.94559412 0.94559412 0.94559412 0.12500285\n",
      " 0.12500285 0.94559412 0.12500285 1.0945116  0.94559412 0.94559412\n",
      " 0.12500285 1.0945116  0.12500285 0.12500285 1.0420457  0.12500285\n",
      " 0.94559412 2.01155444 0.12500285 2.01155444 0.94559412 0.12500285\n",
      " 1.91510287 0.12500285 1.91510287 0.12500285 0.94559412 0.12500285\n",
      " 0.12500285 0.94559412 0.12500285 0.94559412 0.94559412 1.91510287\n",
      " 0.94559412 0.12500285 1.0420457  0.94559412 1.0945116  1.0945116\n",
      " 0.94559412 0.94559412 0.94559412 0.94559412 1.19096318 1.91510287\n",
      " 1.0945116  0.22145443 1.91510287 0.12500285 1.91510287 1.0420457\n",
      " 0.12500285 0.12500285 1.0945116  0.12500285 1.0945116  1.0945116\n",
      " 1.91510287 2.01155444 1.0945116  1.91510287]\n",
      "all_sum is: [3.21975249 1.93498874 1.99632417 1.99632417 3.21975249 1.99632417\n",
      " 1.99632417 3.21975249 1.99632417 3.21975249 1.99632417 3.21975249\n",
      " 1.99632417 3.21975249 3.21975249 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 3.21975249 3.21975249 1.99632417\n",
      " 3.21975249 3.21975249 1.99632417 1.99632417 3.21975249 1.99632417\n",
      " 3.21975249 1.99632417 3.21975249 1.99632417 3.21975249 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 3.21975249 1.99632417 1.99632417\n",
      " 3.21975249 1.99632417 3.21975249 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 3.21975249 1.99632417 3.21975249 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 3.21975249 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 3.21975249 3.21975249 3.21975249\n",
      " 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417 1.99632417\n",
      " 1.99632417 1.99632417 3.21975249 3.21975249 1.99632417 3.21975249\n",
      " 3.21975249 3.21975249 1.99632417 1.99632417]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.019\n",
      "(*) epoch 2, cost 2.915\n",
      "(*) epoch 3, cost 2.621\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.324\n",
      "(*) epoch 2, cost 1.068\n",
      "(*) epoch 3, cost 0.943\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.1837474  1.1837474  0.85637712 1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  2.96819352 0.85637712 1.1837474  1.1837474  1.1837474\n",
      " 0.85637712 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474  0.85637712\n",
      " 1.1837474  0.85637712 1.1837474  2.64082324 2.96819352 1.1837474\n",
      " 1.1837474  1.1837474  0.85637712 1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474  0.85637712\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.2109267  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  0.85637712 1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  0.85637712 1.1837474  0.85637712\n",
      " 1.1837474  0.85637712 1.1837474  0.85637712 1.1837474  1.1837474\n",
      " 0.85637712 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474\n",
      " 1.1837474  1.1837474  1.1837474  0.85637712 1.1837474  0.85637712\n",
      " 1.1837474  1.1837474  1.1837474  1.1837474  1.1837474  1.1837474\n",
      " 0.85637712 1.1837474  0.85637712 1.1837474 ]\n",
      "all_sum is: [0.31690843 0.31690843 0.31690843 0.31690843 0.         0.\n",
      " 0.31690843 0.31690843 0.31690843 0.31690843 0.         0.31690843\n",
      " 0.31690843 0.31690843 0.         0.31690843 0.         0.31690843\n",
      " 0.         0.31690843 0.         0.         0.31690843 0.\n",
      " 0.31690843 0.31690843 0.         0.31690843 0.31690843 0.\n",
      " 0.         0.31690843 0.         0.31690843 0.         0.31690843\n",
      " 0.         0.31690843 0.31690843 0.31690843 0.31690843 0.31690843\n",
      " 0.31690843 0.         0.31690843 0.         0.         0.\n",
      " 0.31690843 0.31690843 0.         0.31690843 0.31690843 0.\n",
      " 0.31690843 0.31690843 0.31690843 0.31690843 0.31690843 0.31690843\n",
      " 0.         0.         0.         0.31690843 0.31690843 0.31690843\n",
      " 0.31690843 0.         0.         0.31690843 0.31690843 0.31690843\n",
      " 0.         0.         0.         0.         0.         0.31690843\n",
      " 0.         0.31690843 0.31690843 0.31690843 0.31690843 0.\n",
      " 0.         0.31690843 0.31690843 0.31690843 0.31690843 0.31690843\n",
      " 0.31690843 0.31690843 0.31690843 0.31690843 0.         0.31690843\n",
      " 0.31690843 0.31690843 0.31690843 0.31690843]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.496\n",
      "(*) epoch 2, cost 0.941\n",
      "(*) epoch 3, cost 0.652\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.884\n",
      "(*) epoch 2, cost 1.701\n",
      "(*) epoch 3, cost 1.456\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.05082309 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.74823914 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045 2.16235045\n",
      " 2.16235045 2.16235045 2.16235045 2.16235045]\n",
      "all_sum is: [1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 3.44127073 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 3.44127073 1.44805652 1.44805652 1.44805652 3.44127073 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 3.44127073\n",
      " 1.44805652 1.44805652 1.44805652 3.44127073 1.44805652 1.19872803\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.19872803 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 3.44127073 1.44805652 1.44805652 1.44805652 1.44805652 1.44805652\n",
      " 1.44805652 1.44805652 1.44805652 1.44805652]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.380\n",
      "(*) epoch 2, cost 0.780\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.809\n",
      "(*) epoch 2, cost 1.270\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.17871449 2.27541759 2.17871449 2.17871449 2.17871449 2.17871449\n",
      " 3.58153946 2.17871449 2.17871449 3.48483636 2.75548432 2.17871449\n",
      " 2.17871449 2.17871449 2.17871449 4.24543708 1.44936245 1.44936245\n",
      " 2.93931521 2.17871449 2.17871449 2.17871449 2.17871449 2.17871449\n",
      " 2.17871449 2.93931521 2.17871449 2.17871449 2.17871449 2.93931521\n",
      " 2.17871449 2.17871449 3.03601831 2.17871449 2.17871449 2.27541759\n",
      " 2.17871449 2.27541759 2.17871449 2.17871449 2.17871449 1.40331008\n",
      " 2.17871449 2.17871449 2.27541759 2.17871449 2.27541759 4.24543708\n",
      " 2.27541759 2.17871449 2.17871449 2.17871449 2.27541759 2.75548432\n",
      " 2.17871449 3.03601831 2.93931521 3.03601831 2.17871449 2.17871449\n",
      " 3.48483636 2.27541759 2.17871449 2.27541759 2.17871449 2.17871449\n",
      " 3.48483636 2.17871449 3.03601831 2.17871449 2.17871449 2.17871449\n",
      " 3.48483636 2.17871449 2.27541759 2.27541759 2.17871449 2.17871449\n",
      " 2.17871449 2.17871449 1.54606555 2.17871449 3.48483636 1.44936245\n",
      " 2.17871449 2.17871449 2.17871449 1.44936245 2.17871449 2.17871449\n",
      " 1.44936245 2.17871449 2.17871449 2.17871449 3.51608504 2.17871449\n",
      " 2.17871449 1.44936245 2.17871449 2.17871449]\n",
      "all_sum is: [1.43011146 0.91383811 1.43011146 1.43011146 1.43011146 1.43011146\n",
      " 1.43011146 0.91383811 1.43011146 0.91383811 0.91383811 1.43011146\n",
      " 1.43011146 0.91383811 0.91383811 1.49024612 1.43011146 1.43011146\n",
      " 1.43011146 0.91383811 2.00651947 1.43011146 0.91383811 0.91383811\n",
      " 1.43011146 0.91383811 1.43011146 0.91383811 0.91383811 1.43011146\n",
      " 1.43011146 1.43011146 1.43011146 0.91383811 1.49024612 2.00651947\n",
      " 1.49024612 0.91383811 0.91383811 2.00651947 1.43011146 1.49024612\n",
      " 2.00651947 1.43011146 1.43011146 0.91383811 1.43011146 0.57640801\n",
      " 0.91383811 1.43011146 1.43011146 0.91383811 0.91383811 2.00651947\n",
      " 1.43011146 2.00651947 1.43011146 2.00651947 2.00651947 1.49024612\n",
      " 1.43011146 1.43011146 1.43011146 1.49024612 1.43011146 0.91383811\n",
      " 0.91383811 0.91383811 1.43011146 2.00651947 0.91383811 0.91383811\n",
      " 0.91383811 1.43011146 1.49024612 1.49024612 1.43011146 1.43011146\n",
      " 0.91383811 0.91383811 2.00651947 0.91383811 1.43011146 1.43011146\n",
      " 0.91383811 1.43011146 0.91383811 1.43011146 1.49024612 1.43011146\n",
      " 1.43011146 1.43011146 0.91383811 0.91383811 0.91383811 1.43011146\n",
      " 1.43011146 0.91383811 1.43011146 1.43011146]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.801\n",
      "(*) epoch 2, cost 3.676\n",
      "(*) epoch 3, cost 3.465\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.359\n",
      "(*) epoch 2, cost 1.758\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.05280478 2.49726091 2.49726091 3.28615272 2.49726091 2.49726091\n",
      " 2.49726091 2.49726091 2.17251432 3.28615272 4.05280478 3.28615272\n",
      " 3.28615272 2.49726091 3.28615272 2.49726091 2.49726091 2.49726091\n",
      " 2.49726091 2.49726091 2.49726091 3.28615272 3.28615272 2.49726091\n",
      " 4.05280478 3.28615272 4.51695001 2.49726091 4.8416966  2.49726091\n",
      " 3.28615272 2.49726091 2.17251432 4.05280478 2.49726091 4.05280478\n",
      " 2.17251432 3.28615272 3.28615272 2.49726091 2.49726091 3.28615272\n",
      " 4.8416966  2.49726091 4.51695001 3.28615272 2.49726091 2.96140614\n",
      " 2.49726091 4.8416966  2.49726091 5.21474711 2.49726091 2.49726091\n",
      " 2.49726091 3.28615272 2.49726091 2.49726091 2.49726091 2.49726091\n",
      " 2.49726091 3.28615272 3.28615272 3.28615272 3.28615272 2.96140614\n",
      " 4.4258553  4.89000053 2.49726091 2.49726091 2.49726091 2.49726091\n",
      " 4.05280478 2.49726091 2.49726091 2.49726091 3.28615272 4.05280478\n",
      " 2.49726091 2.49726091 2.49726091 2.96140614 2.49726091 4.05280478\n",
      " 2.49726091 3.28615272 3.28615272 2.49726091 2.49726091 2.49726091\n",
      " 2.49726091 2.49726091 3.28615272 2.49726091 2.49726091 2.49726091\n",
      " 3.28615272 2.49726091 3.28615272 2.49726091]\n",
      "all_sum is: [2.61613797 3.13552562 3.19577303 3.13552562 3.19577303 3.13552562\n",
      " 2.55589056 3.19577303 3.13552562 3.13552562 3.13552562 2.55589056\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 2.55589056 3.19577303 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.82598785 3.19577303 3.13552562 3.13552562 3.13552562\n",
      " 2.55589056 3.13552562 3.13552562 3.13552562 2.55589056 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 3.19577303 3.13552562\n",
      " 3.13552562 3.13552562 2.55589056 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.19577303 3.13552562 3.13552562 2.55589056\n",
      " 3.13552562 2.55589056 3.13552562 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.19577303 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 2.55589056 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 3.19577303 3.13552562\n",
      " 3.19577303 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562 3.13552562\n",
      " 3.13552562 3.13552562 3.13552562 3.13552562]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.384\n",
      "(*) epoch 2, cost 3.616\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.220\n",
      "(*) epoch 2, cost 2.233\n",
      "(*) epoch 3, cost 1.790\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.71798154 2.71798154 2.74539209 2.71798154 2.71798154 0.60417552\n",
      " 2.71798154 2.71798154 0.60417552 0.60417552 2.71798154 2.71798154\n",
      " 0.60417552 0.60417552 2.71798154 0.63158607 2.67493836 0.63158607\n",
      " 2.71798154 2.71798154 0.60417552 2.71798154 0.63158607 2.71798154\n",
      " 0.63158607 2.71798154 2.74539209 2.74539209 2.71798154 2.71798154\n",
      " 2.71798154 2.71798154 2.71798154 2.71798154 2.74539209 0.60417552\n",
      " 2.71798154 2.71798154 2.71798154 2.71798154 0.60417552 2.71798154\n",
      " 2.71798154 2.71798154 2.71798154 2.71798154 3.25170333 2.71798154\n",
      " 2.71798154 1.1378973  2.71798154 2.71798154 2.71798154 2.71798154\n",
      " 2.71798154 0.60417552 0.60417552 2.11380602 0.60417552 2.71798154\n",
      " 2.71798154 1.1378973  2.71798154 2.74539209 2.71798154 0.60417552\n",
      " 2.71798154 2.11380602 0.63158607 2.74539209 0.60417552 1.1378973\n",
      " 2.74539209 2.71798154 2.71798154 2.14121657 3.25170333 0.60417552\n",
      " 2.71798154 2.71798154 2.74539209 2.71798154 2.71798154 2.71798154\n",
      " 2.71798154 2.71798154 0.         2.71798154 2.71798154 0.53372179\n",
      " 0.60417552 0.60417552 2.74539209 0.60417552 2.71798154 0.63158607\n",
      " 3.25170333 2.71798154 2.71798154 2.71798154]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.94868266 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.259\n",
      "(*) epoch 2, cost 1.397\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.460\n",
      "(*) epoch 2, cost 2.309\n",
      "(*) epoch 3, cost 1.952\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.4382134  0.         0.4382134\n",
      " 0.         0.31991466 0.4382134  0.         0.         0.\n",
      " 0.         0.4382134  0.         0.4382134  0.4382134  0.\n",
      " 0.         0.         0.         0.95368297 0.4382134  0.\n",
      " 0.         0.         0.4382134  0.95368297 0.4382134  0.\n",
      " 0.95368297 0.         0.4382134  0.         0.4382134  0.4382134\n",
      " 0.4382134  0.4382134  0.2854386  0.4382134  0.4382134  0.\n",
      " 0.4382134  0.4382134  0.         0.         0.95368297 1.39189638\n",
      " 0.4382134  0.         1.39189638 0.         1.39189638 0.4382134\n",
      " 0.95368297 0.         0.         0.         0.         0.\n",
      " 0.4382134  0.4382134  0.4382134  0.31991466 0.         0.95368297\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4382134  0.         0.4382134  0.         0.4382134  1.39189638\n",
      " 0.         0.         0.         0.4382134  0.95368297 0.\n",
      " 0.         0.         0.4382134  0.95368297 0.         0.\n",
      " 0.4382134  0.         0.4382134  0.4382134  0.         0.4382134\n",
      " 0.         0.4382134  0.95368297 0.4382134 ]\n",
      "all_sum is: [1.51588472 1.51588472 1.51588472 1.51588472 1.51588472 1.51588472\n",
      " 1.32561546 1.51588472 2.5419428  1.51588472 1.51588472 1.51588472\n",
      " 1.51588472 2.21603808 3.0518269  1.51588472 1.51588472 1.51588472\n",
      " 1.51588472 2.02576882 1.51588472 2.21603808 1.51588472 2.35167354\n",
      " 1.51588472 1.51588472 2.21603808 2.5419428  1.51588472 1.51588472\n",
      " 1.51588472 1.51588472 1.51588472 1.51588472 1.51588472 1.51588472\n",
      " 1.51588472 1.51588472 2.35167354 1.32561546 1.51588472 1.51588472\n",
      " 1.51588472 1.51588472 1.51588472 1.51588472 1.51588472 2.5419428\n",
      " 1.32561546 1.51588472 1.51588472 1.32561546 1.51588472 2.5419428\n",
      " 1.51588472 1.51588472 1.51588472 2.5419428  1.51588472 1.32561546\n",
      " 1.51588472 2.5419428  3.24209616 2.5419428  1.51588472 1.32561546\n",
      " 1.51588472 1.51588472 2.5419428  2.5419428  1.51588472 1.51588472\n",
      " 2.5419428  1.51588472 2.35167354 1.51588472 1.32561546 2.21603808\n",
      " 1.51588472 1.51588472 1.51588472 3.24209616 1.51588472 1.51588472\n",
      " 1.51588472 1.51588472 2.5419428  1.51588472 2.21603808 2.5419428\n",
      " 1.32561546 2.02576882 1.51588472 1.51588472 2.5419428  1.51588472\n",
      " 1.51588472 1.51588472 1.51588472 1.51588472]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.358\n",
      "(*) epoch 2, cost 2.011\n",
      "(*) epoch 3, cost 1.699\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.930\n",
      "(*) epoch 2, cost 2.017\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.86277291 1.86277291 3.62232793 1.60121064 1.86277291 0.57774276\n",
      " 4.7658918  0.26156227 2.02111729 1.60121064 1.60121064 2.1789534\n",
      " 1.86277291 3.8838902  3.8838902  1.60121064 1.86277291 1.86277291\n",
      " 1.86277291 3.8838902  1.91739113 0.         0.         3.8838902\n",
      " 1.86277291 3.8838902  3.62232793 1.60121064 1.60121064 1.60121064\n",
      " 3.62232793 1.91739113 3.8838902  4.20007068 1.86277291 1.86277291\n",
      " 3.62232793 1.60121064 1.86277291 1.60121064 3.62232793 1.86277291\n",
      " 3.62232793 1.60121064 1.86277291 1.60121064 1.60121064 1.60121064\n",
      " 1.86277291 3.8838902  1.60121064 3.8838902  3.8838902  3.8838902\n",
      " 3.62232793 1.60121064 1.86277291 3.62232793 3.8838902  3.8838902\n",
      " 1.60121064 1.86277291 2.02111729 3.8838902  3.8838902  1.60121064\n",
      " 0.         1.86277291 1.60121064 3.62232793 3.8838902  4.20007068\n",
      " 1.86277291 3.8838902  2.74477451 3.8838902  1.91739113 3.8838902\n",
      " 1.86277291 1.60121064 2.258259   3.8838902  1.60121064 0.\n",
      " 1.60121064 3.93850841 1.60121064 3.8838902  1.60121064 1.86277291\n",
      " 3.62232793 3.8838902  1.86277291 1.86277291 3.62232793 1.60121064\n",
      " 3.93850841 0.26156227 3.8838902  3.8838902 ]\n",
      "all_sum is: [4.98819232 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232\n",
      " 4.98819232 4.98819232 4.98819232 4.98819232 2.06884256 3.6069148\n",
      " 4.98819232 3.6069148  4.98819232 4.98819232 3.6069148  4.98819232\n",
      " 3.45012008 3.45012008 4.98819232 4.98819232 2.06884256 2.59453101\n",
      " 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232 5.97081821\n",
      " 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232\n",
      " 4.98819232 4.98819232 4.98819232 6.6345926  4.98819232 4.98819232\n",
      " 3.45012008 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232\n",
      " 3.45012008 4.98819232 3.45012008 4.98819232 5.97081821 4.98819232\n",
      " 4.98819232 4.98819232 3.6069148  4.13260325 5.97081821 4.98819232\n",
      " 4.98819232 3.6069148  4.98819232 4.98819232 4.98819232 4.98819232\n",
      " 4.13260325 4.98819232 4.98819232 4.98819232 4.98819232 4.98819232\n",
      " 4.98819232 3.73395162 4.98819232 3.6069148  4.98819232 3.45012008\n",
      " 3.6069148  4.98819232 3.6069148  4.98819232 4.98819232 4.98819232\n",
      " 4.13260325 4.58954069 3.6069148  3.6069148  4.98819232 3.6069148\n",
      " 4.98819232 4.98819232 4.98819232 3.6069148  4.98819232 2.06884256\n",
      " 3.6069148  4.98819232 4.13260325 2.06884256]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.926\n",
      "(*) epoch 2, cost 3.318\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 10\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.276\n",
      "(*) epoch 2, cost 5.396\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 3.74166373 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738 2.98073738\n",
      " 2.98073738 2.98073738 2.98073738 2.98073738]\n",
      "all_sum is: [1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.54185222 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.54185222 1.05992701 1.05992701 1.05992701\n",
      " 1.05992701 1.05992701 1.05992701 1.05992701]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.763\n",
      "(*) epoch 2, cost 2.367\n",
      "(*) epoch 3, cost 2.067\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.443\n",
      "(*) epoch 2, cost 1.913\n",
      "(*) epoch 3, cost 1.510\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.42481171 1.42481171 0.         1.42481171 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171 2.30815057\n",
      " 1.42481171 2.30815057 1.42481171 1.42481171 1.42481171 0.\n",
      " 2.30815057 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 2.30815057 1.42481171 1.42481171\n",
      " 2.30815057 2.30815057 1.42481171 2.30815057 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 0.         2.30815057 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171 2.30815057 1.42481171\n",
      " 1.42481171 2.30815057 1.42481171 2.30815057 1.42481171 2.30815057\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 2.30815057 0.88333886 1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 0.         1.42481171 1.42481171\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171 1.42481171\n",
      " 1.42481171 0.         1.42481171 2.30815057 1.42481171 1.42481171\n",
      " 1.42481171 0.         1.42481171 1.42481171 1.42481171 2.30815057\n",
      " 1.42481171 1.42481171 1.42481171 1.42481171]\n",
      "all_sum is: [0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.         0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.         0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.         0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556 0.95020556\n",
      " 0.95020556 0.95020556 0.         0.95020556]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.103\n",
      "(*) epoch 2, cost 0.668\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.502\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.54344618 0.54344618 1.54114787 0.97728833 0.54344618 0.97728833\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.97728833 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.97728833 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618 0.54344618\n",
      " 0.54344618 0.54344618 0.54344618 0.54344618]\n",
      "all_sum is: [1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.40482266\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.40482266\n",
      " 1.19924491 1.19924491 1.40482266 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.40482266 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.40482266 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.40482266 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.40482266 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.40482266\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.40482266 1.19924491\n",
      " 1.19924491 1.19924491 1.40482266 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491 1.19924491\n",
      " 1.19924491 1.19924491 1.19924491 1.19924491]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.333\n",
      "(*) epoch 2, cost 0.856\n",
      "(*) epoch 3, cost 0.406\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.029\n",
      "(*) epoch 2, cost 1.700\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314 1.69010314\n",
      " 1.69010314 1.69010314 1.69010314 1.69010314]\n",
      "all_sum is: [0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 1.44747651 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705 0.76354705\n",
      " 0.76354705 0.76354705 0.76354705 0.76354705]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.910\n",
      "(*) epoch 2, cost 2.256\n",
      "(*) epoch 3, cost 1.988\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.160\n",
      "(*) epoch 2, cost 1.961\n",
      "(*) epoch 3, cost 1.454\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.06438715 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.586814\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.586814   0.         0.\n",
      " 0.         0.06438715 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.586814   0.         0.\n",
      " 0.         0.586814   0.         0.         0.         0.\n",
      " 0.         0.586814   0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.06438715 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.06438715\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.60909645 1.60909645 1.60909645 1.60909645 0.         0.\n",
      " 1.60909645 1.60909645 0.21923326 1.82832971 1.60909645 1.60909645\n",
      " 0.         0.         1.60909645 0.         1.60909645 1.82832971\n",
      " 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645\n",
      " 0.         1.60909645 1.60909645 1.60909645 1.60909645 1.60909645\n",
      " 0.         1.60909645 1.60909645 0.         0.         0.\n",
      " 1.60909645 1.60909645 1.60909645 0.         0.         1.60909645\n",
      " 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645\n",
      " 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645 0.\n",
      " 1.60909645 0.         1.60909645 1.60909645 0.         1.60909645\n",
      " 0.         0.         1.60909645 1.60909645 1.82832971 0.\n",
      " 1.60909645 1.60909645 0.         0.         1.60909645 1.60909645\n",
      " 0.         1.60909645 0.         1.60909645 1.60909645 1.60909645\n",
      " 1.60909645 1.60909645 0.         1.60909645 1.60909645 1.60909645\n",
      " 1.60909645 1.60909645 0.         1.60909645 1.60909645 1.60909645\n",
      " 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645 1.60909645\n",
      " 0.         0.         0.         1.60909645]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.844\n",
      "(*) epoch 2, cost 1.202\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.938\n",
      "(*) epoch 2, cost 2.023\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.2794611  0.2794611  0.2794611  0.2794611  0.2794611  0.9254252\n",
      " 0.9254252  0.2794611  0.9254252  0.9254252  0.9254252  0.2794611\n",
      " 0.2794611  0.2794611  0.2794611  1.02799177 2.75067284 0.2794611\n",
      " 0.2794611  0.9254252  0.9254252  3.39663695 3.39663695 0.2794611\n",
      " 0.2794611  0.2794611  0.2794611  0.2794611  0.2794611  0.9254252\n",
      " 0.2794611  0.2794611  0.2794611  0.2794611  0.2794611  0.9254252\n",
      " 0.2794611  0.2794611  0.2794611  0.9254252  0.9254252  0.9254252\n",
      " 0.2794611  0.9254252  0.9254252  0.2794611  0.2794611  0.9254252\n",
      " 0.2794611  0.9254252  0.9254252  0.9254252  0.2794611  0.9254252\n",
      " 0.9254252  0.9254252  2.75067284 0.9254252  0.2794611  0.2794611\n",
      " 0.9254252  0.2794611  0.2794611  0.2794611  0.2794611  0.2794611\n",
      " 2.75067284 0.9254252  1.67395587 0.2794611  0.9254252  2.75067284\n",
      " 0.9254252  0.2794611  0.9254252  0.2794611  0.2794611  0.2794611\n",
      " 0.9254252  0.2794611  0.2794611  3.39663695 0.2794611  0.9254252\n",
      " 0.9254252  0.2794611  0.2794611  0.2794611  0.2794611  0.2794611\n",
      " 0.2794611  0.2794611  0.9254252  0.9254252  0.2794611  0.2794611\n",
      " 0.2794611  0.2794611  1.67395587 0.2794611 ]\n",
      "all_sum is: [2.54030209 2.54030209 2.54030209 3.98171964 2.54030209 3.98171964\n",
      " 2.54030209 3.98171964 3.98171964 2.54030209 2.54030209 3.98171964\n",
      " 2.54030209 2.54030209 3.98171964 2.54030209 2.54030209 2.54030209\n",
      " 3.98171964 2.54030209 3.98171964 2.54030209 2.54030209 2.54030209\n",
      " 3.98171964 3.98171964 2.54030209 2.54030209 2.54030209 2.54030209\n",
      " 3.98171964 2.54030209 2.54030209 3.98171964 2.54030209 3.98171964\n",
      " 2.54030209 2.54030209 2.54030209 3.98171964 3.98171964 3.98171964\n",
      " 2.54030209 2.54030209 3.98171964 3.98171964 2.54030209 2.54030209\n",
      " 2.54030209 3.98171964 2.54030209 3.98171964 2.54030209 2.54030209\n",
      " 3.98171964 2.54030209 2.54030209 2.54030209 2.54030209 2.54030209\n",
      " 2.54030209 3.98171964 3.98171964 2.54030209 2.54030209 2.54030209\n",
      " 2.54030209 3.98171964 2.54030209 3.98171964 2.54030209 3.57258445\n",
      " 3.98171964 2.54030209 3.98171964 2.54030209 3.98171964 3.98171964\n",
      " 3.98171964 3.98171964 2.54030209 3.98171964 2.54030209 3.98171964\n",
      " 2.13116691 3.98171964 2.54030209 2.54030209 3.98171964 3.98171964\n",
      " 2.54030209 2.54030209 2.54030209 2.54030209 2.54030209 3.98171964\n",
      " 2.54030209 3.98171964 2.54030209 2.54030209]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.629\n",
      "(*) epoch 2, cost 1.003\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.394\n",
      "(*) epoch 2, cost 2.623\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.00297153 0.00297153 0.66024194 0.66024194 0.         0.66024194\n",
      " 0.         0.         0.         0.         0.         0.00297153\n",
      " 0.         0.         0.         0.         0.         0.66024194\n",
      " 0.         0.         0.66024194 0.         0.         0.\n",
      " 0.         0.00297153 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.66024194 0.         0.66024194\n",
      " 0.         0.00297153 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00297153 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.66024194 0.         0.         0.         0.         0.\n",
      " 0.66024194 0.         0.         0.         0.66024194 0.66024194\n",
      " 0.         0.         0.         0.66024194 0.         0.\n",
      " 0.66024194 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    2.218642   1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    0.63287439 1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    0.63287439 1.24256\n",
      " 2.01542052 1.24256    0.63287439 1.24256    1.24256    1.24256\n",
      " 1.24256    2.99150252 1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    0.63287439 1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    2.218642\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256    1.24256    1.24256\n",
      " 1.24256    1.24256    1.24256    1.24256   ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.528\n",
      "(*) epoch 2, cost 0.810\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.155\n",
      "(*) epoch 2, cost 1.221\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 1.00741512 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 1.00741512\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503 0.81083503\n",
      " 0.81083503 0.81083503 0.81083503 0.81083503]\n",
      "all_sum is: [3.49812899 5.3840643  3.49812899 4.77597767 5.3840643  4.27900675\n",
      " 3.49812899 5.3840643  5.3840643  3.49812899 5.3840643  3.49812899\n",
      " 3.99509991 5.3840643  5.88103523 5.3840643  5.3840643  5.88103523\n",
      " 3.49812899 3.49812899 3.49812899 5.3840643  2.39307144 3.49812899\n",
      " 3.49812899 5.3840643  3.49812899 3.99509991 5.88103523 3.99509991\n",
      " 5.3840643  5.3840643  3.99509991 5.3840643  5.3840643  5.88103523\n",
      " 3.99509991 5.3840643  5.3840643  2.39307144 5.88103523 5.88103523\n",
      " 4.77597767 3.49812899 5.3840643  3.49812899 3.99509991 5.3840643\n",
      " 2.39307144 5.3840643  5.88103523 5.88103523 3.49812899 3.49812899\n",
      " 4.77597767 5.88103523 2.39307144 3.49812899 2.39307144 3.49812899\n",
      " 2.89004236 5.88103523 3.49812899 2.44254497 2.39307144 3.99509991\n",
      " 3.49812899 4.27900675 3.49812899 3.49812899 5.3840643  5.88103523\n",
      " 5.88103523 5.3840643  5.3840643  3.49812899 5.3840643  5.3840643\n",
      " 2.39307144 3.49812899 3.49812899 3.49812899 5.3840643  5.3840643\n",
      " 3.49812899 5.3840643  3.49812899 3.99509991 3.49812899 5.3840643\n",
      " 3.49812899 5.3840643  3.49812899 5.88103523 4.77597767 5.88103523\n",
      " 2.39307144 5.88103523 3.99509991 3.49812899]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 0.544\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.498\n",
      "(*) epoch 2, cost 4.503\n",
      "(*) epoch 3, cost 4.087\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.15923871 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  1.15994258 0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  1.15994258 0.9985124  0.9985124  0.9985124  1.15994258\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124  0.9985124  0.9985124\n",
      " 0.9985124  0.9985124  0.9985124  0.9985124 ]\n",
      "all_sum is: [1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 0.94776219\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 0.94776219 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.57386347\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.57386347 1.35754844 1.35754844 1.57386347 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844 1.57386347 1.35754844\n",
      " 1.35754844 1.35754844 1.35754844 1.35754844]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.115\n",
      "(*) epoch 2, cost 1.677\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.967\n",
      "(*) epoch 2, cost 2.852\n",
      "(*) epoch 3, cost 2.419\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.85076843 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.85076843 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.85076843 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.85076843 2.85076843 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.85076843 2.71634661 2.71634661 2.71634661 2.85076843\n",
      " 2.71634661 2.71634661 2.85076843 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661\n",
      " 2.71634661 2.71634661 2.71634661 2.71634661 2.71634661 2.85076843\n",
      " 2.71634661 2.71634661 2.71634661 2.85076843]\n",
      "all_sum is: [0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 1.44955439 0.85767575 2.61540895 1.44955439 0.85767575 2.61540895\n",
      " 0.85767575 0.85767575 0.85767575 1.44955439 0.85767575 0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 1.44955439 0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 1.44955439 1.44955439 0.85767575 0.85767575 0.         0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 0.85767575 0.85767575 1.44955439 0.85767575 1.44955439 1.44955439\n",
      " 1.44955439 0.85767575 0.85767575 1.44955439 0.85767575 1.44955439\n",
      " 1.44955439 0.85767575 1.44955439 0.85767575 2.61540895 0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 0.85767575 2.61540895 0.85767575 0.85767575 0.85767575 2.61540895\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 0.85767575 2.61540895 0.85767575 0.85767575 1.44955439 0.85767575\n",
      " 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575 0.85767575\n",
      " 3.20728758 0.85767575 0.85767575 0.85767575]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.133\n",
      "(*) epoch 2, cost 2.609\n",
      "(*) epoch 3, cost 2.383\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.838\n",
      "(*) epoch 2, cost 1.479\n",
      "(*) epoch 3, cost 1.264\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 2.03999596 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975 0.80553975\n",
      " 0.80553975 0.80553975 0.80553975 0.80553975]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.56971722 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.542\n",
      "(*) epoch 2, cost 0.541\n",
      "(*) epoch 3, cost 0.458\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.957\n",
      "(*) epoch 2, cost 2.121\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.7987592  0.25781031 0.54094889 0.25781031 0.7987592  0.25781031\n",
      " 0.         0.25781031 0.         0.7987592  0.7987592  0.25781031\n",
      " 1.1911221  0.25781031 0.54094889 0.         0.54094889 0.7987592\n",
      " 0.7987592  0.7987592  0.         0.54094889 0.25781031 0.\n",
      " 0.7987592  0.7987592  0.7987592  0.7987592  0.25781031 0.54094889\n",
      " 0.         0.90798353 0.7987592  0.7987592  0.7987592  0.25781031\n",
      " 0.7987592  0.54094889 0.         0.7987592  0.25781031 0.\n",
      " 0.54094889 0.7987592  0.54094889 0.25781031 0.54094889 0.7987592\n",
      " 0.54094889 0.54094889 0.54094889 0.54094889 0.7987592  0.7987592\n",
      " 0.25781031 0.54094889 0.25781031 0.         0.54094889 0.\n",
      " 0.54094889 0.54094889 0.25781031 0.54094889 0.         0.7987592\n",
      " 0.25781031 0.54094889 0.         0.25781031 0.54094889 0.25781031\n",
      " 0.7987592  0.         0.54094889 0.7987592  0.         0.\n",
      " 0.         0.         0.         0.7987592  0.         0.43497975\n",
      " 0.54094889 0.7987592  0.54094889 0.25781031 0.7987592  0.7987592\n",
      " 0.54094889 0.25781031 0.25781031 0.         0.25781031 0.25781031\n",
      " 0.54094889 0.25781031 0.54094889 0.25781031]\n",
      "all_sum is: [1.96191793 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.96191793 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.96191793 1.31370796 1.31370796 1.31370796\n",
      " 1.96191793 1.31370796 1.96191793 1.31370796 1.31370796 1.96191793\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.96191793 1.31370796 1.96191793 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.96191793 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.96191793\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.96191793 1.31370796 1.96191793 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.96191793 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.96191793 1.31370796 1.31370796 1.31370796 1.31370796 1.31370796\n",
      " 1.31370796 1.31370796 1.31370796 1.96191793 1.31370796 1.31370796\n",
      " 1.96191793 1.96191793 1.96191793 1.31370796]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.391\n",
      "(*) epoch 2, cost 3.038\n",
      "(*) epoch 3, cost 2.835\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.116\n",
      "(*) epoch 2, cost 1.581\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.28220043 3.28220043 3.97934128 3.28220043 3.28220043 3.28220043\n",
      " 3.97934128 3.28220043 3.28220043 3.97934128 3.28220043 3.28220043\n",
      " 1.80680566 3.97934128 3.97934128 3.97934128 3.28220043 3.28220043\n",
      " 1.80680566 3.28220043 3.97934128 3.28220043 3.28220043 3.28220043\n",
      " 3.28220043 3.97934128 3.97934128 3.97934128 3.97934128 3.28220043\n",
      " 3.97934128 3.28220043 3.28220043 3.97934128 3.97934128 3.28220043\n",
      " 3.28220043 2.50394651 3.28220043 3.97934128 3.28220043 3.97934128\n",
      " 3.28220043 3.97934128 1.80680566 3.28220043 3.28220043 3.97934128\n",
      " 3.28220043 3.28220043 3.28220043 3.97934128 3.97934128 3.28220043\n",
      " 3.28220043 3.28220043 3.28220043 3.28220043 1.80680566 3.28220043\n",
      " 3.28220043 3.97934128 3.28220043 3.97934128 2.50394651 1.80680566\n",
      " 3.97934128 3.28220043 3.28220043 3.28220043 3.28220043 1.80680566\n",
      " 3.97934128 3.97934128 3.28220043 3.97934128 3.97934128 2.50394651\n",
      " 3.97934128 3.28220043 3.97934128 3.28220043 1.80680566 3.28220043\n",
      " 1.80680566 3.28220043 3.97934128 3.97934128 3.97934128 3.97934128\n",
      " 3.28220043 1.80680566 3.28220043 3.97934128 2.50394651 3.28220043\n",
      " 3.28220043 1.80680566 3.97934128 3.28220043]\n",
      "all_sum is: [2.4010885  2.4010885  1.95662507 2.4010885  2.4010885  1.95662507\n",
      " 2.4010885  2.4010885  1.32904625 2.4010885  2.4010885  2.4010885\n",
      " 2.4010885  1.95662507 1.95662507 2.4010885  2.4010885  2.4010885\n",
      " 2.4010885  2.4010885  2.4010885  2.4010885  1.95662507 1.95662507\n",
      " 2.4010885  2.4010885  1.95662507 1.95662507 2.4010885  2.4010885\n",
      " 2.4010885  2.4010885  1.95662507 2.4010885  1.32904625 2.4010885\n",
      " 1.95662507 2.4010885  1.77350968 2.4010885  1.95662507 2.4010885\n",
      " 2.4010885  2.4010885  2.4010885  2.4010885  2.4010885  1.95662507\n",
      " 2.4010885  2.4010885  2.4010885  2.4010885  2.4010885  2.4010885\n",
      " 1.95662507 2.4010885  1.77350968 2.4010885  2.4010885  0.89250458\n",
      " 2.4010885  1.95662507 2.4010885  2.4010885  2.4010885  2.4010885\n",
      " 2.4010885  1.95662507 1.95662507 1.95662507 2.4010885  1.95662507\n",
      " 2.4010885  1.95662507 1.32904625 2.4010885  1.95662507 1.95662507\n",
      " 2.4010885  1.95662507 1.95662507 1.32904625 2.4010885  2.4010885\n",
      " 1.95662507 1.95662507 1.95662507 1.95662507 2.4010885  1.95662507\n",
      " 1.95662507 2.4010885  2.4010885  1.95662507 1.95662507 2.4010885\n",
      " 1.95662507 2.4010885  2.4010885  1.95662507]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.471\n",
      "(*) epoch 2, cost 1.718\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.690\n",
      "(*) epoch 2, cost 2.981\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.95233812 1.95233812 1.95233812 0.         0.         0.\n",
      " 1.95233812 0.         1.95233812 0.         0.         1.95233812\n",
      " 0.         1.95233812 1.95233812 0.         1.95233812 0.\n",
      " 0.         1.95233812 1.95233812 1.95233812 1.95233812 0.\n",
      " 0.         0.         0.         0.         1.95233812 0.\n",
      " 0.         0.         0.         1.95233812 0.         0.\n",
      " 0.         0.         0.         1.95233812 0.         1.95233812\n",
      " 0.         0.         1.95233812 1.95233812 1.95233812 0.\n",
      " 0.         0.         0.         0.         0.         1.95233812\n",
      " 1.95233812 0.         0.         0.         1.95233812 1.95233812\n",
      " 1.95233812 1.95233812 0.         1.95233812 0.         0.\n",
      " 0.         0.         1.95233812 0.         0.         0.\n",
      " 1.95233812 1.95233812 1.95233812 0.         1.95233812 1.95233812\n",
      " 1.95233812 1.95233812 1.95233812 1.95233812 1.95233812 1.95233812\n",
      " 1.95233812 0.         1.95233812 1.95233812 1.95233812 1.95233812\n",
      " 1.95233812 1.95233812 0.         0.         1.95233812 0.\n",
      " 0.         1.95233812 0.         1.95233812]\n",
      "all_sum is: [2.80080949 2.64262077 2.80080949 2.80080949 2.80080949 2.64262077\n",
      " 2.80080949 2.80080949 2.64262077 2.80080949 3.22524726 2.80080949\n",
      " 2.80080949 2.80080949 2.80080949 2.01314188 2.64262077 2.80080949\n",
      " 2.80080949 2.64262077 2.80080949 3.02230434 2.80080949 3.22524726\n",
      " 2.64262077 2.64262077 2.80080949 2.64262077 2.64262077 2.80080949\n",
      " 2.80080949 2.80080949 2.64262077 2.80080949 2.80080949 2.64262077\n",
      " 2.80080949 2.80080949 2.64262077 2.64262077 2.64262077 2.80080949\n",
      " 2.64262077 2.64262077 2.64262077 2.80080949 2.64262077 2.80080949\n",
      " 2.64262077 2.64262077 2.87135974 2.64262077 2.80080949 2.80080949\n",
      " 2.80080949 2.80080949 2.64262077 2.64262077 2.80080949 2.80080949\n",
      " 2.87135974 2.80080949 2.64262077 2.80080949 2.80080949 2.80080949\n",
      " 2.80080949 2.80080949 2.64262077 2.64262077 2.80080949 2.64262077\n",
      " 2.80080949 2.64262077 2.80080949 2.64262077 2.80080949 2.64262077\n",
      " 2.80080949 2.64262077 2.64262077 2.64262077 2.80080949 2.80080949\n",
      " 2.64262077 2.64262077 3.02230434 2.64262077 2.64262077 2.80080949\n",
      " 2.80080949 2.64262077 2.64262077 2.80080949 2.64262077 2.64262077\n",
      " 2.80080949 2.80080949 2.80080949 2.80080949]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.934\n",
      "(*) epoch 2, cost 2.585\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.539\n",
      "(*) epoch 2, cost 2.104\n",
      "(*) epoch 3, cost 1.496\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.35948539 2.91453464 3.35948539 3.35948539 2.91453464 3.35948539\n",
      " 3.35948539 2.91453464 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 3.35948539 3.35948539 2.91453464 2.91453464 3.35948539\n",
      " 3.35948539 1.59030166 1.59030166 3.35948539 3.35948539 1.59030166\n",
      " 3.35948539 3.35948539 2.91453464 3.35948539 3.35948539 2.91453464\n",
      " 3.35948539 3.35948539 3.35948539 3.35948539 1.59030166 3.35948539\n",
      " 3.35948539 3.35948539 3.35948539 3.35948539 2.91453464 3.35948539\n",
      " 3.35948539 3.35948539 1.59030166 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 1.59030166 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 2.91453464 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 1.59030166 3.35948539 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 3.35948539 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 1.59030166 3.35948539 3.35948539 3.35948539 3.35948539\n",
      " 3.35948539 3.35948539 1.59030166 3.35948539 2.91453464 3.35948539\n",
      " 3.35948539 1.59030166 3.35948539 1.59030166 3.35948539 3.35948539\n",
      " 3.35948539 3.35948539 3.35948539 3.35948539 2.91453464 3.35948539\n",
      " 4.06265104 1.59030166 3.35948539 3.35948539]\n",
      "all_sum is: [0.36663761 0.14759056 0.22499921 0.14759056 0.14759056 0.14759056\n",
      " 0.14759056 0.14759056 0.14759056 0.14759056 0.14759056 0.14759056\n",
      " 0.28922897 0.28922897 0.22499921 0.36663761 0.22499921 0.14759056\n",
      " 0.22499921 0.14759056 0.14759056 0.14759056 0.28922897 0.36663761\n",
      " 0.22499921 0.22499921 0.22499921 0.14759056 0.22499921 0.22499921\n",
      " 0.36663761 0.22499921 0.22499921 0.28922897 0.14759056 0.14759056\n",
      " 0.14759056 0.36663761 0.14759056 0.22499921 0.14759056 0.14759056\n",
      " 0.14759056 0.14759056 0.14759056 0.14759056 0.14759056 0.14759056\n",
      " 0.22499921 0.14759056 0.28922897 0.28922897 0.22499921 0.28922897\n",
      " 0.14759056 0.28922897 0.28922897 0.14759056 0.22499921 0.22499921\n",
      " 0.36663761 0.14759056 0.14759056 0.14759056 0.22499921 0.14759056\n",
      " 0.14759056 0.22499921 0.22499921 0.14759056 0.14759056 0.14759056\n",
      " 0.22499921 0.14759056 0.22499921 0.14759056 0.22499921 0.14759056\n",
      " 0.22499921 0.14759056 0.28922897 0.14759056 0.22499921 0.14759056\n",
      " 0.22499921 0.14759056 0.36663761 0.14759056 0.22499921 0.14759056\n",
      " 0.14759056 0.14759056 0.22499921 0.28922897 0.28922897 0.28922897\n",
      " 0.14759056 0.22499921 0.14759056 0.36663761]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.635\n",
      "(*) epoch 2, cost 2.457\n",
      "(*) epoch 3, cost 2.165\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.416\n",
      "(*) epoch 2, cost 3.353\n",
      "(*) epoch 3, cost 3.119\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.80930267 0.80930267 0.80930267 0.80930267 0.80930267 0.80930267\n",
      " 2.53280774 0.80930267 0.80930267 0.80930267 0.80930267 0.80930267\n",
      " 0.         0.80930267 0.80930267 0.80930267 0.80930267 0.80930267\n",
      " 1.37886071 0.80930267 0.80930267 0.80930267 0.03299275 0.80930267\n",
      " 0.80930267 0.80930267 0.80930267 0.03299275 0.80930267 0.80930267\n",
      " 0.03299275 0.80930267 2.53280774 0.80930267 1.75649782 0.80930267\n",
      " 0.80930267 0.80930267 0.03299275 0.80930267 0.80930267 0.03299275\n",
      " 1.30247696 0.80930267 0.80930267 0.80930267 0.03299275 0.80930267\n",
      " 0.80930267 0.03299275 0.03299275 0.80930267 0.77630992 0.80930267\n",
      " 0.80930267 0.03299275 0.80930267 0.80930267 0.03299275 1.37886071\n",
      " 0.80930267 0.80930267 0.80930267 0.80930267 0.80930267 0.80930267\n",
      " 0.03299275 0.03299275 0.80930267 0.80930267 0.80930267 0.52616704\n",
      " 0.80930267 0.80930267 0.80930267 0.80930267 0.80930267 0.03299275\n",
      " 0.80930267 0.80930267 0.80930267 1.30247696 0.77630992 0.80930267\n",
      " 0.80930267 0.80930267 2.53280774 0.80930267 0.80930267 0.03299275\n",
      " 0.03299275 0.80930267 0.52616704 0.03299275 0.03299275 0.80930267\n",
      " 0.80930267 0.80930267 0.80930267 0.80930267]\n",
      "all_sum is: [0.         0.01656144 0.01656144 0.01656144 0.01656144 0.01656144\n",
      " 0.         0.01656144 0.01656144 0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.         0.01656144 0.         0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.         0.         0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.         0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.01656144 0.         0.01656144 0.01656144\n",
      " 0.01656144 0.         0.         0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.         0.01656144 0.         0.01656144 0.01656144\n",
      " 0.01656144 0.         0.01656144 0.01656144 0.01656144 0.\n",
      " 0.         0.         0.01656144 0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.01656144 0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.01656144 0.         0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.01656144 0.         0.         0.01656144\n",
      " 0.         0.         0.33628546 0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.01656144 0.01656144 0.01656144 0.01656144\n",
      " 0.01656144 0.01656144 0.         0.         0.01656144 0.01656144\n",
      " 0.01656144 0.         0.01656144 0.01656144]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.755\n",
      "(*) epoch 2, cost 2.082\n",
      "(*) epoch 3, cost 1.739\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.804\n",
      "(*) epoch 2, cost 2.191\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.52447513 1.52447513 1.52447513 2.14509714 1.52447513 2.14509714\n",
      " 1.52447513 2.14509714 2.14509714 1.52447513 2.14509714 2.14509714\n",
      " 1.52447513 1.52447513 1.52447513 1.52447513 2.14509714 2.14509714\n",
      " 1.52447513 2.14509714 1.52447513 2.14509714 1.52447513 1.52447513\n",
      " 1.52447513 2.14509714 2.14509714 2.14509714 2.14509714 1.52447513\n",
      " 2.44856665 2.14509714 2.14509714 1.52447513 1.52447513 2.14509714\n",
      " 2.14509714 2.14509714 2.14509714 1.52447513 1.52447513 1.52447513\n",
      " 2.44856665 1.52447513 2.14509714 1.52447513 1.52447513 1.52447513\n",
      " 2.14509714 1.52447513 2.14509714 1.52447513 2.14509714 1.52447513\n",
      " 2.14509714 1.52447513 2.14509714 1.52447513 1.52447513 2.14509714\n",
      " 1.52447513 2.14509714 1.52447513 1.52447513 2.14509714 2.14509714\n",
      " 1.52447513 1.52447513 2.14509714 2.14509714 2.14509714 2.14509714\n",
      " 1.52447513 2.14509714 1.52447513 2.14509714 1.52447513 2.14509714\n",
      " 2.14509714 1.52447513 2.14509714 2.14509714 2.14509714 2.14509714\n",
      " 2.14509714 2.14509714 1.52447513 2.14509714 1.52447513 1.52447513\n",
      " 1.52447513 1.52447513 2.44856665 2.14509714 2.14509714 1.52447513\n",
      " 1.52447513 2.14509714 1.52447513 2.14509714]\n",
      "all_sum is: [2.18097079 2.18097079 1.23022176 1.23022176 1.23022176 1.23022176\n",
      " 2.18097079 1.23022176 1.23022176 1.23022176 1.23022176 1.23022176\n",
      " 2.18097079 2.18097079 2.18097079 1.23022176 2.18097079 2.18097079\n",
      " 1.23022176 1.23022176 1.23022176 2.18097079 2.18097079 1.23022176\n",
      " 2.18097079 1.23022176 2.18097079 2.18097079 1.23022176 1.23022176\n",
      " 2.18097079 2.18097079 1.23022176 1.23022176 1.23022176 2.18097079\n",
      " 1.23022176 1.23022176 1.23022176 2.18097079 2.18097079 1.23022176\n",
      " 1.23022176 2.18097079 2.18097079 2.18097079 2.18097079 1.23022176\n",
      " 2.18097079 1.23022176 1.23022176 2.18097079 2.18097079 2.18097079\n",
      " 2.18097079 2.18097079 2.18097079 2.18097079 1.23022176 2.18097079\n",
      " 2.18097079 1.23022176 2.18097079 2.18097079 2.18097079 1.23022176\n",
      " 1.23022176 1.23022176 1.23022176 2.18097079 1.23022176 2.18097079\n",
      " 2.18097079 2.18097079 2.18097079 2.18097079 1.23022176 2.18097079\n",
      " 2.18097079 1.23022176 2.18097079 1.23022176 1.23022176 2.18097079\n",
      " 1.23022176 1.23022176 2.18097079 1.23022176 2.18097079 2.18097079\n",
      " 2.18097079 1.23022176 2.18097079 2.18097079 2.18097079 1.23022176\n",
      " 1.23022176 1.23022176 2.18097079 2.18097079]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.231\n",
      "(*) epoch 2, cost 2.270\n",
      "(*) epoch 3, cost 1.911\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.647\n",
      "(*) epoch 2, cost 2.743\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.39033473 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.15767737 1.15767737 1.15767737 1.15767737 0.         1.15767737\n",
      " 2.21150767 1.15767737 1.15767737 1.15767737 3.36918504 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 0.         1.15767737 0.\n",
      " 3.36918504 3.36918504 1.15767737 0.         1.15767737 3.36918504\n",
      " 1.15767737 2.21150767 0.         1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 3.36918504 0.         1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 1.15767737 1.15767737 1.15767737 3.36918504 1.15767737\n",
      " 0.         1.15767737 0.         3.36918504 1.15767737 1.15767737\n",
      " 3.36918504 1.15767737 3.36918504 1.15767737 1.15767737 1.15767737\n",
      " 1.15767737 3.36918504 1.15767737 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 2\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.322\n",
      "(*) epoch 2, cost 0.424\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.034\n",
      "(*) epoch 2, cost 2.661\n",
      "(*) epoch 3, cost 2.361\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.62567009 3.62567009 3.64502932 1.43886127 3.62567009 2.42386033\n",
      " 2.4045011  2.4045011  2.66003026 2.83156736 2.4045011  3.62567009\n",
      " 3.38950016 1.43886127 4.61066915 3.38950016 4.61066915 3.38950016\n",
      " 4.61066915 3.38950016 3.62567009 3.62567009 3.62567009 2.4045011\n",
      " 2.4045011  4.61066915 3.81656641 4.61066915 2.85092658 3.38950016\n",
      " 4.61066915 3.62567009 4.61066915 3.62567009 2.4045011  4.61066915\n",
      " 4.61066915 2.4045011  3.62567009 3.38950016 2.66003026 3.38950016\n",
      " 2.4045011  4.61066915 2.4045011  3.62567009 2.4045011  3.38950016\n",
      " 3.62567009 2.66003026 3.62567009 3.62567009 3.62567009 3.62567009\n",
      " 1.43886127 2.42386033 3.62567009 3.38950016 3.81656641 3.62567009\n",
      " 1.43886127 2.4045011  4.61066915 2.66003026 3.38950016 4.61066915\n",
      " 3.62567009 3.62567009 2.83156736 3.62567009 3.62567009 4.61066915\n",
      " 2.4045011  3.62567009 3.38950016 3.38950016 3.38950016 3.62567009\n",
      " 3.62567009 3.64502932 3.62567009 2.4045011  2.4045011  2.4045011\n",
      " 4.61066915 4.61066915 3.62567009 3.38950016 2.83156736 3.38950016\n",
      " 3.62567009 2.4045011  2.66003026 4.61066915 2.4045011  3.62567009\n",
      " 3.62567009 3.62567009 1.43886127 3.62567009]\n",
      "all_sum is: [0.30945013 1.73415152 2.06767124 0.64296985 2.06767124 1.73415152\n",
      " 2.06767124 2.06767124 2.06767124 1.73415152 2.06767124 2.06767124\n",
      " 0.64296985 0.64296985 2.06767124 2.06767124 2.06767124 1.73415152\n",
      " 2.06767124 1.73415152 0.64296985 2.71496292 1.73415152 2.06767124\n",
      " 0.64296985 2.06767124 2.06767124 2.06767124 1.73415152 3.04848264\n",
      " 0.64296985 2.06767124 2.06767124 2.06767124 1.73415152 2.06767124\n",
      " 2.06767124 1.73415152 2.06767124 0.64296985 0.64296985 2.06767124\n",
      " 2.06767124 0.64296985 2.06767124 0.64296985 3.04848264 0.64296985\n",
      " 2.06767124 0.30945013 2.06767124 1.29026153 2.06767124 1.73415152\n",
      " 2.06767124 2.06767124 0.64296985 2.06767124 3.04848264 2.06767124\n",
      " 2.06767124 0.64296985 2.06767124 2.06767124 2.06767124 0.30945013\n",
      " 0.30945013 3.04848264 2.06767124 2.06767124 1.73415152 1.73415152\n",
      " 3.04848264 2.06767124 2.06767124 0.30945013 1.73415152 2.06767124\n",
      " 2.06767124 2.06767124 2.06767124 2.06767124 0.64296985 2.06767124\n",
      " 2.06767124 2.06767124 2.06767124 2.06767124 2.06767124 2.06767124\n",
      " 2.06767124 0.64296985 2.06767124 1.73415152 0.64296985 2.06767124\n",
      " 2.06767124 2.06767124 1.73415152 1.73415152]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.182\n",
      "(*) epoch 2, cost 2.669\n",
      "(*) epoch 3, cost 2.334\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.776\n",
      "(*) epoch 2, cost 1.236\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.05788135 0.05788135 0.05788135 0.05788135 0.         0.\n",
      " 0.05788135 0.         0.05788135 0.05788135 0.05788135 0.\n",
      " 0.05788135 0.05788135 0.         0.05788135 0.05788135 0.\n",
      " 0.05788135 0.05788135 1.74192931 0.         0.05788135 0.\n",
      " 0.05788135 0.05788135 0.         0.05788135 0.05788135 0.\n",
      " 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135\n",
      " 0.05788135 0.         0.05788135 0.05788135 0.         0.\n",
      " 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135\n",
      " 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135 0.05788135\n",
      " 0.05788135 0.         0.         0.05788135 0.         0.\n",
      " 0.         0.05788135 0.41117573 0.         0.05788135 0.05788135\n",
      " 0.41117573 0.05788135 0.         0.05788135 0.05788135 0.05788135\n",
      " 0.         0.05788135 0.05788135 0.         0.05788135 0.05788135\n",
      " 0.         0.05788135 0.05788135 0.05788135 0.05788135 0.\n",
      " 0.05788135 0.         0.05788135 0.         0.05788135 0.05788135\n",
      " 1.74192931 0.05788135 0.05788135 0.         0.         0.05788135\n",
      " 0.05788135 0.05788135 0.05788135 0.        ]\n",
      "all_sum is: [1.42274548 1.42274548 1.42274548 1.42274548 1.42274548 1.49530354\n",
      " 1.49530354 1.49530354 1.42274548 1.49530354 1.42274548 1.49530354\n",
      " 1.42274548 1.42274548 1.49530354 1.49530354 1.42274548 1.49530354\n",
      " 1.49530354 1.42274548 1.49530354 1.49530354 1.42274548 1.49530354\n",
      " 1.49530354 1.42274548 1.42274548 2.73304572 1.42274548 1.49530354\n",
      " 1.49530354 1.49530354 1.49530354 1.49530354 1.49530354 1.49530354\n",
      " 2.80560378 1.42274548 1.49530354 1.49530354 1.42274548 1.49530354\n",
      " 1.49530354 1.42274548 1.49530354 1.49530354 1.42274548 1.42274548\n",
      " 1.49530354 1.49530354 1.42274548 1.49530354 1.42274548 1.42274548\n",
      " 1.42274548 1.49530354 1.42274548 1.42274548 1.42274548 1.49530354\n",
      " 1.49530354 1.42274548 1.42274548 1.49530354 1.49530354 2.80560378\n",
      " 1.49530354 1.42274548 1.42274548 1.42274548 1.49530354 1.49530354\n",
      " 1.49530354 1.49530354 1.42274548 1.42274548 1.42274548 1.49530354\n",
      " 1.49530354 1.42274548 1.42274548 1.42274548 1.49530354 1.49530354\n",
      " 1.49530354 1.49530354 1.49530354 1.49530354 1.49530354 1.42274548\n",
      " 1.42274548 1.42274548 1.42274548 1.49530354 1.42274548 1.49530354\n",
      " 1.42274548 1.49530354 1.49530354 1.49530354]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.954\n",
      "(*) epoch 2, cost 2.576\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.930\n",
      "(*) epoch 2, cost 2.112\n",
      "(*) epoch 3, cost 1.827\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102 0.96884102\n",
      " 0.96884102 0.96884102 0.96884102 0.96884102]\n",
      "all_sum is: [0.00600851 0.00600851 0.00600851 0.39041283 0.38440432 0.38440432\n",
      " 0.38440432 0.         0.38440432 0.00600851 1.29083986 0.00600851\n",
      " 0.00600851 0.00600851 0.00600851 1.29083986 0.39041283 0.00600851\n",
      " 0.39041283 0.00600851 0.39041283 0.00600851 0.00600851 0.39041283\n",
      " 0.00600851 0.90643554 0.00600851 0.00600851 0.00600851 0.00600851\n",
      " 0.00600851 0.39041283 0.00600851 0.00600851 0.39041283 0.00600851\n",
      " 0.39041283 0.39041283 0.00600851 0.00600851 0.         0.00600851\n",
      " 0.39041283 0.00600851 0.00600851 0.00600851 0.00600851 0.39041283\n",
      " 0.39041283 0.00600851 0.00600851 0.39041283 0.39041283 0.00600851\n",
      " 0.39041283 0.00600851 0.39041283 0.00600851 0.39041283 0.38440432\n",
      " 0.39041283 0.00600851 0.         0.39041283 0.38440432 0.00600851\n",
      " 0.00600851 0.00600851 0.         0.39041283 0.00600851 0.\n",
      " 0.00600851 0.00600851 0.00600851 0.00600851 0.39041283 0.90643554\n",
      " 0.         0.90643554 0.39041283 0.00600851 0.00600851 0.90042703\n",
      " 0.         0.39041283 0.39041283 0.00600851 0.39041283 0.39041283\n",
      " 0.00600851 0.90042703 0.39041283 0.00600851 0.90643554 0.00600851\n",
      " 0.39041283 0.00600851 0.39041283 1.29083986]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.572\n",
      "(*) epoch 2, cost 1.888\n",
      "(*) epoch 3, cost 1.592\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.683\n",
      "(*) epoch 2, cost 2.479\n",
      "(*) epoch 3, cost 2.159\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.52944355 0.52944355 0.52944355 0.52944355 0.66072909 0.52944355\n",
      " 0.66072909 0.52944355 0.71549085 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.71549085 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.71549085 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.71549085 0.52944355 0.52944355 0.52944355\n",
      " 0.66072909 0.52944355 0.84677639 0.71549085 0.52944355 0.71549085\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.66072909 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 1.01464362 0.71549085 0.52944355 0.52944355\n",
      " 0.66072909 0.52944355 0.52944355 0.52944355 0.52944355 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355 0.66072909 0.52944355\n",
      " 0.52944355 0.52944355 0.52944355 0.52944355]\n",
      "all_sum is: [4.04238506 4.04238506 4.13421315 4.52285494 4.04238506 4.04238506\n",
      " 4.13421315 4.97273514 4.04238506 3.14570441 4.97273514 4.13421315\n",
      " 4.04238506 4.04238506 4.43102684 4.04238506 4.88090704 4.13421315\n",
      " 2.33665941 5.39601931 4.13421315 5.39601931 4.13421315 4.04238506\n",
      " 4.04238506 5.39601931 4.04238506 4.04238506 4.04238506 4.13421315\n",
      " 4.04238506 4.04238506 4.04238506 4.04238506 4.88090704 4.04238506\n",
      " 5.39601931 4.43102684 5.39601931 4.13421315 4.04238506 5.39601931\n",
      " 4.04238506 4.04238506 4.04238506 4.04238506 5.39601931 4.13421315\n",
      " 4.04238506 5.48784741 4.04238506 5.39601931 4.43102684 4.13421315\n",
      " 4.04238506 2.33665941 4.04238506 5.26954883 5.39601931 4.97273514\n",
      " 5.39601931 4.04238506 4.04238506 4.04238506 5.7846611  5.9659462\n",
      " 4.13421315 4.13421315 4.04238506 4.04238506 2.33665941 4.04238506\n",
      " 5.9659462  5.39601931 4.13421315 4.04238506 4.04238506 4.04238506\n",
      " 4.13421315 4.04238506 4.04238506 4.13421315 4.13421315 4.04238506\n",
      " 4.04238506 5.48784741 4.04238506 4.13421315 2.42848751 4.04238506\n",
      " 5.39601931 4.04238506 4.04238506 4.04238506 4.04238506 4.04238506\n",
      " 4.04238506 4.49933867 4.04238506 5.39601931]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.133\n",
      "(*) epoch 2, cost 1.607\n",
      "(*) epoch 3, cost 1.049\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.535\n",
      "(*) epoch 2, cost 2.880\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.96391942 0.43953434 1.40345376 0.96391942\n",
      " 0.43953434 0.96391942 0.43953434 0.         0.43953434 1.40345376\n",
      " 0.         0.43953434 0.43953434 0.         1.40345376 0.43953434\n",
      " 0.         1.40345376 0.43953434 0.96391942 0.         0.43953434\n",
      " 1.40345376 0.96391942 1.40345376 0.43953434 0.         0.\n",
      " 0.         1.40345376 0.         0.43953434 0.43953434 0.43953434\n",
      " 0.96391942 0.96391942 0.96391942 0.         0.43953434 0.\n",
      " 0.         0.         0.         0.         0.43953434 0.43953434\n",
      " 0.96391942 0.         0.43953434 1.40345376 0.         1.95310389\n",
      " 0.43953434 0.43953434 0.43953434 0.43953434 1.51356955 1.51356955\n",
      " 0.         0.         0.43953434 0.43953434 0.96391942 1.40345376\n",
      " 0.43953434 0.96391942 0.         0.         1.40345376 0.43953434\n",
      " 0.43953434 0.96391942 0.         0.         1.40345376 0.\n",
      " 0.96391942 0.         0.         0.         0.         0.\n",
      " 1.40345376 1.40345376 1.95310389 1.40345376 0.43953434 1.40345376\n",
      " 0.         0.         0.43953434 0.43953434 0.43953434 0.43953434\n",
      " 0.43953434 0.         0.96391942 0.        ]\n",
      "all_sum is: [1.09301379 1.09301379 1.09301379 1.09301379 1.29524583 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.29524583 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.29524583 1.29524583 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.29524583 1.09301379 1.39441851 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.29524583 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.29524583 1.09301379 1.09301379 1.39441851 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.29524583 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.29524583 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.29524583\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379 1.09301379\n",
      " 1.09301379 1.09301379 1.09301379 1.09301379]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.870\n",
      "(*) epoch 2, cost 1.649\n",
      "(*) epoch 3, cost 1.456\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.031\n",
      "(*) epoch 2, cost 0.675\n",
      "(*) epoch 3, cost 0.453\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.        0.621362  0.        0.        0.        0.0893374 0.\n",
      " 0.        0.5320246 0.0893374 0.        0.0893374 0.0893374 0.\n",
      " 0.        0.        0.0893374 0.0893374 0.        0.0893374 0.\n",
      " 0.5320246 0.0893374 0.        0.        0.0893374 0.        0.\n",
      " 0.        0.        0.        0.5320246 0.0893374 0.        0.0893374\n",
      " 0.        0.        0.621362  0.        0.0893374 0.        0.\n",
      " 0.        0.        0.        0.5320246 0.        0.        0.0893374\n",
      " 0.        0.        0.        0.5320246 0.0893374 0.        0.0893374\n",
      " 0.        0.        0.        0.        0.        0.0893374 0.0893374\n",
      " 0.        0.        0.        0.        0.        0.0893374 0.\n",
      " 0.        0.        0.0893374 0.        0.0893374 0.        0.\n",
      " 0.5320246 0.        0.        0.        0.        0.0893374 0.\n",
      " 0.        0.0893374 0.        0.        0.        0.        0.\n",
      " 0.0893374 0.        0.        0.0893374 0.        0.        0.\n",
      " 0.0893374 0.       ]\n",
      "all_sum is: [2.01591534 2.01591534 3.13786398 3.13786398 3.13786398 2.01591534\n",
      " 3.13786398 2.01591534 3.13786398 2.01591534 2.01591534 3.13786398\n",
      " 3.13786398 3.13786398 3.13786398 2.01591534 3.13786398 3.13786398\n",
      " 3.13786398 2.01591534 2.01591534 2.81037026 2.01591534 3.13786398\n",
      " 3.13786398 2.01591534 3.13786398 3.13786398 2.01591534 2.81037026\n",
      " 3.13786398 2.01591534 3.13786398 2.01591534 2.01591534 3.13786398\n",
      " 3.13786398 3.13786398 2.01591534 2.01591534 1.68842162 2.01591534\n",
      " 3.13786398 2.81037026 2.01591534 3.13786398 3.13786398 3.13786398\n",
      " 2.81037026 1.68842162 3.13786398 3.13786398 2.01591534 3.13786398\n",
      " 3.13786398 3.13786398 3.13786398 3.13786398 2.81037026 3.13786398\n",
      " 3.13786398 2.01591534 2.01591534 2.01591534 3.13786398 3.13786398\n",
      " 3.13786398 3.13786398 2.01591534 3.13786398 3.13786398 2.01591534\n",
      " 2.81037026 2.01591534 3.13786398 3.13786398 3.13786398 3.13786398\n",
      " 3.13786398 3.13786398 3.13786398 3.13786398 2.01591534 3.13786398\n",
      " 3.13786398 1.68842162 3.13786398 2.01591534 3.13786398 2.01591534\n",
      " 2.01591534 3.13786398 3.13786398 3.13786398 3.13786398 1.68842162\n",
      " 2.81037026 2.01591534 3.13786398 2.01591534]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.097\n",
      "(*) epoch 2, cost 2.062\n",
      "(*) epoch 3, cost 1.789\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.525\n",
      "(*) epoch 2, cost 3.042\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.1592966  2.51034101 2.51034101 2.35619944 2.51034101 2.51034101\n",
      " 2.51034101 2.51034101 2.51034101 2.35619944 2.51034101 2.51034101\n",
      " 2.51034101 2.51034101 2.51034101 2.51034101 2.51034101 2.51034101\n",
      " 3.1592966  2.35619944 2.51034101 2.51034101 3.1592966  2.35619944\n",
      " 2.35619944 2.51034101 3.1592966  2.51034101 3.00515503 2.35619944\n",
      " 2.51034101 2.35619944 2.35619944 2.51034101 2.51034101 3.1592966\n",
      " 2.51034101 2.35619944 2.51034101 2.51034101 2.35619944 2.51034101\n",
      " 3.1592966  2.51034101 2.51034101 3.00515503 2.35619944 2.51034101\n",
      " 2.35619944 2.51034101 2.35619944 2.51034101 2.51034101 2.35619944\n",
      " 2.51034101 2.51034101 3.1592966  2.51034101 2.51034101 2.51034101\n",
      " 2.51034101 2.51034101 2.51034101 2.35619944 3.1592966  2.35619944\n",
      " 2.51034101 3.1592966  2.35619944 2.35619944 2.51034101 2.51034101\n",
      " 2.51034101 3.1592966  2.35619944 2.51034101 2.51034101 2.51034101\n",
      " 2.51034101 2.35619944 2.51034101 2.51034101 2.51034101 2.35619944\n",
      " 2.51034101 2.51034101 3.1592966  2.51034101 2.51034101 2.35619944\n",
      " 2.51034101 2.35619944 2.51034101 2.51034101 2.35619944 2.51034101\n",
      " 2.35619944 2.51034101 2.51034101 2.51034101]\n",
      "all_sum is: [1.63524256 1.22035754 1.63524256 1.22035754 1.13674818 1.63524256\n",
      " 1.63524256 1.63524256 1.63524256 1.63524256 1.63524256 1.13674818\n",
      " 1.22035754 1.22035754 0.72186316 1.13674818 0.72186316 1.22035754\n",
      " 1.22035754 1.22035754 1.22035754 1.63524256 1.63524256 1.63524256\n",
      " 1.63524256 1.63524256 1.63524256 1.63524256 1.63524256 1.63524256\n",
      " 1.22035754 1.63524256 1.63524256 1.63524256 2.05280077 1.63524256\n",
      " 2.46768579 1.63524256 1.22035754 2.46768579 1.63524256 1.63524256\n",
      " 1.63524256 1.63524256 1.22035754 2.46768579 2.06124838 1.63524256\n",
      " 1.63524256 1.63524256 1.63524256 1.13674818 2.06124838 1.13674818\n",
      " 1.63524256 1.63524256 1.63524256 1.63524256 1.13674818 2.06124838\n",
      " 1.13674818 1.13674818 1.63524256 1.63524256 1.22035754 1.63524256\n",
      " 1.63524256 1.63524256 1.22035754 1.63524256 1.63524256 2.46768579\n",
      " 2.06124838 1.63524256 1.63524256 1.63524256 3.11215795 1.63524256\n",
      " 1.63524256 1.63524256 1.63524256 1.63524256 1.22035754 1.63524256\n",
      " 1.63524256 1.22035754 1.63524256 1.22035754 1.13674818 1.22035754\n",
      " 1.63524256 1.63524256 1.13674818 1.63524256 1.63524256 1.63524256\n",
      " 1.63524256 1.13674818 1.63524256 1.22035754]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.275\n",
      "(*) epoch 2, cost 2.429\n",
      "(*) epoch 3, cost 2.076\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.255\n",
      "(*) epoch 2, cost 4.014\n",
      "(*) epoch 3, cost 3.708\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.86804248 3.16036511 3.9456472  3.9456472  3.65332457 3.9456472\n",
      " 3.9456472  2.86804248 3.78903534 3.16036511 3.16036511 3.16036511\n",
      " 2.86804248 3.65332457 3.16036511 3.65332457 2.86804248 2.86804248\n",
      " 2.86804248 3.16036511 2.86804248 3.16036511 3.65332457 2.86804248\n",
      " 3.16036511 2.86804248 3.9456472  3.16036511 3.65332457 3.16036511\n",
      " 3.16036511 3.9456472  3.65332457 2.86804248 2.86804248 2.86804248\n",
      " 3.65332457 2.86804248 3.16036511 3.9456472  3.9456472  3.9456472\n",
      " 2.86804248 3.9456472  3.9456472  3.65332457 2.86804248 2.86804248\n",
      " 2.86804248 3.9456472  3.65332457 3.65332457 3.16036511 2.86804248\n",
      " 2.86804248 2.86804248 3.16036511 3.9456472  3.16036511 3.49671271\n",
      " 2.86804248 3.65332457 3.16036511 3.16036511 2.86804248 3.16036511\n",
      " 3.65332457 3.65332457 2.86804248 3.16036511 3.16036511 2.86804248\n",
      " 3.16036511 3.16036511 3.49671271 3.65332457 3.9456472  3.16036511\n",
      " 3.65332457 2.86804248 3.65332457 3.16036511 3.16036511 3.16036511\n",
      " 2.86804248 3.16036511 3.78903534 3.16036511 3.9456472  2.86804248\n",
      " 3.16036511 3.9456472  3.16036511 2.86804248 3.65332457 3.65332457\n",
      " 3.9456472  2.86804248 3.9456472  2.86804248]\n",
      "all_sum is: [2.08390262 0.7602589  1.20543085 1.63873066 2.74097968 1.63873066\n",
      " 1.83269209 1.63873066 1.63873066 1.39939227 1.63873066 0.7602589\n",
      " 0.7602589  1.63873066 1.63873066 1.63873066 0.7602589  1.63873066\n",
      " 1.63873066 3.06062633 2.08390262 1.63873066 1.63873066 1.63873066\n",
      " 2.08390262 1.98819314 0.7602589  1.20543085 0.7602589  1.20543085\n",
      " 1.48300584 2.08390262 0.95422032 1.63873066 2.08390262 1.63873066\n",
      " 2.27786404 1.83269209 1.63873066 1.63873066 2.27786404 2.27786404\n",
      " 1.63873066 1.63873066 2.08390262 2.8666649  1.63873066 1.63873066\n",
      " 1.63873066 1.63873066 2.27786404 0.7602589  1.63873066 2.08390262\n",
      " 0.7602589  1.63873066 1.63873066 2.08390262 1.63873066 1.63873066\n",
      " 1.63873066 1.63873066 0.95422032 0.60453407 1.98819314 1.83269209\n",
      " 0.95422032 0.7602589  0.7602589  1.63873066 1.63873066 1.63873066\n",
      " 1.63873066 0.7602589  1.63873066 1.20543085 0.7602589  1.63873066\n",
      " 0.95422032 1.83269209 1.63873066 1.63873066 1.83269209 0.95422032\n",
      " 1.63873066 1.63873066 1.63873066 1.63873066 0.95422032 1.63873066\n",
      " 0.95422032 0.7602589  1.63873066 2.8666649  1.83269209 1.63873066\n",
      " 1.63873066 1.63873066 2.08390262 1.63873066]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.237\n",
      "(*) epoch 2, cost 1.876\n",
      "(*) epoch 3, cost 1.410\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.846\n",
      "(*) epoch 2, cost 3.166\n",
      "(*) epoch 3, cost 2.917\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.48426087 2.79207545 2.2406056  2.2406056  2.79207545 2.79207545\n",
      " 3.03573072 2.2406056  2.79207545 2.2406056  2.2406056  2.2406056\n",
      " 2.2406056  2.2406056  2.2406056  2.48426087 2.48426087 2.79207545\n",
      " 2.2406056  2.2406056  2.79207545 2.2406056  2.2406056  2.79207545\n",
      " 2.2406056  2.79207545 3.03573072 2.2406056  2.80864391 2.2406056\n",
      " 2.48426087 2.2406056  2.2406056  2.2406056  2.2406056  2.50082932\n",
      " 2.25717406 2.79207545 2.25717406 2.2406056  2.48426087 2.2406056\n",
      " 2.2406056  2.79207545 2.25717406 2.48426087 2.25717406 2.2406056\n",
      " 2.2406056  3.03573072 2.79207545 2.2406056  2.2406056  2.2406056\n",
      " 2.79207545 2.79207545 2.2406056  2.25717406 2.79207545 2.2406056\n",
      " 2.80864391 2.2406056  2.2406056  2.2406056  2.2406056  2.48426087\n",
      " 2.2406056  2.25717406 3.03573072 2.2406056  2.2406056  2.2406056\n",
      " 2.48426087 2.2406056  2.48426087 2.79207545 2.2406056  2.48426087\n",
      " 2.2406056  2.2406056  2.79207545 2.48426087 2.50082932 2.2406056\n",
      " 2.2406056  2.2406056  2.2406056  2.25717406 3.03573072 2.48426087\n",
      " 2.2406056  2.2406056  2.2406056  2.2406056  2.50082932 2.2406056\n",
      " 2.2406056  2.2406056  2.48426087 2.2406056 ]\n",
      "all_sum is: [0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  2.23735509 0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  2.23735509 0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  2.23735509\n",
      " 0.8817983  0.8817983  0.8817983  2.23735509 0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  2.23735509\n",
      " 1.70323545 0.8817983  0.8817983  0.8817983  0.8817983  1.48787813\n",
      " 1.48787813 0.8817983  0.9716082  0.8817983  2.23735509 0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983  0.8817983  0.8817983\n",
      " 0.8817983  0.8817983  0.8817983  1.48787813 0.8817983  1.70323545\n",
      " 0.8817983  0.8817983  0.8817983  0.8817983 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.714\n",
      "(*) epoch 2, cost 2.258\n",
      "(*) epoch 3, cost 1.996\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.235\n",
      "(*) epoch 2, cost 0.919\n",
      "(*) epoch 3, cost 0.776\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.2400494  1.62375116 2.41904708 1.62375116 2.44475348 2.44475348\n",
      " 2.41904708 2.44475348 1.62375116 2.41904708 2.44475348 2.41904708\n",
      " 2.41904708 2.44475348 3.2400494  1.62375116 1.62375116 1.62375116\n",
      " 2.44475348 2.44475348 1.62375116 3.2400494  2.44475348 2.41904708\n",
      " 1.62375116 2.44475348 3.2400494  2.41904708 1.62375116 2.44475348\n",
      " 2.44475348 1.62375116 2.41904708 1.62375116 2.44475348 1.62375116\n",
      " 1.62375116 2.44475348 2.41904708 2.41904708 3.2400494  1.62375116\n",
      " 3.2400494  1.62375116 3.2400494  1.62375116 1.62375116 2.41904708\n",
      " 2.41904708 1.62375116 3.2400494  2.41904708 2.44475348 2.44475348\n",
      " 2.41904708 2.41904708 2.41904708 2.44475348 2.41904708 2.44475348\n",
      " 2.41904708 1.62375116 1.62375116 2.41904708 2.44475348 3.2400494\n",
      " 3.2400494  1.62375116 2.44475348 1.62375116 2.44475348 2.44475348\n",
      " 3.2400494  3.2400494  2.44475348 2.44475348 2.44475348 1.62375116\n",
      " 3.2400494  1.62375116 3.2400494  3.2400494  2.44475348 1.62375116\n",
      " 2.41904708 1.62375116 1.62375116 1.62375116 3.2400494  2.44475348\n",
      " 2.44475348 2.41904708 2.41904708 3.2400494  2.41904708 2.41904708\n",
      " 3.2400494  2.41904708 2.41904708 2.44475348]\n",
      "all_sum is: [0.91243986 0.91243986 1.39258759 0.91243986 0.91243986 1.91145962\n",
      " 1.19243535 0.91243986 1.91145962 1.39258759 1.91145962 0.91243986\n",
      " 0.91243986 0.19341559 1.91145962 1.67258308 1.19243535 1.39258759\n",
      " 0.19341559 1.39258759 1.91145962 0.91243986 1.91145962 0.91243986\n",
      " 0.91243986 0.91243986 1.19243535 1.19243535 1.91145962 0.19341559\n",
      " 1.91145962 1.91145962 1.91145962 0.19341559 1.91145962 1.19243535\n",
      " 1.91145962 0.91243986 1.19243535 1.91145962 1.39258759 0.91243986\n",
      " 1.67258308 1.91145962 0.91243986 1.91145962 0.19341559 0.19341559\n",
      " 1.91145962 1.19243535 0.91243986 1.39258759 0.91243986 1.91145962\n",
      " 1.91145962 1.91145962 1.19243535 1.91145962 0.91243986 1.39258759\n",
      " 0.91243986 1.19243535 1.19243535 1.91145962 1.39258759 1.19243535\n",
      " 0.91243986 1.91145962 0.91243986 2.39160735 0.91243986 0.91243986\n",
      " 1.91145962 1.91145962 0.91243986 1.91145962 2.39160735 1.91145962\n",
      " 0.91243986 0.19341559 0.91243986 2.39160735 1.91145962 0.91243986\n",
      " 0.67356332 2.39160735 0.19341559 1.91145962 0.67356332 0.67356332\n",
      " 1.91145962 0.91243986 0.91243986 1.19243535 1.67258308 0.19341559\n",
      " 1.19243535 2.39160735 0.19341559 0.19341559]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.288\n",
      "(*) epoch 2, cost 1.914\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.262\n",
      "(*) epoch 2, cost 3.699\n",
      "(*) epoch 3, cost 3.506\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02215043 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.02215043 0.\n",
      " 0.         0.02215043 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.15735321 0.02215043 0.         0.\n",
      " 0.         0.         0.         0.02215043 0.         0.\n",
      " 0.         0.         0.6163109  0.         0.         0.\n",
      " 0.         0.02215043 0.6163109  0.         0.         0.\n",
      " 0.02215043 0.         0.02215043 0.         0.02215043 0.\n",
      " 0.         0.         0.02215043 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.02215043 0.         0.02215043 0.         0.         0.\n",
      " 0.02215043 0.02215043 0.         0.         0.         0.02215043\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.48445098 2.92133402 2.92133402 1.51991628 0.08303324 2.92133402\n",
      " 0.08303324 0.08303324 2.92133402 2.92133402 1.48445098 1.51991628\n",
      " 2.92133402 1.48445098 1.51991628 2.92133402 2.92133402 0.08303324\n",
      " 2.92133402 1.51991628 1.51991628 2.92133402 1.48445098 1.51991628\n",
      " 1.51991628 2.92133402 2.92133402 1.51991628 1.51991628 1.48445098\n",
      " 1.48445098 2.92133402 0.08303324 1.51991628 0.08303324 1.51991628\n",
      " 1.48445098 1.48445098 2.92133402 2.92133402 1.51991628 2.92133402\n",
      " 2.92133402 2.92133402 2.92133402 2.92133402 2.92133402 1.48445098\n",
      " 1.51991628 0.08303324 2.92133402 0.08303324 1.48445098 1.48445098\n",
      " 1.51991628 1.48445098 2.92133402 0.08303324 2.92133402 1.51991628\n",
      " 1.51991628 2.92133402 2.92133402 1.51991628 2.92133402 1.48445098\n",
      " 1.48445098 0.08303324 2.92133402 1.48445098 1.51991628 2.92133402\n",
      " 2.92133402 2.92133402 2.92133402 1.51991628 2.92133402 1.51991628\n",
      " 1.51991628 0.08303324 2.92133402 0.08303324 0.08303324 1.48445098\n",
      " 1.72799884 2.92133402 2.92133402 1.51991628 1.51991628 1.51991628\n",
      " 2.92133402 1.48445098 1.51991628 0.08303324 2.92133402 1.51991628\n",
      " 1.48445098 0.08303324 2.92133402 0.08303324]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.885\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.659\n",
      "(*) epoch 2, cost 3.058\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.76348925 2.35917668 2.35917668 2.35917668 0.76348925 2.35917668\n",
      " 0.76348925 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 0.76348925 2.35917668 0.76348925 0.76348925 2.35917668 2.35917668\n",
      " 2.35917668 0.76348925 0.76348925 0.76348925 2.35917668 0.76348925\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 0.76348925 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 0.76348925 2.35917668 2.35917668 2.35917668 0.76348925 2.35917668\n",
      " 2.35917668 0.76348925 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 0.76348925 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 0.76348925 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668 2.35917668\n",
      " 0.76348925 0.76348925 0.76348925 2.35917668 2.35917668 2.35917668\n",
      " 0.76348925 2.35917668 0.76348925 2.35917668]\n",
      "all_sum is: [3.42559464 3.42559464 3.37554744 3.37554744 3.37554744 3.42559464\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.37554744 3.42559464\n",
      " 3.42559464 3.37554744 3.42559464 3.42559464 3.42559464 3.42559464\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464\n",
      " 3.37554744 3.37554744 3.37554744 3.42559464 3.42559464 3.42559464\n",
      " 3.37554744 3.42559464 3.42559464 3.37554744 3.42559464 3.37554744\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464 1.80460718\n",
      " 3.37554744 3.42559464 3.42559464 3.42559464 3.37554744 3.42559464\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.37554744 3.42559464\n",
      " 3.42559464 3.37554744 3.42559464 3.37554744 3.42559464 3.42559464\n",
      " 3.42559464 3.37554744 3.37554744 3.42559464 3.37554744 3.37554744\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464\n",
      " 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464 3.42559464\n",
      " 3.42559464 3.42559464 3.37554744 3.42559464 3.42559464 3.42559464\n",
      " 3.42559464 3.37554744 3.42559464 3.37554744 3.42559464 3.42559464\n",
      " 3.42559464 3.42559464 3.42559464 3.37554744 3.42559464 3.42559464\n",
      " 3.37554744 3.42559464 3.37554744 3.42559464]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.308\n",
      "(*) epoch 2, cost 3.735\n",
      "(*) epoch 3, cost 3.370\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.107\n",
      "(*) epoch 2, cost 1.532\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.0785451  0.66768345 0.0785451  0.0785451  0.78002534 0.66768345\n",
      " 0.0785451  0.0785451  0.0785451  0.0785451  0.0785451  0.66768345\n",
      " 0.66768345 0.0785451  0.19088699 0.0785451  0.66768345 0.0785451\n",
      " 0.78002534 0.0785451  0.0785451  0.0785451  0.66768345 0.0785451\n",
      " 0.0785451  0.66768345 0.0785451  0.0785451  0.0785451  0.0785451\n",
      " 0.66768345 0.66768345 0.0785451  0.0785451  0.66768345 0.0785451\n",
      " 0.66768345 0.66768345 0.0785451  0.0785451  0.0785451  0.66768345\n",
      " 0.66768345 0.66768345 0.0785451  0.0785451  0.78002534 0.0785451\n",
      " 0.0785451  0.19088699 0.0785451  0.0785451  0.66768345 0.66768345\n",
      " 0.0785451  0.66768345 0.0785451  0.0785451  0.66768345 0.0785451\n",
      " 0.0785451  0.0785451  0.19088699 0.0785451  0.0785451  0.0785451\n",
      " 0.0785451  0.0785451  0.78002534 0.66768345 0.19088699 0.0785451\n",
      " 0.66768345 0.0785451  0.19088699 0.0785451  0.66768345 0.19088699\n",
      " 0.19088699 0.0785451  0.19088699 0.19088699 0.0785451  0.19088699\n",
      " 0.66768345 0.19088699 0.0785451  0.19088699 0.0785451  0.0785451\n",
      " 0.66768345 0.0785451  0.66768345 0.66768345 0.66768345 0.0785451\n",
      " 0.0785451  0.78002534 0.0785451  0.0785451 ]\n",
      "all_sum is: [0.         0.40222555 0.         0.         0.         0.40222555\n",
      " 0.40222555 0.         0.         0.         0.40222555 0.58131772\n",
      " 0.58131772 0.40222555 0.98354327 0.58131772 0.98354327 0.58131772\n",
      " 0.         0.58131772 0.98354327 0.58131772 0.58131772 0.\n",
      " 0.58131772 0.         0.40222555 0.40222555 0.58131772 0.58131772\n",
      " 0.         0.40222555 0.40222555 0.         0.         0.\n",
      " 0.58131772 0.58131772 0.         0.98354327 0.         0.58131772\n",
      " 0.         0.58131772 0.40222555 0.         0.40222555 0.\n",
      " 0.40222555 0.58131772 0.98354327 0.         0.58131772 0.58131772\n",
      " 0.40222555 0.98354327 0.58131772 0.         0.58131772 0.58131772\n",
      " 0.58131772 0.58131772 0.58131772 0.         0.         0.40222555\n",
      " 0.98354327 0.58131772 0.40222555 0.58131772 0.58131772 0.\n",
      " 0.         0.98354327 0.         0.40222555 0.58131772 0.\n",
      " 0.58131772 0.40222555 0.98354327 0.         0.         0.98354327\n",
      " 0.58131772 0.         0.40222555 0.98354327 0.58131772 0.40222555\n",
      " 0.40222555 0.40222555 0.         0.58131772 0.40222555 0.\n",
      " 0.58131772 0.         0.58131772 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.168\n",
      "(*) epoch 2, cost 1.683\n",
      "(*) epoch 3, cost 1.416\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.172\n",
      "(*) epoch 2, cost 1.853\n",
      "(*) epoch 3, cost 1.656\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.0837071  1.0837071  1.62787508 1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.62787508 1.0837071  1.0837071  2.2720433\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071  1.0837071  1.0837071\n",
      " 1.0837071  1.0837071  1.0837071  1.0837071 ]\n",
      "all_sum is: [4.04742581 1.92632269 4.04742581 4.04742581 1.92632269 4.04742581\n",
      " 4.04742581 4.04742581 4.04742581 1.92632269 1.92632269 1.92632269\n",
      " 4.04742581 1.92632269 4.04742581 4.04742581 4.04742581 1.92632269\n",
      " 1.92632269 4.04742581 1.92632269 4.2198426  4.04742581 1.92632269\n",
      " 1.92632269 4.04742581 1.92632269 4.04742581 1.92632269 4.04742581\n",
      " 1.92632269 4.04742581 1.92632269 1.92632269 4.04742581 1.92632269\n",
      " 1.92632269 4.04742581 4.04742581 1.92632269 1.92632269 4.04742581\n",
      " 1.92632269 4.04742581 1.92632269 2.09873948 4.04742581 1.92632269\n",
      " 4.04742581 4.04742581 1.92632269 4.04742581 4.04742581 1.92632269\n",
      " 4.04742581 1.92632269 4.04742581 4.04742581 1.92632269 1.92632269\n",
      " 4.04742581 4.04742581 4.04742581 1.92632269 1.92632269 1.92632269\n",
      " 1.92632269 4.04742581 1.92632269 1.92632269 1.92632269 1.92632269\n",
      " 4.04742581 4.04742581 4.04742581 1.92632269 1.92632269 1.92632269\n",
      " 4.04742581 4.04742581 4.04742581 4.04742581 1.92632269 1.92632269\n",
      " 1.92632269 4.2198426  1.92632269 4.04742581 4.04742581 4.04742581\n",
      " 4.04742581 4.04742581 4.04742581 4.04742581 1.92632269 4.04742581\n",
      " 4.04742581 1.92632269 4.04742581 1.92632269]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.427\n",
      "(*) epoch 2, cost 0.953\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.344\n",
      "(*) epoch 2, cost 0.857\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.41164195 0.41164195 0.41164195 0.41164195 1.06331628\n",
      " 0.41164195 0.41164195 0.41164195 0.73653173 0.         1.09089983\n",
      " 0.         0.41164195 0.         0.41164195 0.41164195 0.41164195\n",
      " 0.41164195 0.41164195 0.41164195 0.41164195 0.41164195 0.41164195\n",
      " 0.32488978 0.41164195 0.41164195 1.79984801 0.         0.41164195\n",
      " 0.41164195 1.47495823 0.         0.41164195 0.41164195 0.41164195\n",
      " 1.47495823 0.41164195 0.73653173 0.41164195 1.79984801 1.47495823\n",
      " 0.32488978 0.41164195 0.41164195 0.73653173 1.09089983 0.41164195\n",
      " 0.41164195 1.47495823 0.41164195 0.41164195 0.41164195 0.73653173\n",
      " 0.41164195 1.38820606 0.41164195 0.41164195 0.41164195 0.41164195\n",
      " 0.         1.38820606 1.79984801 0.41164195 0.41164195 0.41164195\n",
      " 0.76601005 1.47495823 1.47495823 0.         0.         0.41164195\n",
      " 0.41164195 0.         1.38820606 0.41164195 0.41164195 0.41164195\n",
      " 0.41164195 0.         0.73653173 0.41164195 0.73653173 0.\n",
      " 0.73653173 0.         0.41164195 0.41164195 0.41164195 0.41164195\n",
      " 0.41164195 0.73653173 0.41164195 0.73653173 0.73653173 0.41164195\n",
      " 0.41164195 0.41164195 0.41164195 0.41164195]\n",
      "all_sum is: [6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 4.54184809 4.54184809 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 4.54184809 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 4.54184809 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 4.54184809 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 4.54184809 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 7.00620089 6.60807969 6.60807969\n",
      " 6.60807969 6.60807969 6.60807969 6.60807969]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.488\n",
      "(*) epoch 2, cost 1.916\n",
      "(*) epoch 3, cost 1.488\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.053\n",
      "(*) epoch 2, cost 1.301\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.40400606 1.56081247 1.56081247 1.40400606 1.40400606 1.40400606\n",
      " 1.40400606 1.40400606 1.40400606 1.40400606 1.40400606 1.56081247\n",
      " 1.40400606 0.89513419 1.40400606 1.40400606 1.56081247 1.56081247\n",
      " 1.56081247 1.56081247 1.56081247 1.56081247 1.56081247 1.40400606\n",
      " 1.40400606 1.56081247 1.40400606 1.56081247 1.56081247 1.40400606\n",
      " 1.05194061 1.40400606 1.40400606 1.40400606 1.56081247 1.40400606\n",
      " 1.40400606 1.56081247 1.40400606 1.40400606 0.89513419 1.40400606\n",
      " 1.56081247 0.89513419 1.56081247 1.05194061 0.89513419 1.40400606\n",
      " 1.56081247 1.56081247 1.05194061 1.56081247 1.05194061 1.40400606\n",
      " 1.40400606 1.40400606 1.05194061 1.05194061 1.40400606 1.40400606\n",
      " 1.56081247 1.40400606 1.40400606 1.40400606 1.56081247 1.56081247\n",
      " 1.40400606 1.56081247 1.56081247 1.40400606 0.89513419 1.56081247\n",
      " 1.40400606 1.40400606 1.40400606 0.89513419 1.56081247 1.56081247\n",
      " 0.89513419 1.05194061 1.56081247 1.56081247 1.56081247 1.56081247\n",
      " 1.40400606 1.56081247 1.56081247 1.40400606 1.56081247 1.56081247\n",
      " 1.56081247 1.40400606 1.40400606 1.40400606 1.40400606 1.40400606\n",
      " 1.56081247 1.40400606 1.56081247 1.56081247]\n",
      "all_sum is: [1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.87470418 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  3.01552218\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.87470418 1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.87470418 1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871  1.8262871  1.8262871\n",
      " 1.8262871  1.8262871  1.8262871  1.8262871 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.207\n",
      "(*) epoch 2, cost 1.964\n",
      "(*) epoch 3, cost 1.753\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.865\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.57947382 1.57947382 1.57947382 1.57947382 2.47567342 1.57947382\n",
      " 1.57947382 1.57947382 2.47567342 1.57947382 1.57947382 1.57947382\n",
      " 2.47567342 1.57947382 1.57947382 1.57947382 2.47567342 2.47567342\n",
      " 1.57947382 1.57947382 1.57947382 1.57947382 2.47567342 1.57947382\n",
      " 1.57947382 1.57947382 1.57947382 1.57947382 1.57947382 2.24799989\n",
      " 1.57947382 1.57947382 2.47567342 1.57947382 1.57947382 1.57947382\n",
      " 1.57947382 1.57947382 1.57947382 2.47567342 1.57947382 2.47567342\n",
      " 1.57947382 2.47567342 1.57947382 2.47567342 2.47567342 2.47567342\n",
      " 1.57947382 1.57947382 2.47567342 1.57947382 1.57947382 1.57947382\n",
      " 1.57947382 2.47567342 2.47567342 1.57947382 2.47567342 2.47567342\n",
      " 2.47567342 1.57947382 1.57947382 2.24799989 1.57947382 1.57947382\n",
      " 1.57947382 1.57947382 2.47567342 1.57947382 2.47567342 1.57947382\n",
      " 1.57947382 2.47567342 1.57947382 1.57947382 1.57947382 2.47567342\n",
      " 1.57947382 1.57947382 1.57947382 1.57947382 1.57947382 1.57947382\n",
      " 1.57947382 1.57947382 2.47567342 2.47567342 1.57947382 2.47567342\n",
      " 1.57947382 1.57947382 1.57947382 1.57947382 2.47567342 1.57947382\n",
      " 1.35180029 2.47567342 1.57947382 1.57947382]\n",
      "all_sum is: [1.93223493 1.93223493 1.93223493 1.93223493 1.93223493 3.16048248\n",
      " 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.93223493 1.90290382 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.93223493 1.62106663 1.93223493 1.93223493 1.93223493 1.62106663\n",
      " 1.93223493 1.93223493 1.59173552 1.90290382 1.90290382 1.93223493\n",
      " 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493 1.84489524\n",
      " 1.93223493 1.93223493 1.93223493 1.00554625 3.13115137 1.90290382\n",
      " 1.93223493 3.13115137 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.90290382 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.62106663 1.93223493 1.62106663 1.93223493 1.93223493 1.93223493\n",
      " 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493 1.93223493\n",
      " 1.93223493 1.93223493 1.93223493 1.93223493 1.90290382 1.93223493\n",
      " 1.93223493 1.93223493 1.62106663 1.00554625 1.93223493 2.84931417\n",
      " 1.93223493 1.93223493 1.93223493 1.90290382 1.93223493 1.93223493\n",
      " 1.93223493 1.90290382 3.13115137 1.93223493 1.62106663 1.93223493\n",
      " 1.93223493 1.93223493 1.62106663 1.90290382]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.593\n",
      "(*) epoch 2, cost 1.775\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.033\n",
      "(*) epoch 2, cost 4.086\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 1.59907356\n",
      " 0.88346872 2.36604751 0.88346872 0.88346872 2.36604751 0.88346872\n",
      " 1.59907356 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 1.59907356 0.88346872\n",
      " 1.59907356 0.88346872 0.88346872 0.88346872 0.88346872 1.59907356\n",
      " 0.88346872 1.59907356 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 1.59907356 1.59907356 0.88346872 0.88346872 0.88346872\n",
      " 1.59907356 0.88346872 0.88346872 1.59907356 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 1.59907356\n",
      " 0.88346872 2.36604751 2.86289357 2.86289357 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 1.59907356 0.88346872 0.88346872\n",
      " 0.88346872 1.59907356 0.88346872 1.59907356 0.88346872 0.88346872\n",
      " 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872 0.88346872\n",
      " 0.88346872 1.59907356 0.88346872 0.88346872]\n",
      "all_sum is: [0.10520903 0.64129905 0.64129905 0.10520903 0.10520903 0.\n",
      " 0.64129905 0.64129905 0.53609002 0.         0.10520903 0.10520903\n",
      " 0.10520903 0.10520903 0.10520903 0.10520903 0.10520903 0.\n",
      " 0.53609002 0.10520903 0.10520903 0.64129905 0.10520903 0.64129905\n",
      " 0.10520903 0.64129905 0.10520903 0.53609002 0.10520903 0.64129905\n",
      " 0.64129905 0.10520903 0.64129905 0.10520903 0.         0.10520903\n",
      " 0.10520903 0.64129905 0.64129905 0.         0.53609002 0.10520903\n",
      " 0.10520903 0.64129905 0.10520903 0.10520903 0.10520903 0.10520903\n",
      " 0.53609002 0.10520903 0.64129905 0.64129905 0.10520903 0.10520903\n",
      " 0.10520903 0.53609002 0.10520903 0.10520903 0.10520903 0.10520903\n",
      " 0.10520903 0.53609002 0.64129905 0.         0.64129905 0.10520903\n",
      " 0.         0.64129905 0.53609002 0.         0.53609002 0.64129905\n",
      " 0.10520903 0.64129905 0.64129905 0.53609002 0.         0.10520903\n",
      " 0.10520903 0.64129905 0.10520903 0.         0.10520903 0.53609002\n",
      " 0.10520903 0.10520903 0.10520903 0.10520903 0.10520903 0.64129905\n",
      " 0.64129905 0.10520903 0.10520903 0.64129905 0.10520903 0.64129905\n",
      " 0.         0.10520903 0.         0.10520903]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.934\n",
      "(*) epoch 2, cost 0.693\n",
      "(*) epoch 3, cost 0.521\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.325\n",
      "(*) epoch 2, cost 2.619\n",
      "(*) epoch 3, cost 2.074\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.10886996 1.10886996 1.10886996 3.01718345 1.10886996 3.01718345\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 3.01718345\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 2.29490344\n",
      " 1.10886996 3.01718345 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 3.01718345 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 3.01718345 3.01718345 1.10886996 1.10886996 1.10886996\n",
      " 3.01718345 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 3.01718345\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996 1.10886996\n",
      " 1.10886996 1.10886996 1.10886996 1.10886996 3.01718345 1.10886996\n",
      " 1.10886996 1.10886996 3.01718345 1.10886996 1.10886996 2.29490344\n",
      " 1.10886996 1.10886996 1.10886996 4.20321692 1.10886996 1.10886996\n",
      " 1.10886996 3.01718345 3.01718345 1.10886996]\n",
      "all_sum is: [5.20225924 4.64100202 4.64100202 5.20225924 2.1785113  5.30443011\n",
      " 5.20225924 4.64100202 1.51508321 5.20225924 4.64100202 5.20225924\n",
      " 5.20225924 5.20225924 4.64100202 4.64100202 2.07634043 5.20225924\n",
      " 1.51508321 4.64100202 1.51508321 5.20225924 4.64100202 4.7431729\n",
      " 5.20225924 5.20225924 5.20225924 4.64100202 6.80178521 1.51508321\n",
      " 6.34269887 5.20225924 4.64100202 3.11460918 5.20225924 4.64100202\n",
      " 5.20225924 4.7431729  1.61725408 4.7431729  4.7431729  4.64100202\n",
      " 4.7431729  5.30443011 4.64100202 2.07634043 4.64100202 5.20225924\n",
      " 4.64100202 5.30443011 5.20225924 5.20225924 5.20225924 5.20225924\n",
      " 5.20225924 6.240528   5.30443011 2.07634043 4.64100202 5.20225924\n",
      " 5.20225924 2.07634043 2.07634043 4.64100202 4.64100202 4.64100202\n",
      " 4.04905423 1.51508321 4.04905423 4.64100202 4.64100202 4.7431729\n",
      " 4.64100202 1.51508321 4.64100202 5.20225924 2.07634043 4.7431729\n",
      " 5.20225924 2.07634043 5.20225924 4.64100202 1.51508321 5.20225924\n",
      " 4.64100202 6.240528   4.64100202 4.64100202 6.240528   5.20225924\n",
      " 4.7431729  3.38562614 4.7431729  4.64100202 4.64100202 2.07634043\n",
      " 5.20225924 5.20225924 4.64100202 3.38562614]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.299\n",
      "(*) epoch 2, cost 0.511\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.072\n",
      "(*) epoch 2, cost 4.198\n",
      "(*) epoch 3, cost 3.849\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.17830165 3.7676844  4.17830165 3.7676844  3.7676844  4.17830165\n",
      " 4.17830165 4.17830165 4.17830165 4.17830165 3.7676844  4.17830165\n",
      " 4.17830165 4.17830165 3.7676844  3.7676844  4.17830165 4.17830165\n",
      " 3.7676844  4.17830165 4.17830165 3.7676844  4.17830165 4.17830165\n",
      " 3.7676844  4.17830165 3.7676844  4.17830165 4.17830165 4.17830165\n",
      " 4.17830165 4.17830165 4.17830165 3.7676844  3.7676844  3.7676844\n",
      " 4.17830165 4.17830165 4.17830165 4.17830165 4.17830165 4.17830165\n",
      " 3.7676844  3.7676844  4.17830165 4.17830165 4.17830165 3.7676844\n",
      " 3.7676844  4.17830165 4.17830165 3.7676844  4.17830165 4.17830165\n",
      " 3.7676844  4.17830165 3.7676844  4.17830165 3.7676844  4.17830165\n",
      " 4.17830165 4.17830165 4.17830165 3.7676844  3.7676844  4.17830165\n",
      " 4.17830165 4.17830165 4.17830165 4.17830165 3.7676844  4.17830165\n",
      " 3.7676844  3.7676844  3.7676844  3.7676844  3.7676844  4.17830165\n",
      " 4.17830165 3.7676844  3.7676844  3.7676844  4.17830165 4.17830165\n",
      " 4.17830165 4.17830165 4.17830165 4.17830165 3.7676844  4.17830165\n",
      " 3.7676844  4.17830165 4.17830165 4.17830165 3.7676844  4.17830165\n",
      " 4.17830165 4.17830165 3.7676844  3.7676844 ]\n",
      "all_sum is: [3.13285132 3.13285132 3.13285132 3.13285132 2.78522984 3.13285132\n",
      " 3.13285132 2.78522984 3.13285132 2.78522984 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 2.78522984 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 2.78522984 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 2.78522984 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 2.78522984 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 2.78522984 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 2.78522984 3.13285132 3.13285132 3.13285132\n",
      " 4.95270154 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 2.78522984 3.13285132\n",
      " 4.60508006 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132 3.13285132\n",
      " 3.13285132 3.13285132 3.13285132 3.13285132]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.355\n",
      "(*) epoch 2, cost 2.730\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.640\n",
      "(*) epoch 2, cost 2.014\n",
      "(*) epoch 3, cost 1.700\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.08705143 3.83891964 3.83891964 4.08705143 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 4.08705143\n",
      " 3.83891964 3.83891964 3.83891964 4.08705143 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 4.08705143 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 4.08705143 3.83891964 3.83891964 4.08705143 3.14308881\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 4.08705143 3.83891964\n",
      " 3.83891964 4.08705143 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 4.08705143 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.98281873 3.83891964 3.83891964\n",
      " 3.83891964 3.83891964 3.83891964 3.83891964]\n",
      "all_sum is: [1.89471806 1.89471806 2.13493197 0.24021391 1.89471806 3.1576753\n",
      " 0.24021391 3.1576753  3.1576753  2.07497788 1.50317116 3.39788921\n",
      " 0.24021391 2.13493197 1.89471806 0.         1.89471806 1.89471806\n",
      " 1.26295724 2.13493197 2.13493197 1.89471806 3.39788921 1.89471806\n",
      " 3.33793512 1.26295724 3.39788921 1.26295724 3.1576753  1.89471806\n",
      " 1.89471806 3.39788921 2.13493197 3.1576753  3.1576753  3.1576753\n",
      " 1.89471806 1.26295724 1.89471806 3.1576753  3.33793512 1.89471806\n",
      " 2.13493197 0.         3.1576753  3.39788921 2.31519179 3.1576753\n",
      " 0.         0.18025982 2.13493197 1.89471806 1.89471806 3.57814904\n",
      " 3.1576753  3.1576753  3.39788921 3.1576753  3.1576753  3.1576753\n",
      " 3.57814904 1.89471806 2.13493197 1.89471806 3.1576753  3.1576753\n",
      " 3.39788921 1.89471806 2.13493197 1.50317116 2.07497788 0.24021391\n",
      " 3.1576753  1.26295724 2.13493197 2.13493197 3.1576753  1.44321706\n",
      " 2.07497788 1.89471806 0.         2.13493197 2.13493197 3.39788921\n",
      " 2.07497788 3.1576753  3.39788921 3.1576753  0.18025982 1.89471806\n",
      " 1.26295724 1.89471806 1.50317116 3.39788921 0.         1.89471806\n",
      " 3.39788921 1.89471806 1.89471806 3.1576753 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.503\n",
      "(*) epoch 2, cost 0.563\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.171\n",
      "(*) epoch 2, cost 2.408\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.75246283 1.75246283 1.75246283 1.75246283 1.97145161 1.97145161\n",
      " 1.97145161 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 2.21373676 1.75246283 1.75246283\n",
      " 1.97145161 1.97145161 1.97145161 1.97145161 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283 1.97145161\n",
      " 1.75246283 1.75246283 1.97145161 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.97145161 1.75246283 1.97145161 1.75246283 1.97145161\n",
      " 1.97145161 1.75246283 1.75246283 1.75246283 1.75246283 1.97145161\n",
      " 1.97145161 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 2.21373676 1.97145161 1.75246283 1.75246283 2.21373676\n",
      " 1.97145161 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.97145161 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283 1.75246283\n",
      " 1.97145161 1.75246283 1.75246283 1.75246283]\n",
      "all_sum is: [0.13501151 0.13501151 0.13501151 0.13501151 0.96289161 0.13501151\n",
      " 0.96289161 0.13501151 0.96289161 0.13501151 0.13501151 0.13501151\n",
      " 0.96289161 0.13501151 0.13501151 0.13501151 0.96289161 0.96289161\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.96289161 0.13501151\n",
      " 0.13501151 0.13501151 0.96289161 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.96289161 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.96289161 0.13501151 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.96289161 0.13501151 0.13501151 0.96289161\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.96289161 0.13501151\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.13501151 0.96289161 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151\n",
      " 0.13501151 0.96289161 0.13501151 0.13501151 0.13501151 0.96289161\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151 0.13501151 0.96289161\n",
      " 0.13501151 0.13501151 0.13501151 0.13501151]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.247\n",
      "(*) epoch 2, cost 1.750\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.864\n",
      "(*) epoch 2, cost 1.694\n",
      "(*) epoch 3, cost 1.402\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [ 9.870817   10.69692319  7.50790729  9.870817   10.69692319  9.870817\n",
      "  9.55031227  9.870817    6.36129637  6.36129637  8.72420608  7.50790729\n",
      "  7.51378956 10.69692319  7.50790729  9.15436953  7.50790729  9.870817\n",
      "  7.50790729  7.50790729  8.72420608  8.33401349 10.69692319  7.50790729\n",
      " 10.69692319  9.870817    7.50790729  8.33401349 10.69692319 10.69692319\n",
      "  6.36129637  8.76417694  7.50790729  8.33401349  7.50790729 10.69692319\n",
      "  8.72420608 10.69692319  9.15436953 10.69692319 10.69692319  9.870817\n",
      "  7.50790729  8.33401349  9.870817   10.69692319  8.33401349  8.33401349\n",
      "  7.50790729 10.69692319  6.36129637 10.69692319  6.36129637  8.33401349\n",
      "  9.55031227  8.33401349  8.33401349 10.69692319  9.870817    8.33401349\n",
      " 10.69692319  6.36129637  8.33401349  8.33401349  8.33401349  7.18740257\n",
      "  8.33401349  9.870817   11.12708665  9.870817    8.33401349  7.18740257\n",
      " 10.69692319 10.69692319 10.30098046 10.69692319  7.50790729  9.55031227\n",
      "  8.72420608  8.33401349  8.33401349  6.36129637  6.36129637  7.93807075\n",
      "  8.33401349  6.36129637 10.69692319  7.50790729 10.69692319  8.33401349\n",
      "  7.50790729  9.55031227  7.50790729  8.33401349  7.93807075  8.33401349\n",
      "  6.36129637  9.870817    8.33401349  9.870817  ]\n",
      "all_sum is: [2.50826175 2.50826175 2.50826175 2.50826175 2.50826175 2.50826175\n",
      " 2.50826175 2.50826175 2.50826175 2.50826175 0.78659296 2.50826175\n",
      " 2.50826175 2.50826175 2.50826175 0.78659296 0.78659296 2.50826175\n",
      " 0.78659296 2.50826175 0.78659296 2.50826175 2.50826175 2.50826175\n",
      " 2.50826175 2.50826175 2.50826175 2.50826175 2.50826175 2.50826175\n",
      " 2.50826175 2.50826175 2.50826175 2.50826175 2.50826175 0.78659296\n",
      " 2.50826175 0.78659296 2.50826175 2.50826175 2.50826175 2.50826175\n",
      " 2.50826175 2.50826175 0.78659296 2.50826175 2.50826175 2.50826175\n",
      " 0.78659296 2.50826175 0.78659296 2.50826175 2.50826175 2.50826175\n",
      " 0.78659296 2.50826175 2.50826175 0.78659296 0.78659296 2.50826175\n",
      " 0.78659296 2.50826175 2.50826175 2.50826175 2.50826175 0.78659296\n",
      " 2.50826175 2.50826175 2.50826175 2.50826175 2.50826175 0.78659296\n",
      " 2.50826175 2.50826175 2.50826175 0.78659296 2.50826175 0.78659296\n",
      " 2.50826175 2.50826175 0.78659296 2.50826175 2.50826175 2.50826175\n",
      " 2.50826175 2.50826175 2.50826175 0.78659296 0.78659296 0.78659296\n",
      " 0.78659296 2.50826175 2.50826175 0.78659296 2.50826175 2.50826175\n",
      " 0.78659296 0.78659296 2.50826175 2.50826175]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 12\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.603\n",
      "(*) epoch 2, cost 6.232\n",
      "(*) epoch 3, cost 5.931\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.737\n",
      "(*) epoch 2, cost 3.089\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.92449217 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.94382878 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 2.70947001 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.94382878 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.94382878 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.94382878 1.92449217 1.92449217 1.92449217 1.92449217 2.70947001\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217 1.94382878 1.92449217\n",
      " 1.92449217 1.92449217 1.94382878 1.94382878 1.94382878 2.70947001\n",
      " 1.94382878 1.94382878 2.72880662 1.92449217 2.70947001 1.92449217\n",
      " 2.70947001 1.92449217 1.94382878 1.92449217 2.70947001 1.92449217\n",
      " 1.92449217 2.70947001 1.92449217 1.92449217 1.92449217 1.92449217\n",
      " 2.70947001 1.92449217 1.92449217 1.92449217 1.92449217 1.94382878\n",
      " 1.92449217 1.92449217 1.92449217 1.92449217]\n",
      "all_sum is: [0.28842025 0.28842025 1.31420893 0.28842025 0.28842025 0.28842025\n",
      " 0.28842025 0.         0.28842025 0.28842025 0.28842025 0.28842025\n",
      " 0.         0.28842025 0.         0.28842025 0.28842025 0.28842025\n",
      " 0.28842025 0.         0.28842025 1.70814228 0.28842025 0.28842025\n",
      " 1.11720564 1.11720564 1.11720564 0.         0.28842025 0.\n",
      " 0.28842025 0.28842025 0.         0.28842025 0.         0.28842025\n",
      " 0.28842025 1.16777715 1.11720564 0.28842025 1.11720564 0.28842025\n",
      " 0.28842025 1.16777715 1.26363742 0.72327228 0.28842025 0.87935689\n",
      " 0.         0.28842025 0.28842025 1.16777715 0.72327228 0.28842025\n",
      " 0.         1.11720564 0.28842025 0.28842025 0.         0.\n",
      " 0.28842025 0.28842025 0.28842025 0.28842025 0.28842025 0.\n",
      " 0.         0.98694994 0.28842025 1.16777715 1.11720564 1.55205767\n",
      " 0.28842025 1.11720564 1.11720564 1.99656253 0.28842025 1.11720564\n",
      " 1.55205767 0.72327228 0.         0.72327228 0.28842025 0.28842025\n",
      " 0.28842025 1.16777715 0.         0.28842025 1.16777715 0.28842025\n",
      " 0.82878539 0.28842025 0.28842025 1.11720564 0.28842025 0.\n",
      " 0.72327228 0.         0.87935689 0.28842025]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.438\n",
      "(*) epoch 2, cost 1.482\n",
      "(*) epoch 3, cost 1.219\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.074\n",
      "(*) epoch 2, cost 2.675\n",
      "(*) epoch 3, cost 2.479\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.69212589 1.70855531 1.35271667 2.04484255 0.69212589 0.\n",
      " 2.04484255 0.69212589 2.04484255 0.69212589 1.35271667 2.04484255\n",
      " 1.35271667 2.04484255 1.35271667 2.4006812  1.35271667 1.35271667\n",
      " 2.04484255 2.4006812  0.69212589 0.69212589 2.04484255 1.35271667\n",
      " 1.35271667 0.69212589 0.69212589 0.         2.04484255 2.04484255\n",
      " 0.69212589 1.35271667 2.04484255 2.04484255 2.04484255 0.69212589\n",
      " 2.04484255 0.69212589 2.04484255 1.35271667 1.35271667 0.\n",
      " 2.04484255 0.69212589 2.04484255 2.04484255 2.04484255 2.04484255\n",
      " 1.70855531 1.35271667 1.35271667 2.4006812  2.04484255 2.04484255\n",
      " 0.         2.04484255 2.04484255 2.4006812  2.04484255 2.04484255\n",
      " 0.         0.         2.04484255 1.35271667 2.04484255 0.69212589\n",
      " 1.04796453 2.04484255 0.         0.69212589 1.35271667 2.04484255\n",
      " 0.69212589 2.04484255 0.69212589 0.69212589 1.35271667 2.04484255\n",
      " 2.04484255 0.69212589 2.04484255 1.04796453 1.35271667 2.04484255\n",
      " 1.35271667 2.04484255 2.04484255 0.         2.04484255 1.35271667\n",
      " 0.69212589 0.69212589 0.         2.04484255 0.69212589 1.35271667\n",
      " 2.04484255 0.         2.04484255 1.35271667]\n",
      "all_sum is: [3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 2.81802126 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 2.81802126 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.97849862 3.70415724 3.97849862\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.97849862 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 2.81802126 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 2.81802126 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 2.81802126 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.97849862 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.97849862 3.70415724 3.70415724\n",
      " 3.70415724 3.70415724 3.70415724 3.97849862]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.496\n",
      "(*) epoch 2, cost 2.016\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.801\n",
      "(*) epoch 2, cost 1.944\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.25851406 1.47584238 1.47584238 1.37681739 1.47584238 0.5941457\n",
      " 1.47584238 1.37681739 0.5941457  2.25851406 1.47584238 0.5941457\n",
      " 0.5941457  0.5941457  0.5941457  0.5941457  0.5941457  0.5941457\n",
      " 1.47584238 1.37681739 0.5941457  2.60268316 0.5941457  1.37681739\n",
      " 1.37681739 0.5941457  0.5941457  0.5941457  1.47584238 1.37681739\n",
      " 0.5941457  1.37681739 1.37681739 0.5941457  1.47584238 1.37681739\n",
      " 1.37681739 1.37681739 1.47584238 1.37681739 0.5941457  1.37681739\n",
      " 0.5941457  0.5941457  2.60268316 2.25851406 1.47584238 0.5941457\n",
      " 1.37681739 1.47584238 0.5941457  0.5941457  1.47584238 0.5941457\n",
      " 0.5941457  0.5941457  1.47584238 0.5941457  1.47584238 1.37681739\n",
      " 1.37681739 1.37681739 0.5941457  0.5941457  0.5941457  0.5941457\n",
      " 0.5941457  0.5941457  2.25851406 0.5941457  0.5941457  0.5941457\n",
      " 1.47584238 1.37681739 0.5941457  1.37681739 0.5941457  0.5941457\n",
      " 0.5941457  2.25851406 0.5941457  0.5941457  0.5941457  1.37681739\n",
      " 1.47584238 0.5941457  0.5941457  0.5941457  0.5941457  0.5941457\n",
      " 1.47584238 1.37681739 2.25851406 0.5941457  0.5941457  1.47584238\n",
      " 0.5941457  0.5941457  1.37681739 0.5941457 ]\n",
      "all_sum is: [2.35099349 2.37400313 2.37400313 2.37400313 2.35099349 2.35099349\n",
      " 2.37400313 2.35099349 2.35099349 2.35099349 2.37400313 2.35099349\n",
      " 2.37400313 2.35099349 2.35099349 2.35099349 2.37400313 2.35099349\n",
      " 2.35099349 2.35099349 2.35099349 2.35099349 2.35099349 2.35099349\n",
      " 2.37400313 2.37400313 2.35099349 2.35099349 2.97600798 2.35099349\n",
      " 2.35099349 2.35099349 2.35099349 2.37400313 2.35099349 2.35099349\n",
      " 2.35099349 2.37400313 2.37400313 2.35099349 2.35099349 2.37400313\n",
      " 2.35099349 2.37400313 2.35099349 2.35099349 0.65730771 2.35099349\n",
      " 2.35099349 2.35099349 2.35099349 2.35099349 2.37400313 2.37400313\n",
      " 2.35099349 2.35099349 2.35099349 2.35099349 2.35099349 2.35099349\n",
      " 2.37400313 2.37400313 2.37400313 2.35099349 2.35099349 2.35099349\n",
      " 2.35099349 2.35099349 2.35099349 2.37400313 2.35099349 2.35099349\n",
      " 2.37400313 2.35099349 2.35099349 2.35099349 2.35099349 2.35099349\n",
      " 2.37400313 0.65730771 2.35099349 2.35099349 2.35099349 2.37400313\n",
      " 2.35099349 2.37400313 2.37400313 2.37400313 2.37400313 2.37400313\n",
      " 2.35099349 0.65730771 2.37400313 2.35099349 2.35099349 2.37400313\n",
      " 2.35099349 2.37400313 2.37400313 2.35099349]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.123\n",
      "(*) epoch 2, cost 0.649\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.564\n",
      "(*) epoch 2, cost 2.701\n",
      "(*) epoch 3, cost 2.503\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  3.053177\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  4.20174408\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  3.053177   5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  3.053177   5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431  5.0740431  5.0740431\n",
      " 5.0740431  5.0740431  5.0740431  5.0740431 ]\n",
      "all_sum is: [4.38087255 4.35747693 5.12408532 4.33763649 5.12408532 4.33763649\n",
      " 2.65408213 2.65408213 4.33763649 5.12408532 4.33763649 2.65408213\n",
      " 4.33763649 4.38087255 2.65408213 5.12408532 2.65408213 3.57102809\n",
      " 4.33763649 4.33763649 5.12408532 4.33763649 2.65408213 3.59442372\n",
      " 1.91086936 4.33763649 4.33763649 4.33763649 5.12408532 4.33763649\n",
      " 3.59442372 4.33763649 2.65408213 3.44053097 4.33763649 3.61426415\n",
      " 3.57102809 1.88747374 3.44053097 2.65408213 1.88747374 3.44053097\n",
      " 4.33763649 4.33763649 4.33763649 5.12408532 2.65408213 2.67392257\n",
      " 4.35747693 3.57102809 4.33763649 3.57102809 3.59442372 3.44053097\n",
      " 5.12408532 2.65408213 4.33763649 4.33763649 3.57102809 4.33763649\n",
      " 4.33763649 3.44053097 4.35747693 3.44053097 2.65408213 4.33763649\n",
      " 4.33763649 4.33763649 4.33763649 2.65408213 4.33763649 5.12408532\n",
      " 1.88747374 2.65408213 5.12408532 4.33763649 4.33763649 4.35747693\n",
      " 4.33763649 3.57102809 2.65408213 4.33763649 5.12408532 4.33763649\n",
      " 1.88747374 1.91086936 4.33763649 2.65408213 2.65408213 4.33763649\n",
      " 5.12408532 3.57102809 4.33763649 4.33763649 4.33763649 3.57102809\n",
      " 2.65408213 5.12408532 4.33763649 3.44053097]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.664\n",
      "(*) epoch 2, cost 0.758\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.768\n",
      "(*) epoch 2, cost 3.080\n",
      "(*) epoch 3, cost 2.693\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.18 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.16282424 1.16282424 1.16282424 0.         1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.83563579 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.83563579 1.16282424 1.16282424 1.16282424 1.83563579 1.16282424\n",
      " 1.83563579 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.83563579 1.16282424 1.16282424 1.16282424 1.83563579 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.83563579 1.83563579\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.83563579 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 0.         1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.83563579 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424 1.16282424\n",
      " 1.16282424 1.16282424 1.16282424 1.16282424 1.83563579 1.83563579\n",
      " 1.16282424 1.16282424 0.67281155 1.16282424]\n",
      "all_sum is: [2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 0.         2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 0.         2.80664938\n",
      " 2.80664938 0.         2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 0.         2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 0.         2.80664938 2.80664938 0.         2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 0.         2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 0.         2.80664938 2.80664938 2.80664938 2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938 0.         2.80664938\n",
      " 2.80664938 2.80664938 2.80664938 2.80664938]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.397\n",
      "(*) epoch 2, cost 2.152\n",
      "(*) epoch 3, cost 1.783\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.228\n",
      "(*) epoch 2, cost 0.962\n",
      "(*) epoch 3, cost 0.739\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.29 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639 1.21583639\n",
      " 1.21583639 1.21583639 1.21583639 1.21583639]\n",
      "all_sum is: [0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.15424141 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.15424141\n",
      " 0.05069366 0.05069366 0.05069366 0.15424141 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.15424141 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.15424141 0.15424141\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.15424141 0.05069366 0.05069366 0.15424141 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.15424141 0.05069366 0.05069366 0.05069366 0.05069366\n",
      " 0.05069366 0.05069366 0.05069366 0.15424141]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.431\n",
      "(*) epoch 2, cost 1.618\n",
      "(*) epoch 3, cost 1.191\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.18 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.062\n",
      "(*) epoch 2, cost 0.460\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.42868346 2.97194418 0.50854089 0.50854089 0.23668935 0.23668935\n",
      " 0.31654678 3.16393828 2.89208675 3.08408085 0.42868346 0.42868346\n",
      " 0.31654678 0.42868346 0.50854089 0.50854089 2.97194418 0.50854089\n",
      " 0.23668935 0.23668935 0.31654678 2.89208675 0.50854089 0.50854089\n",
      " 0.23668935 0.23668935 2.89208675 3.08408085 0.23668935 0.23668935\n",
      " 0.31654678 0.42868346 2.89208675 2.89208675 0.50854089 2.97194418\n",
      " 2.89208675 0.42868346 0.31654678 0.23668935 0.50854089 3.08408085\n",
      " 0.31654678 0.23668935 0.23668935 0.42868346 0.23668935 2.89208675\n",
      " 3.08408085 0.23668935 0.42868346 0.31654678 2.97194418 0.31654678\n",
      " 2.97194418 0.31654678 0.31654678 2.97194418 0.23668935 3.08408085\n",
      " 0.42868346 0.50854089 0.31654678 3.16393828 3.08408085 2.89208675\n",
      " 2.97194418 0.23668935 3.08408085 3.16393828 0.50854089 0.50854089\n",
      " 2.89208675 3.16393828 3.08408085 0.31654678 0.50854089 0.42868346\n",
      " 0.50854089 0.42868346 0.31654678 0.31654678 2.89208675 0.42868346\n",
      " 2.97194418 3.08408085 0.42868346 0.31654678 2.89208675 2.97194418\n",
      " 0.42868346 0.23668935 0.31654678 0.50854089 2.97194418 0.42868346\n",
      " 0.50854089 0.50854089 0.50854089 2.97194418]\n",
      "all_sum is: [1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.87234006 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 2.59417247 1.76075076 1.76075076 1.76075076\n",
      " 1.87234006 1.76075076 1.76075076 1.76075076 1.76075076 1.87234006\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076 1.76075076\n",
      " 1.76075076 1.76075076 1.76075076 1.76075076]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.396\n",
      "(*) epoch 2, cost 2.011\n",
      "(*) epoch 3, cost 1.784\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.286\n",
      "(*) epoch 2, cost 0.442\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.97571236 0.97571236 0.97571236 0.95235073 0.50402918 0.97571236\n",
      " 0.48066754 0.95235073 0.95235073 0.95235073 0.48066754 0.97571236\n",
      " 0.95235073 0.95235073 0.95235073 0.95235073 0.95235073 0.95235073\n",
      " 0.95235073 0.97571236 0.95235073 0.97571236 0.95235073 0.95235073\n",
      " 0.95235073 0.48066754 0.48066754 0.95235073 0.95235073 0.95235073\n",
      " 0.48066754 0.95235073 0.95235073 0.95235073 0.97571236 0.95235073\n",
      " 0.97571236 0.95235073 0.97571236 0.95235073 0.97571236 0.95235073\n",
      " 0.95235073 0.95235073 0.50402918 0.95235073 0.95235073 0.97571236\n",
      " 0.50402918 0.95235073 0.97571236 0.95235073 0.95235073 0.97571236\n",
      " 0.95235073 0.95235073 0.95235073 0.95235073 0.95235073 0.95235073\n",
      " 0.95235073 0.95235073 0.97571236 0.95235073 0.97571236 3.5369702\n",
      " 0.48066754 0.95235073 0.95235073 0.95235073 0.95235073 0.95235073\n",
      " 0.97571236 0.95235073 0.95235073 0.95235073 0.50402918 0.48066754\n",
      " 0.95235073 0.95235073 0.95235073 0.97571236 0.97571236 0.95235073\n",
      " 0.97571236 0.48066754 0.95235073 0.95235073 0.95235073 0.95235073\n",
      " 0.50402918 0.95235073 0.97571236 0.95235073 0.50402918 0.97571236\n",
      " 0.95235073 0.95235073 0.48066754 0.48066754]\n",
      "all_sum is: [1.20736863 1.62451957 1.80411592 1.62451957 1.20736863 1.66366852\n",
      " 1.62451957 1.62451957 1.20736863 2.18211792 1.20736863 2.18211792\n",
      " 1.62451957 1.76496698 1.20736863 1.20736863 2.22126687 1.62451957\n",
      " 1.20736863 1.20736863 2.18211792 1.62451957 1.20736863 1.62451957\n",
      " 1.62451957 2.18211792 1.24651757 1.20736863 1.62451957 1.20736863\n",
      " 1.62451957 1.20736863 1.24651757 1.20736863 1.20736863 1.20736863\n",
      " 1.76496698 2.18211792 1.62451957 1.20736863 1.62451957 1.76496698\n",
      " 1.62451957 2.18211792 2.18211792 1.20736863 1.20736863 1.62451957\n",
      " 1.20736863 1.20736863 2.18211792 2.18211792 1.62451957 1.62451957\n",
      " 1.76496698 2.18211792 1.20736863 1.76496698 1.62451957 1.62451957\n",
      " 1.20736863 1.76496698 1.24651757 1.20736863 1.62451957 1.62451957\n",
      " 2.12224415 1.62451957 1.20736863 1.20736863 1.76496698 1.20736863\n",
      " 1.20736863 1.62451957 1.62451957 1.20736863 1.62451957 1.62451957\n",
      " 1.24651757 1.76496698 2.18211792 1.76496698 1.62451957 1.76496698\n",
      " 2.18211792 2.18211792 1.20736863 1.62451957 1.62451957 1.20736863\n",
      " 2.18211792 1.76496698 1.20736863 1.76496698 1.20736863 1.20736863\n",
      " 1.62451957 1.20736863 2.18211792 1.62451957]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.441\n",
      "(*) epoch 2, cost 2.237\n",
      "(*) epoch 3, cost 1.981\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.032\n",
      "(*) epoch 2, cost 1.536\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.43968222 0.43968222 0.43968222 0.43968222 0.43968222\n",
      " 0.43968222 1.69705824 0.33872805 1.25737602 0.         0.43968222\n",
      " 0.         0.         0.43968222 1.20468558 0.43968222 0.43968222\n",
      " 1.20468558 0.43968222 1.20468558 0.33872805 0.77841028 1.20468558\n",
      " 0.43968222 1.69705824 0.43968222 0.43968222 0.         0.43968222\n",
      " 0.43968222 0.76500336 1.20468558 0.43968222 0.43968222 0.43968222\n",
      " 0.76500336 0.77841028 0.43968222 1.20468558 0.43968222 1.26449198\n",
      " 0.43968222 0.43968222 0.         0.         1.36544615 0.77841028\n",
      " 0.43968222 0.43968222 0.43968222 0.43968222 2.80078965 0.43968222\n",
      " 0.77841028 0.43968222 0.77841028 0.43968222 0.43968222 0.\n",
      " 0.         0.43968222 0.77841028 1.20468558 0.43968222 0.43968222\n",
      " 0.         1.20468558 0.43968222 0.         1.36544615 0.77841028\n",
      " 0.43968222 0.43968222 2.9481433  1.20468558 1.20468558 0.\n",
      " 0.         0.43968222 0.43968222 2.03578629 0.43968222 0.43968222\n",
      " 0.         1.25737602 0.43968222 1.20468558 1.54341363 0.43968222\n",
      " 1.69705824 0.         1.20468558 0.43968222 0.76500336 1.20468558\n",
      " 0.77841028 1.20468558 0.         0.        ]\n",
      "all_sum is: [2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537 2.43377537\n",
      " 2.43377537 2.43377537 2.43377537 2.43377537]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.062\n",
      "(*) epoch 2, cost 2.350\n",
      "(*) epoch 3, cost 1.840\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.989\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.30816076 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [2.44436041 1.54149814 1.62900033 1.54149814 1.54149814 1.62900033\n",
      " 1.54149814 2.48895497 2.48895497 1.54149814 1.62900033 1.54149814\n",
      " 1.54149814 1.54149814 1.62900033 1.54149814 3.39181723 2.44436041\n",
      " 2.44436041 2.5318626  1.54149814 1.54149814 1.62900033 1.54149814\n",
      " 1.62900033 1.54149814 2.5318626  1.54149814 1.54149814 1.54149814\n",
      " 1.54149814 2.5318626  2.48895497 1.54149814 1.54149814 1.54149814\n",
      " 2.44436041 1.62900033 1.54149814 1.54149814 2.5318626  2.48895497\n",
      " 1.54149814 1.54149814 2.5318626  1.54149814 1.54149814 1.54149814\n",
      " 1.54149814 1.54149814 2.48895497 1.54149814 1.62900033 1.62900033\n",
      " 3.39181723 1.54149814 2.48895497 1.62900033 1.54149814 2.48895497\n",
      " 1.54149814 1.54149814 1.62900033 1.54149814 1.54149814 1.62900033\n",
      " 2.48895497 2.57645716 2.48895497 1.62900033 1.54149814 2.48895497\n",
      " 1.54149814 1.54149814 1.54149814 2.44436041 1.54149814 1.54149814\n",
      " 3.47931943 2.57645716 1.62900033 1.54149814 1.54149814 1.62900033\n",
      " 1.62900033 1.54149814 1.62900033 2.44436041 1.54149814 2.44436041\n",
      " 2.44436041 1.54149814 1.54149814 1.54149814 1.54149814 1.54149814\n",
      " 2.48895497 2.48895497 1.54149814 1.54149814]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.987\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.552\n",
      "(*) epoch 2, cost 2.789\n",
      "(*) epoch 3, cost 2.452\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.18986546 0.18986546 0.63282807 0.4101939  0.18986546 0.18986546\n",
      " 0.18986546 0.63282807 0.18986546 0.18986546 0.18986546 0.18986546\n",
      " 0.63282807 0.4101939  0.29819419 0.18986546 0.18986546 0.63282807\n",
      " 0.18986546 0.63282807 0.18986546 0.63282807 0.18986546 0.18986546\n",
      " 1.04974557 0.18986546 0.18986546 0.18986546 0.18986546 0.18986546\n",
      " 0.18986546 0.18986546 0.29819419 0.18986546 0.63282807 0.18986546\n",
      " 0.18986546 0.18986546 0.18986546 0.18986546 0.18986546 0.18986546\n",
      " 0.63282807 0.18986546 0.63282807 0.18986546 0.18986546 0.18986546\n",
      " 0.29819419 0.18986546 0.18986546 0.18986546 0.18986546 0.63282807\n",
      " 0.18986546 0.18986546 0.63282807 0.18986546 0.18986546 0.18986546\n",
      " 0.18986546 0.4101939  0.18986546 0.7411568  0.18986546 0.18986546\n",
      " 0.18986546 0.4101939  0.18986546 0.18986546 0.18986546 0.85315652\n",
      " 0.18986546 0.18986546 0.18986546 1.6010369  1.04974557 0.29819419\n",
      " 0.4101939  0.63282807 0.18986546 0.63282807 0.18986546 0.18986546\n",
      " 0.18986546 0.29819419 0.18986546 0.18986546 0.18986546 0.63282807\n",
      " 0.63282807 1.42132689 0.18986546 0.18986546 0.18986546 0.18986546\n",
      " 0.18986546 1.04974557 0.18986546 0.18986546]\n",
      "all_sum is: [0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.05930575\n",
      " 0.15315429 0.05930575 0.15315429 0.15315429 0.15315429 0.05930575\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 1.57454604 0.15315429 0.57020508 0.15315429 0.15315429 0.15315429\n",
      " 0.05930575 0.57020508 0.15315429 0.15315429 2.22873601 2.22873601\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 1.57454604 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 2.22873601 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.05930575 0.15315429 0.15315429 0.15315429 0.15315429\n",
      " 0.15315429 0.15315429 0.15315429 0.15315429]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.868\n",
      "(*) epoch 2, cost 1.924\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.696\n",
      "(*) epoch 2, cost 2.721\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.60850554 1.43871001 0.60850554 0.60850554 0.60850554 1.43871001\n",
      " 0.60850554 0.60850554 0.60850554 0.60850554 1.43871001 1.43871001\n",
      " 0.60850554 1.43871001 0.60850554 1.43871001 1.43871001 1.43871001\n",
      " 1.43871001 1.43871001 0.60850554 0.60850554 0.60850554 0.60850554\n",
      " 1.43871001 0.60850554 1.43871001 0.60850554 0.60850554 0.60850554\n",
      " 0.60850554 1.43871001 1.43871001 0.60850554 0.60850554 1.43871001\n",
      " 1.43871001 0.60850554 0.60850554 0.60850554 0.60850554 0.60850554\n",
      " 0.60850554 0.60850554 1.43871001 0.60850554 0.60850554 0.60850554\n",
      " 1.43871001 0.60850554 1.43871001 0.60850554 1.43871001 1.43871001\n",
      " 0.60850554 1.43871001 0.60850554 0.60850554 1.43871001 1.43871001\n",
      " 0.60850554 0.60850554 1.43871001 0.60850554 0.60850554 1.43871001\n",
      " 1.43871001 1.43871001 1.43871001 0.60850554 0.60850554 0.60850554\n",
      " 1.43871001 0.60850554 0.60850554 0.60850554 1.43871001 0.60850554\n",
      " 1.43871001 1.43871001 1.43871001 1.43871001 1.43871001 0.60850554\n",
      " 1.43871001 1.43871001 1.43871001 0.60850554 0.60850554 1.43871001\n",
      " 1.43871001 0.60850554 1.43871001 0.60850554 1.43871001 1.43871001\n",
      " 1.43871001 0.60850554 1.43871001 0.60850554]\n",
      "all_sum is: [0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 1.16447148 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991 0.94028991\n",
      " 0.94028991 0.94028991 0.94028991 0.94028991]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.124\n",
      "(*) epoch 2, cost 1.939\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.024\n",
      "(*) epoch 2, cost 1.618\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.20018713 2.19167144 2.19167144 2.19167144 1.20018713 2.19167144\n",
      " 1.20018713 1.20018713 2.19167144 1.20018713 1.20018713 2.19167144\n",
      " 2.19167144 2.79234316 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 2.19167144 1.20018713 1.20018713 1.20018713 2.19167144\n",
      " 1.20018713 1.20018713 1.20018713 1.20018713 2.10411985 1.20018713\n",
      " 1.20018713 1.20018713 2.19167144 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 1.20018713 1.20018713 2.19167144 1.20018713 1.20018713\n",
      " 1.20018713 2.19167144 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 2.19167144 1.20018713 1.20018713 2.19167144 1.20018713 1.20018713\n",
      " 1.20018713 1.20018713 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 2.19167144 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 1.20018713 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 1.20018713 2.19167144 2.10411985 1.20018713 2.19167144\n",
      " 1.20018713 1.20018713 1.20018713 1.20018713 2.19167144 2.19167144\n",
      " 1.20018713 2.19167144 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 3.09560416 2.19167144 1.20018713 1.20018713 1.20018713 1.20018713\n",
      " 1.20018713 1.20018713 1.20018713 1.20018713]\n",
      "all_sum is: [2.59526318 2.59526318 2.59526318 2.59526318 3.08062194 2.59526318\n",
      " 2.59526318 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 3.08062194 2.59526318 2.59526318\n",
      " 3.08062194 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 3.08062194 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 3.08062194 2.59526318 2.59526318 2.59526318 3.08062194\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318 2.59526318\n",
      " 2.59526318 2.59526318 2.59526318 2.59526318]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.063\n",
      "(*) epoch 2, cost 1.929\n",
      "(*) epoch 3, cost 1.619\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.245\n",
      "(*) epoch 2, cost 1.582\n",
      "(*) epoch 3, cost 1.236\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.01869993 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.01869993 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.01869993 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [0.65437757 0.65437757 0.65437757 0.65437757 0.65794891 0.65437757\n",
      " 2.07244414 0.65437757 0.65437757 0.65437757 2.07244414 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 2.07244414\n",
      " 0.65437757 0.65437757 2.07244414 0.65437757 0.65437757 2.07601548\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65794891 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 0.65437757 0.65794891 0.65437757 0.65437757 0.         0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 2.07244414 0.65437757\n",
      " 0.65437757 1.41806657 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 2.07244414 0.65437757 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 2.07244414 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 2.07244414\n",
      " 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757 0.65437757\n",
      " 2.07244414 0.65794891 2.07601548 0.65437757]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.509\n",
      "(*) epoch 2, cost 2.241\n",
      "(*) epoch 3, cost 2.046\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.574\n",
      "(*) epoch 2, cost 3.131\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.20805431 2.20805431 2.20805431 2.30559531 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 1.07240615\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.30559531 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.30559531 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.77470311 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.77470311 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.30559531 2.77470311 2.20805431 2.20805431 2.20805431\n",
      " 2.20805431 2.20805431 2.20805431 2.20805431]\n",
      "all_sum is: [2.07789264 3.19648019 2.07789264 2.07789264 1.0392912  1.0392912\n",
      " 1.59861881 2.07789264 1.0392912  2.07789264 2.07789264 0.56001737\n",
      " 0.56001737 0.56001737 1.0392912  1.0392912  2.07789264 2.07789264\n",
      " 1.0392912  2.07789264 1.0392912  1.59861881 2.07789264 1.59861881\n",
      " 1.59861881 0.56001737 0.56001737 1.59861881 2.07789264 3.19648019\n",
      " 1.0392912  0.56001737 1.0392912  1.67860491 1.0392912  2.07789264\n",
      " 3.66769793 2.07789264 1.0392912  2.07789264 0.56001737 2.07789264\n",
      " 1.0392912  2.07789264 0.56001737 1.0392912  1.0392912  1.59861881\n",
      " 2.07789264 1.59861881 4.70629937 2.07789264 1.59861881 0.56001737\n",
      " 2.07789264 2.07789264 3.19648019 2.07789264 2.15787875 2.07789264\n",
      " 0.56001737 3.19648019 2.07789264 4.70629937 0.56001737 2.07789264\n",
      " 1.59861881 2.07789264 1.59861881 1.59861881 2.71720635 2.07789264\n",
      " 2.07789264 2.07789264 1.59861881 3.19648019 3.19648019 2.07789264\n",
      " 2.07789264 2.07789264 1.59861881 1.59861881 1.0392912  1.0392912\n",
      " 1.59861881 2.07789264 1.0392912  6.30416075 2.07789264 2.07789264\n",
      " 2.07789264 2.07789264 2.07789264 2.07789264 1.0392912  1.0392912\n",
      " 1.59861881 2.71720635 2.07789264 1.0392912 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.725\n",
      "(*) epoch 2, cost 2.290\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.282\n",
      "(*) epoch 2, cost 2.525\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.03874002 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.14683739\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14683739 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.16578566 0.82468187 0.82468187 0.82468187 0.82468187 1.16578566\n",
      " 0.82468187 1.16578566 1.16578566 1.16578566 1.16578566 1.16578566\n",
      " 0.82468187 0.82468187 0.82468187 1.16578566 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 1.16578566\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 1.16578566 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 1.16578566\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 0.82468187 1.16578566\n",
      " 0.82468187 0.82468187 0.82468187 0.61006389 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 0.82468187 1.16578566 0.82468187 0.82468187\n",
      " 0.82468187 0.82468187 1.16578566 0.82468187 0.82468187 0.82468187\n",
      " 0.82468187 1.16578566 0.82468187 1.16578566 1.16578566 1.16578566\n",
      " 0.82468187 0.82468187 0.82468187 0.82468187 1.16578566 0.82468187\n",
      " 0.82468187 1.16578566 0.61006389 1.16578566 0.82468187 1.16578566\n",
      " 0.82468187 0.82468187 1.16578566 0.82468187]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.422\n",
      "(*) epoch 2, cost 1.110\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.756\n",
      "(*) epoch 2, cost 0.627\n",
      "(*) epoch 3, cost 0.475\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.54161788 0.54161788 0.35466621 0.35466621 0.35466621 0.54161788\n",
      " 0.54161788 0.54161788 0.35466621 0.54161788 0.35466621 0.35466621\n",
      " 0.35466621 0.54161788 0.54161788 0.35466621 0.54161788 0.35466621\n",
      " 0.35466621 0.35466621 0.54161788 0.54161788 0.35466621 0.35466621\n",
      " 0.54161788 0.35466621 0.35466621 0.54161788 0.35466621 0.54161788\n",
      " 0.35466621 0.35466621 0.54161788 0.35466621 0.54161788 0.54161788\n",
      " 0.35466621 0.54161788 0.54161788 0.35466621 1.75579356 0.54161788\n",
      " 0.54161788 0.35466621 0.54161788 0.35466621 0.54161788 0.54161788\n",
      " 0.54161788 0.35466621 0.54161788 0.35466621 0.35466621 0.35466621\n",
      " 0.54161788 0.35466621 0.35466621 0.35466621 0.35466621 0.35466621\n",
      " 0.35466621 0.35466621 0.54161788 0.35466621 0.35466621 0.54161788\n",
      " 1.75579356 1.40112734 2.10247421 0.54161788 0.54161788 1.75579356\n",
      " 0.54161788 0.35466621 0.35466621 0.35466621 0.54161788 0.54161788\n",
      " 0.35466621 0.54161788 0.35466621 0.54161788 0.35466621 0.35466621\n",
      " 0.35466621 0.54161788 0.35466621 0.54161788 0.35466621 0.35466621\n",
      " 0.35466621 0.35466621 0.54161788 0.54161788 0.54161788 0.35466621\n",
      " 0.35466621 0.54161788 0.54161788 0.54161788]\n",
      "all_sum is: [2.84679953 2.59631068 2.84679953 2.84679953 2.59631068 2.05553508\n",
      " 2.84679953 2.84679953 2.84679953 2.59631068 2.84679953 2.84679953\n",
      " 0.88250233 2.05553508 2.84679953 0.88250233 1.13299118 2.84679953\n",
      " 2.84679953 2.84679953 1.80504623 2.84679953 1.13299118 2.59631068\n",
      " 2.84679953 2.05553508 2.84679953 2.84679953 2.84679953 2.59631068\n",
      " 2.59631068 2.84679953 1.80504623 2.84679953 1.13299118 1.13299118\n",
      " 2.05553508 2.84679953 2.84679953 2.59631068 1.13299118 2.59631068\n",
      " 2.59631068 2.84679953 2.84679953 2.84679953 2.84679953 1.13299118\n",
      " 1.13299118 0.34172673 2.05553508 2.84679953 1.80504623 0.34172673\n",
      " 2.05553508 2.59631068 2.05553508 2.84679953 2.59631068 1.13299118\n",
      " 0.34172673 2.59631068 2.59631068 2.84679953 2.84679953 1.80504623\n",
      " 2.59631068 2.84679953 2.84679953 1.80504623 2.59631068 2.05553508\n",
      " 1.13299118 2.84679953 2.84679953 2.84679953 1.13299118 2.05553508\n",
      " 2.84679953 2.59631068 2.05553508 2.84679953 2.84679953 1.80504623\n",
      " 2.84679953 2.84679953 2.59631068 2.59631068 2.59631068 2.59631068\n",
      " 2.59631068 2.05553508 2.84679953 2.59631068 1.80504623 2.84679953\n",
      " 2.05553508 2.84679953 2.84679953 2.59631068]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.727\n",
      "(*) epoch 2, cost 1.807\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.842\n",
      "(*) epoch 2, cost 2.455\n",
      "(*) epoch 3, cost 2.141\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.014969   2.014969   3.59901831 2.014969   2.014969   2.014969\n",
      " 3.59901831 3.59901831 2.014969   3.59901831 2.014969   3.59901831\n",
      " 2.014969   3.59901831 2.014969   2.014969   2.014969   3.59901831\n",
      " 2.014969   3.59901831 3.59901831 2.014969   2.014969   3.59901831\n",
      " 3.59901831 3.59901831 3.59901831 3.59901831 3.59901831 3.59901831\n",
      " 3.59901831 3.59901831 2.014969   3.59901831 3.59901831 2.014969\n",
      " 3.59901831 3.59901831 3.59901831 3.59901831 3.59901831 2.014969\n",
      " 3.59901831 3.59901831 3.59901831 2.014969   2.014969   3.59901831\n",
      " 3.59901831 3.59901831 3.59901831 3.59901831 3.59901831 3.59901831\n",
      " 3.59901831 2.014969   4.71514586 4.14060371 3.59901831 3.59901831\n",
      " 2.014969   2.014969   3.59901831 2.014969   2.014969   2.014969\n",
      " 3.59901831 2.014969   3.59901831 3.59901831 3.59901831 3.59901831\n",
      " 3.59901831 3.59901831 2.014969   2.014969   3.59901831 3.59901831\n",
      " 3.59901831 3.59901831 3.59901831 2.014969   3.59901831 2.014969\n",
      " 2.014969   3.59901831 2.014969   3.59901831 3.59901831 2.014969\n",
      " 4.71514586 2.014969   3.59901831 2.014969   2.014969   2.014969\n",
      " 3.59901831 2.014969   3.59901831 2.014969  ]\n",
      "all_sum is: [2.08485599 1.35603203 1.35603203 1.35603203 2.08485599 1.35603203\n",
      " 0.         2.08485599 2.08485599 1.35603203 0.72882396 0.72882396\n",
      " 0.         1.35603203 1.35603203 2.08485599 2.08485599 0.72882396\n",
      " 2.08485599 1.35603203 1.35603203 1.35603203 1.35603203 1.35603203\n",
      " 0.         1.35603203 1.35603203 0.         2.08485599 1.35603203\n",
      " 1.35603203 2.08485599 1.35603203 2.08485599 1.35603203 2.08485599\n",
      " 1.35603203 1.35603203 2.08485599 2.08485599 2.08485599 2.08485599\n",
      " 0.         1.35603203 2.08485599 2.08485599 1.35603203 1.35603203\n",
      " 2.08485599 1.35603203 0.         2.08485599 2.08485599 0.\n",
      " 2.08485599 2.08485599 2.08485599 2.08485599 2.08485599 0.\n",
      " 2.08485599 1.35603203 2.08485599 1.35603203 2.08485599 2.08485599\n",
      " 0.         0.         1.35603203 1.35603203 2.08485599 1.35603203\n",
      " 0.         1.35603203 1.35603203 0.         2.08485599 1.35603203\n",
      " 2.08485599 2.08485599 0.         1.35603203 1.35603203 1.35603203\n",
      " 0.         2.08485599 1.35603203 2.08485599 2.08485599 2.08485599\n",
      " 2.08485599 1.35603203 0.         0.72882396 1.35603203 1.35603203\n",
      " 0.72882396 1.35603203 1.35603203 1.35603203]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.338\n",
      "(*) epoch 2, cost 3.141\n",
      "(*) epoch 3, cost 2.711\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.685\n",
      "(*) epoch 2, cost 2.631\n",
      "(*) epoch 3, cost 2.310\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 1.15911483 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 1.15911483 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109 0.51105109\n",
      " 0.51105109 0.51105109 0.51105109 0.51105109]\n",
      "all_sum is: [0.         0.         0.         0.         0.35949345 0.35949345\n",
      " 0.35949345 0.         2.16236639 0.         0.         0.\n",
      " 0.         0.35949345 0.         0.         0.         0.35949345\n",
      " 0.35949345 0.35949345 0.35949345 0.35949345 0.35949345 0.35949345\n",
      " 0.         0.35949345 0.         0.35949345 0.         0.\n",
      " 0.35949345 0.         0.         0.35949345 3.36182774 0.35949345\n",
      " 0.35949345 1.80287294 0.         0.35949345 0.         0.35949345\n",
      " 0.35949345 1.80287294 0.         0.         0.         0.35949345\n",
      " 2.16236639 0.         0.         2.16236639 0.         0.\n",
      " 0.         0.         0.         2.15088295 0.         0.\n",
      " 0.         0.35949345 0.35949345 0.         0.         0.\n",
      " 0.         0.         0.         1.55895481 0.         0.\n",
      " 0.         0.         0.         0.35949345 0.         0.\n",
      " 0.35949345 1.91844826 0.35949345 0.         0.35949345 0.\n",
      " 0.         0.         1.80287294 0.         0.35949345 0.\n",
      " 2.16236639 0.         2.16236639 0.         0.         0.\n",
      " 0.         0.         0.         0.35949345]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.848\n",
      "(*) epoch 2, cost 1.008\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.427\n",
      "(*) epoch 2, cost 1.676\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 2.34803124 1.68110549 1.68110549\n",
      " 0.         1.68110549 2.34803124 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 2.34803124 1.68110549 2.34803124 2.34803124\n",
      " 2.34803124 1.68110549 2.34803124 1.68110549 1.68110549 0.\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 2.34803124 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 2.34803124 1.68110549 0.\n",
      " 1.68110549 2.34803124 2.34803124 1.68110549 1.68110549 0.\n",
      " 1.68110549 1.68110549 1.68110549 2.34803124 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549\n",
      " 2.34803124 2.34803124 2.34803124 1.68110549 1.68110549 1.68110549\n",
      " 2.34803124 1.68110549 2.34803124 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 0.\n",
      " 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549 1.68110549\n",
      " 1.68110549 1.68110549 1.68110549 0.66692575]\n",
      "all_sum is: [2.306692   2.21944002 2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   1.96943663 2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.21944002 2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   1.7288769\n",
      " 2.306692   2.306692   2.306692   2.306692   2.21944002 2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692   2.306692   2.306692\n",
      " 2.306692   2.306692   2.306692   2.306692  ]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.883\n",
      "(*) epoch 2, cost 1.833\n",
      "(*) epoch 3, cost 1.660\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.446\n",
      "(*) epoch 2, cost 3.540\n",
      "(*) epoch 3, cost 3.087\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.55184461 4.05978775 4.05978775 4.05978775 5.1835776  2.55184461\n",
      " 2.35895707 3.16076097 3.48274691 5.1835776  3.48274691 4.28455082\n",
      " 5.1835776  4.05978775 5.1835776  4.05978775 4.05978775 3.16076097\n",
      " 4.2526753  2.35895707 5.37646514 4.05978775 2.35895707 5.1835776\n",
      " 2.35895707 4.05978775 3.67563446 3.48274691 4.05978775 3.16076097\n",
      " 2.35895707 5.1835776  1.80783596 3.16076097 3.48274691 4.05978775\n",
      " 5.1835776  4.05978775 4.86159166 2.35895707 4.05978775 3.48274691\n",
      " 3.16076097 2.35895707 2.35895707 4.05978775 2.35895707 2.35895707\n",
      " 2.35895707 2.35895707 5.1835776  4.05978775 5.1835776  4.86159166\n",
      " 3.48274691 3.48274691 5.1835776  2.35895707 4.05978775 3.48274691\n",
      " 4.05978775 3.48274691 2.35895707 4.2526753  2.93162581 5.1835776\n",
      " 2.35895707 5.1835776  3.48274691 5.1835776  3.48274691 4.05978775\n",
      " 2.35895707 2.35895707 4.05978775 4.05978775 2.35895707 2.55184461\n",
      " 3.48274691 5.98538151 5.98538151 3.70155419 2.55184461 2.4532748\n",
      " 3.48274691 4.05978775 4.05978775 2.35895707 2.35895707 4.2526753\n",
      " 3.48274691 5.1835776  4.05978775 2.35895707 5.37646514 2.55184461\n",
      " 4.05978775 4.05978775 4.05978775 2.35895707]\n",
      "all_sum is: [0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 1.6247441  0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 1.6247441  0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 1.6247441  0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637 0.88614637\n",
      " 0.88614637 0.88614637 0.88614637 0.88614637]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.033\n",
      "(*) epoch 2, cost 3.335\n",
      "(*) epoch 3, cost 2.950\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.274\n",
      "(*) epoch 2, cost 1.659\n",
      "(*) epoch 3, cost 1.395\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.93344177 2.24263609 0.93344177 2.24263609 2.24263609 0.93344177\n",
      " 1.36531848 2.24263609 2.24263609 2.24263609 0.93344177 2.67451281\n",
      " 2.24263609 3.45862474 2.24263609 2.24263609 2.67451281 2.24263609\n",
      " 2.67451281 2.24263609 0.         2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 2.24263609 2.67451281 2.24263609 2.67451281\n",
      " 2.24263609 0.         2.24263609 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 1.30919432 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 2.67451281 0.93344177 2.24263609 2.24263609\n",
      " 0.9915325  2.24263609 1.36531848 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 2.24263609 2.24263609 0.93344177 2.24263609\n",
      " 0.93344177 2.24263609 1.30919432 0.93344177 2.24263609 2.24263609\n",
      " 2.24263609 2.24263609 0.93344177 2.24263609 2.24263609 2.67451281\n",
      " 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609 0.93344177\n",
      " 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609 2.24263609\n",
      " 2.24263609 2.67451281 1.74107104 2.24263609]\n",
      "all_sum is: [2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083 2.09181083\n",
      " 2.09181083 2.09181083 2.09181083 2.09181083]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.078\n",
      "(*) epoch 2, cost 1.916\n",
      "(*) epoch 3, cost 1.824\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.074\n",
      "(*) epoch 2, cost 0.461\n",
      "(*) epoch 3, cost 0.287\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "all_sum is: [0.72564429 1.32909805 0.72564429 0.72564429 0.72564429 1.51482065\n",
      " 0.72564429 1.32909805 1.32909805 1.32909805 0.72564429 1.32909805\n",
      " 1.32909805 1.32909805 2.19533364 1.32909805 0.72564429 1.32909805\n",
      " 1.32909805 0.72564429 0.72564429 1.32909805 1.32909805 0.72564429\n",
      " 0.72564429 1.32909805 0.72564429 1.32909805 1.32909805 0.72564429\n",
      " 1.32909805 0.72564429 0.72564429 0.72564429 1.32909805 0.72564429\n",
      " 1.32909805 1.32909805 0.72564429 0.72564429 0.72564429 0.72564429\n",
      " 1.32909805 1.32909805 0.72564429 1.32909805 2.19533364 0.72564429\n",
      " 1.32909805 0.72564429 1.59187988 0.91136689 1.32909805 1.32909805\n",
      " 0.72564429 1.32909805 0.72564429 0.72564429 1.32909805 0.72564429\n",
      " 1.32909805 1.32909805 1.32909805 0.72564429 1.32909805 1.32909805\n",
      " 0.72564429 1.32909805 0.72564429 1.32909805 1.51482065 1.32909805\n",
      " 0.72564429 0.72564429 0.72564429 1.32909805 1.32909805 0.72564429\n",
      " 0.72564429 1.32909805 0.72564429 0.72564429 0.72564429 0.72564429\n",
      " 0.72564429 0.72564429 1.32909805 0.72564429 0.72564429 0.72564429\n",
      " 0.72564429 0.72564429 1.32909805 0.72564429 0.72564429 0.72564429\n",
      " 0.72564429 0.72564429 1.32909805 1.32909805]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.496\n",
      "(*) epoch 2, cost 1.760\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.287\n",
      "(*) epoch 2, cost 2.403\n",
      "(*) epoch 3, cost 2.149\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.48099057 1.20509904 0.48099057 0.48099057 0.48099057 1.20509904\n",
      " 1.20509904 0.48099057 1.20509904 1.20509904 2.37068177 1.64657331\n",
      " 2.37068177 1.64657331 0.48099057 1.64657331 2.37068177 0.48099057\n",
      " 0.48099057 0.48099057 0.48099057 0.48099057 0.48099057 1.20509904\n",
      " 2.37068177 0.48099057 1.64657331 1.20509904 0.48099057 1.64657331\n",
      " 0.48099057 2.37068177 1.20509904 0.48099057 0.48099057 1.20509904\n",
      " 1.64657331 0.48099057 1.20509904 2.37068177 0.48099057 1.20509904\n",
      " 1.20509904 1.64657331 1.20509904 2.37068177 0.48099057 1.20509904\n",
      " 0.48099057 0.48099057 1.20509904 0.48099057 1.20509904 0.48099057\n",
      " 1.64657331 1.20509904 0.48099057 0.48099057 1.64657331 1.64657331\n",
      " 1.64657331 1.20509904 0.48099057 0.48099057 0.48099057 0.48099057\n",
      " 0.48099057 0.48099057 2.37068177 1.64657331 0.48099057 0.48099057\n",
      " 0.48099057 1.20509904 0.48099057 2.37068177 0.48099057 1.64657331\n",
      " 1.20509904 0.48099057 0.48099057 0.48099057 1.20509904 0.48099057\n",
      " 1.64657331 0.48099057 0.48099057 0.48099057 1.64657331 0.48099057\n",
      " 0.48099057 0.48099057 0.48099057 0.48099057 0.48099057 0.48099057\n",
      " 0.48099057 1.20509904 0.48099057 0.48099057]\n",
      "all_sum is: [0.88966842 0.88966842 0.88966842 1.47801669 0.88966842 0.88966842\n",
      " 0.88966842 1.47801669 0.88966842 1.47801669 0.88966842 0.88966842\n",
      " 0.88966842 0.88966842 1.47801669 0.88966842 1.47801669 0.88966842\n",
      " 0.88966842 1.47801669 1.47801669 1.47801669 0.88966842 1.47801669\n",
      " 1.68255373 1.47801669 0.88966842 1.47801669 1.47801669 1.09420545\n",
      " 0.88966842 0.88966842 0.88966842 0.88966842 0.88966842 1.09420545\n",
      " 1.09420545 0.88966842 0.88966842 0.88966842 0.88966842 1.09420545\n",
      " 0.88966842 0.88966842 0.88966842 1.09420545 0.88966842 1.47801669\n",
      " 0.88966842 0.88966842 1.47801669 0.88966842 0.88966842 0.88966842\n",
      " 1.47801669 1.47801669 1.09420545 0.88966842 0.88966842 1.47801669\n",
      " 0.88966842 0.88966842 1.47801669 0.88966842 1.47801669 1.47801669\n",
      " 1.47801669 1.47801669 0.88966842 1.09420545 1.47801669 0.88966842\n",
      " 0.88966842 0.88966842 0.88966842 0.88966842 1.47801669 0.88966842\n",
      " 0.88966842 0.88966842 0.88966842 1.47801669 0.88966842 0.88966842\n",
      " 0.88966842 0.88966842 0.88966842 0.88966842 0.88966842 0.88966842\n",
      " 0.88966842 0.88966842 1.47801669 1.47801669 0.88966842 0.88966842\n",
      " 0.88966842 1.09420545 0.88966842 0.88966842]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.879\n",
      "(*) epoch 2, cost 2.661\n",
      "(*) epoch 3, cost 2.385\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.321\n",
      "(*) epoch 2, cost 4.084\n",
      "(*) epoch 3, cost 3.932\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.35952506 0.35952506 0.35952506 1.17259643 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.89028487\n",
      " 1.17259643 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 1.17259643\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.89028487 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 1.17259643 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.89028487 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506 0.35952506\n",
      " 0.35952506 0.35952506 0.35952506 1.17259643 1.17259643 0.35952506\n",
      " 1.17259643 0.89028487 0.35952506 1.17259643]\n",
      "all_sum is: [0.60610863 2.17468511 0.60610863 0.60610863 1.94420388 1.48948691\n",
      " 1.08767476 0.60610863 1.97105304 1.48948691 0.60610863 1.48948691\n",
      " 1.48948691 0.60610863 0.60610863 1.48948691 1.68340614 1.69311898\n",
      " 1.08767476 0.60610863 1.08767476 1.48948691 0.60610863 1.74057181\n",
      " 0.85719352 1.97105304 0.60610863 0.60610863 1.48948691 0.60610863\n",
      " 1.56802266 0.60610863 0.60610863 0.68464437 0.60610863 0.60610863\n",
      " 0.60610863 0.60610863 0.60610863 0.60610863 0.60610863 0.60610863\n",
      " 0.60610863 0.60610863 0.60610863 0.93572927 0.60610863 3.02150139\n",
      " 0.60610863 0.60610863 0.60610863 1.48948691 0.60610863 0.60610863\n",
      " 1.08767476 0.85719352 0.60610863 0.60610863 1.48948691 0.60610863\n",
      " 0.60610863 1.48948691 0.60610863 1.48948691 0.68464437 0.60610863\n",
      " 0.60610863 0.60610863 0.85719352 1.48948691 0.60610863 1.33875965\n",
      " 0.60610863 2.22213794 0.60610863 0.60610863 0.60610863 1.48948691\n",
      " 1.68340614 2.22213794 0.60610863 1.69311898 1.48948691 1.69311898\n",
      " 0.60610863 1.68340614 0.60610863 0.60610863 1.08767476 0.60610863\n",
      " 0.60610863 0.60610863 0.60610863 1.08767476 0.85719352 1.74057181\n",
      " 1.48948691 0.60610863 1.48948691 1.48114018]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.895\n",
      "(*) epoch 2, cost 4.530\n",
      "(*) epoch 3, cost 4.355\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.398\n",
      "(*) epoch 2, cost 2.306\n",
      "(*) epoch 3, cost 1.965\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.18013361 2.18013361 2.18013361 2.67073773 2.43133422 2.67073773\n",
      " 2.67073773 2.18013361 2.18013361 1.9407301  2.18013361 2.67073773\n",
      " 2.67073773 2.18013361 2.18013361 2.18013361 2.18013361 2.18013361\n",
      " 2.67073773 2.67073773 2.18013361 2.67073773 2.18013361 2.18013361\n",
      " 2.18013361 2.18013361 2.67073773 2.18013361 2.18013361 2.18013361\n",
      " 2.18013361 2.67073773 2.67073773 2.67073773 2.67073773 2.18013361\n",
      " 2.18013361 2.18013361 2.18013361 2.18013361 2.18013361 2.67073773\n",
      " 2.18013361 2.18013361 2.67073773 2.18013361 2.18013361 2.67073773\n",
      " 2.18013361 2.18013361 2.67073773 2.18013361 2.67073773 2.18013361\n",
      " 2.18013361 2.18013361 2.18013361 2.67073773 2.67073773 2.18013361\n",
      " 2.18013361 2.18013361 2.18013361 2.18013361 2.67073773 2.67073773\n",
      " 2.18013361 2.18013361 2.18013361 2.67073773 2.67073773 2.18013361\n",
      " 2.67073773 2.67073773 2.67073773 2.18013361 2.18013361 2.67073773\n",
      " 2.67073773 2.18013361 2.18013361 2.67073773 2.67073773 2.18013361\n",
      " 2.67073773 2.18013361 2.67073773 2.67073773 2.67073773 2.67073773\n",
      " 2.18013361 2.18013361 2.67073773 2.18013361 2.67073773 2.18013361\n",
      " 2.18013361 2.18013361 2.18013361 2.18013361]\n",
      "all_sum is: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.912021   2.46477662 2.46477662 1.912021   2.46477662 1.912021\n",
      " 1.912021   1.912021   1.912021   1.912021   1.912021   2.46477662\n",
      " 2.46477662 1.912021   2.46477662 1.912021   2.46477662 2.46477662\n",
      " 1.912021   3.88614149 1.912021   1.912021   1.912021   1.912021\n",
      " 1.912021   2.46477662 2.46477662 2.46477662 2.4307786  1.912021\n",
      " 2.46477662 2.46477662 1.912021   1.912021   1.912021   2.46477662\n",
      " 3.88614149 1.912021   1.912021   1.912021   2.46477662 1.912021\n",
      " 1.912021   1.912021   2.46477662 1.912021   1.912021   1.912021\n",
      " 3.88614149 1.912021   2.46477662 2.98353423 1.912021   2.46477662\n",
      " 1.912021   1.912021   1.912021   1.912021   1.912021   1.912021\n",
      " 3.88614149 2.46477662 1.912021   1.912021   1.912021   1.912021\n",
      " 2.46477662 2.46477662 1.912021   1.912021   1.912021   1.912021\n",
      " 1.912021   2.46477662 1.912021   1.912021   2.46477662 1.912021\n",
      " 1.912021   1.912021   1.912021   2.46477662 1.912021   1.912021\n",
      " 1.912021   1.912021   1.912021   1.912021   1.912021   1.912021\n",
      " 2.46477662 1.912021   1.912021   2.46477662 1.912021   2.46477662\n",
      " 1.912021   1.912021   2.46477662 2.46477662]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.176\n",
      "(*) epoch 2, cost 1.238\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.823\n",
      "(*) epoch 2, cost 4.484\n",
      "(*) epoch 3, cost 4.190\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.98941624 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.98941624 1.98941624 1.98941624 1.56093235 1.56093235 1.98941624\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.98941624 1.98941624\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.98941624 1.56093235 1.98941624 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.98941624 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.98941624 1.98941624 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235 1.56093235\n",
      " 1.56093235 1.56093235 1.98941624 1.56093235]\n",
      "all_sum is: [2.32417329 0.10998213 0.10998213 0.         0.10998213 0.10998213\n",
      " 0.64564658 0.53566445 0.         0.         1.78850885 0.\n",
      " 0.         0.10998213 0.53566445 2.32417329 0.10998213 0.64564658\n",
      " 2.32417329 0.10998213 0.64564658 0.53566445 0.10998213 1.78850885\n",
      " 0.53566445 0.10998213 0.         0.10998213 0.         1.36513854\n",
      " 0.10998213 1.78850885 1.78850885 0.10998213 2.21419116 0.53566445\n",
      " 0.10998213 1.67852672 0.         0.10998213 0.10998213 0.64564658\n",
      " 0.10998213 1.78850885 0.10998213 0.10998213 1.67852672 0.10998213\n",
      " 0.10998213 1.78850885 0.         0.10998213 0.10998213 1.90080298\n",
      " 1.78850885 1.25515641 0.64564658 0.         0.         0.64564658\n",
      " 1.78850885 0.64564658 0.10998213 0.53566445 0.64564658 2.32417329\n",
      " 0.10998213 0.10998213 0.64564658 0.         0.10998213 0.\n",
      " 0.         0.10998213 0.10998213 0.88974088 2.32417329 1.78850885\n",
      " 1.78850885 0.         1.67852672 1.67852672 1.67852672 0.\n",
      " 0.10998213 0.10998213 0.64564658 0.10998213 0.         1.78850885\n",
      " 0.53566445 0.64564658 0.53566445 0.         0.10998213 1.67852672\n",
      " 0.64564658 0.53566445 2.21419116 0.10998213]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.927\n",
      "(*) epoch 2, cost 1.876\n",
      "(*) epoch 3, cost 1.585\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.623\n",
      "(*) epoch 2, cost 1.993\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.7201719  2.41317748 2.41317748 1.05015385 0.90680335 2.26982699\n",
      " 2.41317748 1.05015385 2.26982699 2.41317748 2.41317748 2.26982699\n",
      " 2.8635224  2.26982699 0.90680335 2.26982699 2.7201719  2.26982699\n",
      " 2.41317748 2.26982699 2.41317748 2.41317748 2.41317748 2.41317748\n",
      " 2.41317748 2.41317748 2.41317748 2.41317748 2.7201719  2.26982699\n",
      " 2.41317748 2.26982699 2.26982699 2.41317748 2.26982699 2.7201719\n",
      " 2.8635224  2.41317748 2.41317748 2.41317748 2.41317748 2.41317748\n",
      " 2.41317748 0.90680335 2.26982699 2.41317748 2.41317748 2.26982699\n",
      " 2.41317748 2.41317748 2.26982699 2.41317748 1.05015385 2.41317748\n",
      " 2.41317748 2.26982699 1.05015385 2.26982699 2.41317748 2.41317748\n",
      " 2.26982699 2.26982699 2.26982699 2.41317748 2.8635224  0.90680335\n",
      " 2.26982699 2.41317748 1.05015385 2.41317748 2.26982699 2.41317748\n",
      " 2.41317748 2.26982699 1.05015385 2.41317748 2.41317748 2.41317748\n",
      " 2.26982699 2.41317748 2.41317748 2.41317748 2.26982699 0.90680335\n",
      " 2.41317748 0.90680335 1.05015385 2.41317748 2.41317748 2.41317748\n",
      " 2.41317748 2.26982699 2.26982699 2.41317748 2.41317748 2.26982699\n",
      " 2.41317748 2.41317748 2.41317748 2.7201719 ]\n",
      "all_sum is: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.73608336\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.73608336\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861 0.59247861\n",
      " 0.59247861 0.59247861 0.59247861 0.59247861]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.543\n",
      "(*) epoch 2, cost 1.914\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.719\n",
      "(*) epoch 2, cost 1.498\n",
      "(*) epoch 3, cost 1.318\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.49157145 1.03555738 1.03555738 0.98257172 1.03555738 1.03555738\n",
      " 1.03555738 1.49157145 1.49157145 0.98257172 1.49157145 1.49157145\n",
      " 1.49157145 1.49157145 0.98257172 1.0433038  1.41328146 1.49157145\n",
      " 1.41328146 0.95726739 1.49157145 0.98257172 1.03555738 1.49157145\n",
      " 1.03555738 0.58728973 0.98257172 1.49157145 1.03555738 1.0433038\n",
      " 0.98257172 0.58728973 1.49157145 1.49157145 1.03555738 1.49157145\n",
      " 0.53430406 1.03555738 1.49157145 1.03555738 1.41328146 1.03555738\n",
      " 1.49157145 1.49157145 0.07828999 1.49157145 1.49157145 1.03555738\n",
      " 1.49157145 1.49157145 1.0433038  1.0433038  1.49157145 1.03555738\n",
      " 0.98257172 0.98257172 1.49157145 0.45601407 1.41328146 1.03555738\n",
      " 1.49157145 1.49157145 1.0433038  1.49157145 0.53430406 0.98257172\n",
      " 0.07828999 1.03555738 1.49157145 1.41328146 0.07828999 1.49157145\n",
      " 0.58728973 1.49157145 1.49157145 1.49157145 1.0433038  1.49157145\n",
      " 1.0433038  0.98257172 1.41328146 1.0433038  1.03555738 1.03555738\n",
      " 1.41328146 1.03555738 1.49157145 0.44826765 1.03555738 1.49157145\n",
      " 0.52655765 1.0433038  1.49157145 1.49157145 0.52655765 1.49157145\n",
      " 0.98257172 1.49157145 1.0433038  1.49157145]\n",
      "all_sum is: [3.96452409 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 5.11605064 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 5.11605064 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409 5.11605064\n",
      " 3.96452409 5.11605064 3.96452409 3.96452409 3.96452409 5.11605064\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409\n",
      " 3.96452409 5.11605064 3.96452409 3.96452409 5.74507599 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 4.59354944 3.96452409 3.96452409\n",
      " 5.11605064 4.07816481 3.96452409 4.07816481 3.96452409 5.11605064\n",
      " 5.11605064 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409 3.96452409\n",
      " 3.96452409 3.96452409 3.96452409 4.59354944 3.96452409 3.96452409\n",
      " 3.96452409 5.11605064 5.11605064 3.96452409 3.96452409 3.96452409\n",
      " 3.96452409 5.11605064 5.11605064 3.96452409 3.96452409 5.11605064\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 4.59354944 5.74507599\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409 4.07816481 5.11605064\n",
      " 3.96452409 3.96452409 3.96452409 3.96452409]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.560\n",
      "(*) epoch 2, cost 3.203\n",
      "(*) epoch 3, cost 2.987\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.825\n",
      "(*) epoch 2, cost 2.486\n",
      "(*) epoch 3, cost 2.073\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.0515787  2.71536346 3.0515787  3.0515787  3.0515787  3.0515787\n",
      " 3.0515787  3.0515787  2.71536346 2.71536346 2.71536346 2.71536346\n",
      " 3.0515787  3.0515787  3.0515787  2.71536346 4.10239052 3.0515787\n",
      " 3.0515787  3.0515787  3.0515787  3.0515787  2.71536346 3.0515787\n",
      " 3.0515787  3.0515787  3.0515787  2.71536346 3.0515787  3.0515787\n",
      " 2.71536346 4.10239052 4.10239052 2.71536346 3.0515787  4.43860576\n",
      " 3.0515787  3.0515787  3.0515787  3.0515787  2.71536346 2.71536346\n",
      " 4.43860576 2.71536346 3.0515787  3.0515787  3.0515787  3.0515787\n",
      " 3.0515787  2.71536346 3.0515787  3.0515787  3.0515787  2.71536346\n",
      " 3.0515787  2.71536346 2.71536346 3.0515787  3.0515787  4.43860576\n",
      " 3.0515787  3.0515787  3.0515787  3.0515787  3.0515787  2.71536346\n",
      " 2.71536346 2.71536346 3.0515787  2.71536346 3.0515787  3.0515787\n",
      " 2.71536346 3.0515787  3.0515787  3.0515787  3.0515787  3.0515787\n",
      " 2.71536346 3.0515787  3.0515787  3.0515787  3.0515787  3.0515787\n",
      " 2.71536346 2.71536346 3.0515787  3.0515787  3.0515787  2.71536346\n",
      " 3.0515787  2.71536346 3.0515787  2.71536346 3.0515787  3.0515787\n",
      " 3.0515787  2.71536346 2.71536346 3.0515787 ]\n",
      "all_sum is: [4.18956744 4.18956744 3.1519629  4.18956744 3.1519629  3.1519629\n",
      " 4.18956744 3.1519629  4.18956744 3.65448319 3.1519629  3.1519629\n",
      " 3.1519629  3.1519629  3.1519629  3.1519629  3.1519629  3.1519629\n",
      " 3.1519629  3.1519629  4.18956744 3.65448319 3.1519629  4.18956744\n",
      " 3.1519629  4.18956744 3.1519629  4.18956744 4.18956744 3.1519629\n",
      " 3.1519629  3.1519629  3.1519629  4.69208774 4.18956744 3.1519629\n",
      " 3.1519629  3.1519629  3.1519629  3.1519629  3.1519629  3.1519629\n",
      " 3.1519629  4.69208774 3.1519629  3.1519629  3.1519629  3.1519629\n",
      " 3.1519629  3.65448319 3.1519629  3.1519629  3.1519629  3.1519629\n",
      " 3.1519629  3.1519629  3.65448319 3.1519629  3.1519629  4.18956744\n",
      " 4.18956744 3.1519629  3.1519629  3.1519629  4.18956744 3.1519629\n",
      " 3.1519629  3.1519629  3.1519629  3.1519629  3.1519629  4.18956744\n",
      " 3.1519629  4.18956744 4.18956744 3.1519629  4.18956744 4.18956744\n",
      " 3.1519629  3.65448319 3.1519629  3.1519629  3.1519629  3.1519629\n",
      " 3.65448319 3.1519629  3.65448319 4.18956744 4.69208774 4.18956744\n",
      " 4.18956744 3.1519629  3.1519629  3.65448319 3.65448319 3.1519629\n",
      " 4.18956744 3.1519629  3.1519629  3.1519629 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.114\n",
      "(*) epoch 2, cost 1.792\n",
      "(*) epoch 3, cost 1.524\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.961\n",
      "(*) epoch 2, cost 1.905\n",
      "(*) epoch 3, cost 1.410\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233 1.4624233\n",
      " 1.4624233 1.4624233]\n",
      "all_sum is: [1.52508317 1.52508317 0.         1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.59953219 0.         0.         1.52508317\n",
      " 3.12461536 1.52508317 1.52508317 1.52508317 1.52508317 0.\n",
      " 1.52508317 1.52508317 1.52508317 0.         0.         1.52508317\n",
      " 1.52508317 1.52508317 0.         1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 3.12461536 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 3.12461536 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 0.         0.         1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317 3.12461536\n",
      " 1.59953219 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 0.         1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 0.         1.52508317 1.52508317 1.52508317 1.52508317\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317 1.52508317 0.\n",
      " 1.52508317 1.52508317 1.52508317 1.52508317]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.841\n",
      "(*) epoch 2, cost 1.244\n",
      "(*) epoch 3, cost 1.069\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.576\n",
      "(*) epoch 2, cost 1.608\n",
      "(*) epoch 3, cost 1.229\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.27591963 0.         1.71111973 0.         0.99898352 0.27591963\n",
      " 1.98703936 0.27591963 0.         1.98703936 0.         0.\n",
      " 1.71111973 0.         1.98703936 0.         1.71111973 0.27591963\n",
      " 1.71111973 0.27591963 1.98703936 0.27591963 0.27591963 0.\n",
      " 0.27591963 1.98703936 0.27591963 0.         0.27591963 0.\n",
      " 0.27591963 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.27591963 1.71111973 0.27591963 1.71111973\n",
      " 1.71111973 0.27591963 1.71111973 0.         0.         0.\n",
      " 0.         0.27591963 0.         1.71111973 0.         0.\n",
      " 1.71111973 1.98703936 0.27591963 0.         0.27591963 0.27591963\n",
      " 0.         1.71111973 0.         0.         0.         1.98703936\n",
      " 1.98703936 0.51615732 1.71111973 0.         0.         0.\n",
      " 0.         0.27591963 0.         1.71111973 0.27591963 1.71111973\n",
      " 0.         1.98703936 1.71111973 1.98703936 0.         0.27591963\n",
      " 2.71010325 1.91244585 0.         0.         0.         0.27591963\n",
      " 0.         1.47622927 0.         0.27591963 0.47724575 0.\n",
      " 1.71111973 0.27591963 0.27591963 0.        ]\n",
      "all_sum is: [2.65425939 2.77509114 2.77509114 1.12285133 1.12285133 2.77509114\n",
      " 2.77509114 2.77509114 2.77509114 2.77509114 1.12285133 2.77509114\n",
      " 3.3452118  1.12285133 1.12285133 3.3452118  1.12285133 1.12285133\n",
      " 2.77509114 2.77509114 2.77509114 2.65425939 2.77509114 4.3064992\n",
      " 1.12285133 1.12285133 2.77509114 1.12285133 1.12285133 2.77509114\n",
      " 2.77509114 2.77509114 1.12285133 2.77509114 2.77509114 2.65425939\n",
      " 1.12285133 2.77509114 4.3064992  1.12285133 1.12285133 2.77509114\n",
      " 2.77509114 1.12285133 2.65425939 2.77509114 2.77509114 2.77509114\n",
      " 1.69297199 2.77509114 1.69297199 2.77509114 1.69297199 3.3452118\n",
      " 2.77509114 1.12285133 1.12285133 2.77509114 2.65425939 2.77509114\n",
      " 2.77509114 2.77509114 2.65425939 1.12285133 2.65425939 1.12285133\n",
      " 2.77509114 1.69297199 2.65425939 1.12285133 2.65425939 1.12285133\n",
      " 1.12285133 2.77509114 1.12285133 4.87661986 4.3064992  1.12285133\n",
      " 2.77509114 1.12285133 3.3452118  1.69297199 2.77509114 4.87661986\n",
      " 2.77509114 2.65425939 1.12285133 1.12285133 2.77509114 2.77509114\n",
      " 2.77509114 2.65425939 2.65425939 2.77509114 1.12285133 2.77509114\n",
      " 1.65223981 1.12285133 2.77509114 2.77509114]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.363\n",
      "(*) epoch 2, cost 3.697\n",
      "(*) epoch 3, cost 3.134\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.575\n",
      "(*) epoch 2, cost 2.897\n",
      "(*) epoch 3, cost 2.649\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949 1.29153949\n",
      " 1.29153949 1.29153949 1.29153949 1.29153949]\n",
      "all_sum is: [0.6023523  0.6023523  0.6023523  1.35486345 0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.56239911 0.6023523  0.6023523  0.6023523\n",
      " 0.56239911 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  1.35486345 1.35486345\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 1.35486345 0.6023523  0.56239911 0.6023523  0.56239911 0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.56239911 0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.56239911 0.56239911 0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  1.35486345\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.56239911 0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523  0.6023523  0.6023523\n",
      " 0.6023523  0.6023523  0.6023523  0.6023523 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.085\n",
      "(*) epoch 2, cost 1.181\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.901\n",
      "(*) epoch 2, cost 1.180\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.21816764 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 1.36556784 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 1.54580574 0.23020334 0.23020334 1.21816764 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 1.54580574 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 1.21816764 2.53377004 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 1.21816764 0.23020334\n",
      " 0.23020334 0.23020334 1.21816764 0.23020334 1.21816764 1.21816764\n",
      " 0.23020334 0.23020334 1.54580574 0.23020334 1.21816764 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334 0.23020334 1.54580574\n",
      " 1.87683679 0.23020334 0.23020334 1.21816764 0.23020334 0.23020334\n",
      " 1.54580574 0.23020334 1.21816764 0.23020334 1.21816764 0.23020334\n",
      " 0.23020334 0.23020334 0.23020334 0.23020334]\n",
      "all_sum is: [1.29770294 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 0.56230821 1.29770294 0.56230821 1.29770294 1.29770294\n",
      " 1.29770294 1.17646075 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294 1.78624549\n",
      " 1.29770294 1.29770294 1.29770294 2.71700331 1.29770294 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 2.71700331 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 0.56230821 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 1.78624549 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 2.71700331 1.29770294 2.71700331 1.29770294 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294 2.71700331\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 0.56230821 1.29770294\n",
      " 2.71700331 1.29770294 2.71700331 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 1.29770294 2.71700331 1.29770294\n",
      " 0.56230821 1.29770294 1.29770294 1.29770294 1.29770294 1.29770294\n",
      " 1.29770294 1.29770294 1.29770294 2.71700331 1.29770294 2.71700331\n",
      " 1.29770294 2.71700331 1.29770294 1.29770294]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.184\n",
      "(*) epoch 2, cost 4.205\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.683\n",
      "(*) epoch 2, cost 2.370\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.74939632 0.35911046 0.74939632\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.74939632\n",
      " 0.35911046 1.51447095 0.35911046 0.35911046 1.90674918 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.74939632 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.74939632 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.74939632 0.35911046 0.35911046 0.35911046 1.90674918\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.74939632 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 1.90674918 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046 0.35911046\n",
      " 0.35911046 0.35911046 0.35911046 0.35911046]\n",
      "all_sum is: [2.88582614 2.88582614 2.88582614 2.88582614 2.88582614 2.88582614\n",
      " 3.43404834 2.88582614 2.88582614 2.88582614 2.88582614 2.88582614\n",
      " 3.43404834 2.88582614 3.43404834 3.16125117 2.88582614 2.88582614\n",
      " 2.88582614 2.88582614 3.43404834 2.88582614 3.43404834 3.43404834\n",
      " 2.88582614 2.88582614 3.43404834 3.43404834 3.70947337 3.16125117\n",
      " 2.88582614 3.43404834 3.43404834 3.43404834 2.88582614 2.88582614\n",
      " 2.88582614 2.88582614 3.43404834 3.43404834 3.43404834 3.43404834\n",
      " 3.43404834 3.43404834 2.88582614 2.88582614 2.88582614 3.16125117\n",
      " 2.88582614 2.88582614 3.43404834 2.88582614 2.88582614 2.88582614\n",
      " 2.88582614 2.88582614 3.43404834 2.88582614 3.43404834 3.43404834\n",
      " 1.53912136 3.43404834 2.88582614 2.88582614 2.88582614 2.88582614\n",
      " 3.43404834 2.88582614 2.88582614 2.88582614 3.43404834 2.88582614\n",
      " 2.88582614 2.88582614 2.88582614 3.43404834 3.16125117 1.53912136\n",
      " 2.88582614 2.88582614 2.88582614 2.88582614 2.88582614 2.88582614\n",
      " 2.88582614 2.88582614 1.53912136 2.88582614 2.88582614 2.88582614\n",
      " 3.43404834 3.43404834 3.43404834 3.43404834 3.43404834 2.88582614\n",
      " 3.43404834 2.88582614 2.88582614 2.88582614]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.279\n",
      "(*) epoch 2, cost 0.923\n",
      "(*) epoch 3, cost 0.718\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.601\n",
      "(*) epoch 2, cost 2.673\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.20899088 1.20899088 0.8885807  1.20899088 0.8885807  1.20899088\n",
      " 0.8885807  0.8885807  3.14423975 0.8885807  1.20899088 3.90944515\n",
      " 1.20899088 1.6537861  1.20899088 4.08836484 1.83270579 1.83270579\n",
      " 1.6537861  1.20899088 0.8885807  0.8885807  0.8885807  3.46464993\n",
      " 0.8885807  1.20899088 1.20899088 1.20899088 0.8885807  0.8885807\n",
      " 3.46464993 0.8885807  0.8885807  1.97419628 2.15311597 0.8885807\n",
      " 1.20899088 0.8885807  3.14423975 1.20899088 0.8885807  0.8885807\n",
      " 1.20899088 1.20899088 3.14423975 0.8885807  1.20899088 0.8885807\n",
      " 0.8885807  0.8885807  3.46464993 1.83270579 3.14423975 1.20899088\n",
      " 3.14423975 1.20899088 0.8885807  0.8885807  3.46464993 0.8885807\n",
      " 0.8885807  0.8885807  1.20899088 1.83270579 0.8885807  3.14423975\n",
      " 1.97419628 3.14423975 1.20899088 0.8885807  2.15311597 1.20899088\n",
      " 0.8885807  1.20899088 2.15311597 1.20899088 1.20899088 0.8885807\n",
      " 0.8885807  4.08836484 1.20899088 2.15311597 1.97419628 0.8885807\n",
      " 0.8885807  0.8885807  1.20899088 0.8885807  0.8885807  1.20899088\n",
      " 1.83270579 4.40877502 1.20899088 0.8885807  0.8885807  1.20899088\n",
      " 1.20899088 1.20899088 0.8885807  0.8885807 ]\n",
      "all_sum is: [1.96568162 1.96568162 1.96568162 1.96568162 1.71530457 1.96568162\n",
      " 1.96568162 1.95960506 1.71530457 1.96568162 1.96568162 1.96568162\n",
      " 1.96568162 1.71530457 3.49141782 1.71530457 1.71530457 1.96568162\n",
      " 1.71530457 1.96568162 1.96568162 1.96568162 1.96568162 1.96568162\n",
      " 1.96568162 1.96568162 1.71530457 1.95960506 1.71530457 1.96568162\n",
      " 1.71530457 1.71530457 1.71530457 1.71530457 1.96568162 1.96568162\n",
      " 1.96568162 3.24104078 1.96568162 1.96568162 1.71530457 1.96568162\n",
      " 1.71530457 1.96568162 1.96568162 1.95960506 1.71530457 1.95960506\n",
      " 1.96568162 1.96568162 1.71530457 1.71530457 1.96568162 1.96568162\n",
      " 1.96568162 1.71530457 1.71530457 1.71530457 1.71530457 1.71530457\n",
      " 1.96568162 1.96568162 1.96568162 1.96568162 1.95960506 1.95960506\n",
      " 3.0948849  1.96568162 1.70922802 1.96568162 1.70922802 1.71530457\n",
      " 1.71530457 1.71530457 1.96568162 1.70922802 1.96568162 1.96568162\n",
      " 1.71530457 1.96568162 1.95960506 3.24104078 1.96568162 1.96568162\n",
      " 1.96568162 3.24104078 1.96568162 1.96568162 1.96568162 1.96568162\n",
      " 1.70922802 1.96568162 1.96568162 1.70922802 1.71530457 1.96568162\n",
      " 1.96568162 1.96568162 1.71530457 1.71530457]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.838\n",
      "(*) epoch 2, cost 1.624\n",
      "(*) epoch 3, cost 1.411\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.264\n",
      "(*) epoch 2, cost 1.979\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  1.266164   0.9229175  1.7360452  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  1.47551737 0.9229175  1.18344533\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  1.18344533 0.9229175  0.9229175  1.47551737\n",
      " 1.47551737 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  1.47551737 0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  1.47551737 0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  1.47551737\n",
      " 0.9229175  0.9229175  0.9229175  1.18344533 0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  0.9229175  0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  1.47551737 0.9229175\n",
      " 0.9229175  0.9229175  0.9229175  0.9229175  1.18344533 0.9229175\n",
      " 0.9229175  1.18344533 0.9229175  0.9229175  1.47551737 0.9229175\n",
      " 1.47551737 0.9229175  0.9229175  0.9229175 ]\n",
      "all_sum is: [3.01720502 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 3.01720502 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 3.01720502 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 3.01720502 3.01720502 3.01720502 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 3.01720502 3.01720502\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 3.01720502 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257 1.26936257\n",
      " 1.26936257 1.26936257 1.26936257 1.26936257]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.384\n",
      "(*) epoch 2, cost 1.708\n",
      "(*) epoch 3, cost 1.251\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.609\n",
      "(*) epoch 2, cost 2.569\n",
      "(*) epoch 3, cost 2.082\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 1.9622061  2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 4.1562935  2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 4.1562935\n",
      " 2.86253333 2.86253333 4.1562935  2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 3.83522518 2.86253333 2.28327442\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 4.1562935  2.54146501 2.54146501 2.86253333\n",
      " 2.86253333 2.86253333 2.54146501 4.1562935  2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.54146501 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333 2.28327442 2.86253333\n",
      " 2.86253333 4.1562935  2.86253333 2.86253333 2.86253333 2.86253333\n",
      " 2.86253333 2.86253333 2.86253333 2.86253333]\n",
      "all_sum is: [0.75923916 0.         0.         0.         0.0698534  0.\n",
      " 1.36197517 0.         0.60273601 0.3271145  0.         0.3271145\n",
      " 0.75923916 0.0698534  0.         1.36197517 0.60273601 0.\n",
      " 0.60273601 0.60273601 0.         0.         0.60273601 0.\n",
      " 0.60273601 0.         0.75923916 0.60273601 0.         0.60273601\n",
      " 0.60273601 0.         0.         0.         0.         0.\n",
      " 0.92985051 0.60273601 1.36197517 0.         0.60273601 0.\n",
      " 0.         0.         0.60273601 0.60273601 0.         0.\n",
      " 0.         0.75923916 0.60273601 0.         0.         0.\n",
      " 0.         0.60273601 0.         0.         0.         0.60273601\n",
      " 0.         0.         0.75923916 0.60273601 0.         1.36197517\n",
      " 0.         0.60273601 0.         0.         0.75923916 0.60273601\n",
      " 0.60273601 0.60273601 0.60273601 0.60273601 0.         0.3271145\n",
      " 0.         0.75923916 0.92985051 0.         0.         0.\n",
      " 0.         0.3271145  0.60273601 0.         0.60273601 0.60273601\n",
      " 0.         0.75923916 0.         0.         0.         0.\n",
      " 0.60273601 0.         0.60273601 0.3271145 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.296\n",
      "(*) epoch 2, cost 2.346\n",
      "(*) epoch 3, cost 2.184\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.902\n",
      "(*) epoch 2, cost 3.542\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.39237342 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.86519929 0.55232145 0.86519929 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.86519929 0.55232145 0.55232145\n",
      " 0.55232145 2.14248541 2.14248541 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 2.45536325\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 1.70525126\n",
      " 0.55232145 0.55232145 0.55232145 0.86519929 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.69287413 0.55232145 1.39237342 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.86519929 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.86519929 2.14248541\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 2.14248541 2.14248541 0.55232145\n",
      " 0.55232145 0.55232145 0.55232145 0.55232145]\n",
      "all_sum is: [3.10671981 1.68672318 1.68672318 1.68672318 1.68672318 3.10671981\n",
      " 3.10671981 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 1.68672318 1.68672318 1.68672318 3.10671981 1.68672318\n",
      " 3.10671981 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 3.10671981 3.10671981 2.68173363 1.68672318 1.68672318\n",
      " 1.68672318 1.68672318 1.68672318 3.10671981 1.68672318 1.68672318\n",
      " 3.10671981 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 1.68672318 1.68672318 3.10671981 1.68672318 1.68672318\n",
      " 1.68672318 3.10671981 3.10671981 3.10671981 1.68672318 2.68173363\n",
      " 3.10671981 3.10671981 3.10671981 3.10671981 1.68672318 1.68672318\n",
      " 3.10671981 3.10671981 1.68672318 3.10671981 1.68672318 3.10671981\n",
      " 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 3.10671981 1.68672318 3.10671981 3.10671981 1.68672318\n",
      " 1.68672318 3.10671981 3.10671981 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318 1.68672318\n",
      " 1.68672318 1.68672318 1.68672318 1.68672318 3.10671981 1.68672318\n",
      " 1.68672318 1.68672318 3.10671981 1.68672318]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.682\n",
      "(*) epoch 2, cost 1.297\n",
      "(*) epoch 3, cost 1.054\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.707\n",
      "(*) epoch 2, cost 1.373\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.39396541 2.61568113 2.61568113 2.39396541 3.50005097 3.50005097\n",
      " 3.50005097 3.50005097 3.50005097 2.39396541 2.39396541 2.39396541\n",
      " 2.61568113 3.50005097 3.50005097 1.50959558 2.39396541 2.39396541\n",
      " 3.50005097 1.50959558 3.50005097 3.50005097 2.39396541 3.42559305\n",
      " 3.50005097 1.50959558 3.50005097 2.61568113 3.50005097 1.50959558\n",
      " 2.39396541 2.61568113 1.50959558 2.61568113 2.39396541 1.50959558\n",
      " 1.50959558 2.61568113 3.50005097 2.39396541 3.50005097 3.50005097\n",
      " 2.61568113 3.50005097 3.50005097 3.50005097 3.50005097 3.50005097\n",
      " 3.50005097 3.50005097 1.50959558 3.50005097 2.39396541 2.61568113\n",
      " 2.39396541 2.39396541 3.50005097 2.39396541 1.50959558 3.50005097\n",
      " 2.39396541 2.61568113 2.39396541 2.61568113 3.50005097 2.61568113\n",
      " 2.39396541 2.39396541 3.50005097 2.39396541 3.50005097 3.50005097\n",
      " 3.50005097 3.50005097 2.39396541 3.50005097 2.61568113 2.39396541\n",
      " 3.50005097 2.39396541 2.39396541 2.39396541 2.39396541 2.39396541\n",
      " 2.39396541 3.50005097 1.50959558 2.39396541 2.39396541 1.50959558\n",
      " 3.50005097 2.39396541 3.50005097 2.39396541 3.50005097 1.50959558\n",
      " 3.50005097 2.61568113 3.50005097 3.50005097]\n",
      "all_sum is: [1.46855256 1.81369857 1.46855256 2.04903961 1.81369857 2.04903961\n",
      " 1.46855256 1.46855256 0.13435696 1.46855256 1.46855256 1.46855256\n",
      " 1.46855256 2.39418562 2.64168971 1.46855256 1.46855256 1.81369857\n",
      " 1.46855256 1.46855256 1.46855256 1.46855256 1.81369857 1.46855256\n",
      " 1.46855256 0.13435696 1.46855256 1.81369857 1.81369857 1.81369857\n",
      " 0.13435696 1.81369857 1.81369857 0.13435696 1.46855256 0.47950297\n",
      " 1.46855256 1.46855256 1.46855256 1.46855256 1.46855256 1.46855256\n",
      " 0.13435696 1.46855256 1.46855256 1.30749411 0.13435696 1.46855256\n",
      " 1.46855256 1.81369857 1.46855256 1.46855256 0.13435696 1.81369857\n",
      " 1.46855256 1.46855256 1.81369857 1.46855256 0.13435696 1.46855256\n",
      " 1.46855256 1.46855256 0.13435696 1.46855256 1.46855256 1.46855256\n",
      " 1.46855256 2.04903961 1.46855256 1.46855256 1.46855256 1.46855256\n",
      " 1.46855256 1.46855256 1.46855256 1.81369857 1.46855256 1.46855256\n",
      " 0.13435696 1.46855256 1.46855256 1.81369857 1.46855256 1.81369857\n",
      " 1.81369857 2.04903961 1.46855256 1.81369857 1.46855256 1.46855256\n",
      " 1.46855256 1.46855256 2.04903961 1.46855256 1.46855256 0.47950297\n",
      " 1.46855256 0.13435696 1.81369857 1.46855256]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.450\n",
      "(*) epoch 2, cost 3.595\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.033\n",
      "(*) epoch 2, cost 2.177\n",
      "(*) epoch 3, cost 2.002\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.63156121 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.33090699 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.63156121 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.63156121 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999 0.01084999\n",
      " 0.01084999 0.01084999 0.01084999 0.01084999]\n",
      "all_sum is: [2.22274946 3.11570496 3.11570496 2.22274946 2.22274946 3.11570496\n",
      " 2.22274946 2.22274946 2.22274946 3.11570496 3.11570496 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 3.11570496 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 3.11570496 3.11570496\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 3.11570496 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 3.11570496\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 3.11570496 2.22274946 2.22274946 3.11570496\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 3.11570496 2.22274946 3.11570496 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946 2.22274946\n",
      " 2.22274946 2.22274946 2.22274946 2.22274946]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.078\n",
      "(*) epoch 2, cost 1.692\n",
      "(*) epoch 3, cost 1.477\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.517\n",
      "(*) epoch 2, cost 2.032\n",
      "(*) epoch 3, cost 1.659\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.54298816 0.60250606 0.54298816 0.54298816 2.12331412 0.54298816\n",
      " 0.54298816 0.54298816 0.54298816 0.60250606 0.60250606 0.60250606\n",
      " 0.60250606 0.54298816 0.60250606 0.54298816 0.60250606 0.54298816\n",
      " 0.60250606 0.54298816 2.18283203 0.54298816 0.54298816 0.54298816\n",
      " 0.60250606 0.54298816 0.60250606 0.60250606 0.60250606 0.60250606\n",
      " 0.54298816 0.60250606 0.54298816 0.54298816 0.60250606 0.54298816\n",
      " 0.54298816 0.60250606 0.54298816 0.54298816 0.54298816 0.54298816\n",
      " 0.54298816 0.60250606 0.54298816 0.54298816 0.60250606 0.54298816\n",
      " 0.54298816 0.54298816 0.54298816 0.60250606 0.60250606 2.12331412\n",
      " 0.54298816 0.54298816 0.54298816 0.54298816 0.54298816 0.60250606\n",
      " 0.60250606 0.54298816 0.54298816 0.54298816 0.60250606 0.60250606\n",
      " 0.54298816 0.54298816 0.54298816 0.54298816 0.60250606 0.60250606\n",
      " 0.54298816 0.54298816 0.54298816 0.54298816 0.60250606 0.54298816\n",
      " 0.54298816 2.12331412 0.54298816 0.54298816 0.60250606 0.54298816\n",
      " 2.18283203 0.54298816 0.60250606 0.54298816 0.54298816 0.60250606\n",
      " 0.54298816 0.54298816 0.60250606 0.60250606 0.60250606 0.54298816\n",
      " 0.54298816 0.60250606 0.54298816 0.60250606]\n",
      "all_sum is: [0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 1.80696419\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 1.80696419 1.3745272\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 1.80696419 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 1.80696419 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 3.19912017 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 0.56861547 2.29714577 0.56861547 0.56861547 0.56861547 0.56861547\n",
      " 2.29714577 0.56861547 2.29714577 1.80696419]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.798\n",
      "(*) epoch 2, cost 2.641\n",
      "(*) epoch 3, cost 2.393\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.399\n",
      "(*) epoch 2, cost 1.471\n",
      "(*) epoch 3, cost 1.239\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.71445669\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.84328296\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.02504897 0.         0.         0.\n",
      " 0.13741891 0.         0.         0.         0.         0.\n",
      " 0.84328296 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.84328296 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [0.1434239  0.77727801 0.77727801 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.92070192 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.92070192 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.92070192 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.92070192 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.92070192 0.92070192 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.         0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.         0.77727801 0.92070192 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801 0.\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801\n",
      " 0.92070192 0.77727801 0.77727801 0.         0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.81408449 0.92070192 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801 0.77727801\n",
      " 0.77727801 0.1434239  0.77727801 0.77727801 0.92070192 0.77727801\n",
      " 0.77727801 0.77727801 0.77727801 0.77727801]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.663\n",
      "(*) epoch 2, cost 1.145\n",
      "(*) epoch 3, cost 0.716\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.719\n",
      "(*) epoch 2, cost 1.249\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.79393913 4.76474859 2.685735   5.70080579 2.685735   5.70080579\n",
      " 4.79393913 4.02732715 3.25662553 5.10911449 4.76474859 4.79393913\n",
      " 4.76474859 4.76474859 4.76474859 4.76474859 4.79393913 2.685735\n",
      " 4.79393913 4.79393913 4.76474859 4.76474859 4.76474859 4.79393913\n",
      " 3.0301009  4.79393913 4.79393913 4.76474859 4.79393913 4.76474859\n",
      " 4.79393913 3.93696756 4.79393913 4.76474859 4.79393913 4.76474859\n",
      " 5.10911449 3.59260166 4.79393913 4.79393913 4.79393913 4.79393913\n",
      " 4.76474859 4.76474859 4.79393913 4.76474859 5.07992395 4.79393913\n",
      " 5.07992395 4.76474859 4.76474859 4.76474859 3.71215178 4.79393913\n",
      " 5.07992395 4.79393913 5.10911449 5.07992395 5.07992395 5.10911449\n",
      " 3.6217922  4.76474859 4.76474859 2.685735   3.59260166 4.79393913\n",
      " 4.79393913 4.76474859 5.10911449 2.685735   4.76474859 4.79393913\n",
      " 4.79393913 4.76474859 5.10911449 4.79393913 4.79393913 4.76474859\n",
      " 4.76474859 5.07992395 4.76474859 4.76474859 5.07992395 4.79393913\n",
      " 4.79393913 4.76474859 4.76474859 4.76474859 4.76474859 4.76474859\n",
      " 4.79393913 4.79393913 4.76474859 2.685735   4.76474859 4.76474859\n",
      " 4.76474859 4.76474859 4.79393913 5.10911449]\n",
      "all_sum is: [1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411 1.09190411\n",
      " 1.09190411 1.09190411 1.09190411 1.09190411]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.671\n",
      "(*) epoch 2, cost 2.968\n",
      "(*) epoch 3, cost 2.721\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.005\n",
      "(*) epoch 2, cost 1.276\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  3.44921393 2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  3.44921393 2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  3.44921393 2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 3.44921393 3.44921393 2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  3.44921393 2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  3.44921393 2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  3.44921393 2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  3.44921393 2.9411321  2.9411321  2.9411321  2.9411321\n",
      " 2.9411321  2.9411321  3.44921393 2.9411321 ]\n",
      "all_sum is: [1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948 1.20826948\n",
      " 1.20826948 1.20826948 1.20826948 1.20826948]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.701\n",
      "(*) epoch 2, cost 1.518\n",
      "(*) epoch 3, cost 1.069\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 2\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.055\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.15019761\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.15019761\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718 0.51142718\n",
      " 0.51142718 0.51142718 0.51142718 0.51142718]\n",
      "all_sum is: [0.86399459 1.26252403 0.86399459 0.86399459 1.26252403 1.26252403\n",
      " 0.86399459 0.86399459 0.86399459 1.26252403 0.86399459 0.86399459\n",
      " 0.86399459 1.26252403 1.26252403 0.86399459 0.86399459 1.26252403\n",
      " 1.26252403 0.86399459 0.86399459 1.26252403 1.26252403 0.86399459\n",
      " 0.86399459 1.26252403 0.86399459 0.86399459 0.86399459 0.86399459\n",
      " 1.26252403 1.26252403 1.26252403 1.26252403 0.86399459 0.86399459\n",
      " 1.26252403 0.86399459 1.26252403 1.26252403 0.86399459 0.86399459\n",
      " 0.86399459 1.26252403 0.86399459 1.26252403 0.86399459 1.26252403\n",
      " 0.86399459 1.26252403 1.26252403 0.86399459 1.26252403 0.86399459\n",
      " 0.86399459 1.26252403 0.86399459 0.86399459 1.26252403 1.26252403\n",
      " 0.86399459 1.26252403 0.86399459 1.26252403 0.86399459 0.86399459\n",
      " 0.86399459 1.26252403 1.26252403 1.26252403 0.86399459 1.26252403\n",
      " 0.86399459 0.86399459 1.26252403 1.26252403 0.86399459 1.26252403\n",
      " 0.86399459 1.26252403 0.86399459 0.86399459 0.86399459 1.26252403\n",
      " 0.86399459 1.26252403 1.26252403 0.86399459 0.86399459 0.86399459\n",
      " 1.26252403 0.86399459 0.86399459 0.86399459 1.26252403 0.86399459\n",
      " 0.86399459 0.86399459 0.86399459 0.86399459]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.994\n",
      "(*) epoch 2, cost 3.297\n",
      "(*) epoch 3, cost 2.985\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.788\n",
      "(*) epoch 2, cost 1.699\n",
      "(*) epoch 3, cost 1.477\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 0.79914594 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.01154292 3.49538516 3.49538516 3.49538516\n",
      " 3.01154292 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.01154292 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.01154292\n",
      " 3.01154292 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.01154292 3.49538516\n",
      " 3.49538516 0.3153037  3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.18008146 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 0.79914594 3.01154292 3.49538516 3.49538516 3.49538516 3.49538516\n",
      " 3.49538516 3.49538516 3.49538516 3.49538516]\n",
      "all_sum is: [1.68365538 1.68365538 0.30622321 0.30622321 0.30622321 0.30622321\n",
      " 0.30622321 1.68365538 0.30622321 0.30622321 0.30622321 0.09963148\n",
      " 0.30622321 0.30622321 1.68365538 0.30622321 0.30622321 0.30622321\n",
      " 1.68365538 0.30622321 1.68365538 0.30622321 1.68365538 1.68365538\n",
      " 0.30622321 1.68365538 0.30622321 0.30622321 1.68365538 0.30622321\n",
      " 0.30622321 1.68365538 0.30622321 1.68365538 0.30622321 1.68365538\n",
      " 0.30622321 1.68365538 0.30622321 0.30622321 1.68365538 1.68365538\n",
      " 0.30622321 0.30622321 1.68365538 1.68365538 1.68365538 0.30622321\n",
      " 0.30622321 0.30622321 0.30622321 0.30622321 1.31466141 1.68365538\n",
      " 1.68365538 0.30622321 0.30622321 0.30622321 0.30622321 0.30622321\n",
      " 0.30622321 0.30622321 0.30622321 1.68365538 0.30622321 1.68365538\n",
      " 1.68365538 0.30622321 0.30622321 0.30622321 1.68365538 1.68365538\n",
      " 0.30622321 0.30622321 0.30622321 1.31466141 0.30622321 0.30622321\n",
      " 0.30622321 0.30622321 0.30622321 0.30622321 0.30622321 1.68365538\n",
      " 1.68365538 0.30622321 0.30622321 0.30622321 1.68365538 0.30622321\n",
      " 1.68365538 0.30622321 1.68365538 0.30622321 1.68365538 0.30622321\n",
      " 1.68365538 0.30622321 1.68365538 1.31466141]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.663\n",
      "(*) epoch 2, cost 1.984\n",
      "(*) epoch 3, cost 1.784\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.887\n",
      "(*) epoch 2, cost 1.662\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.29275651 1.95251592 1.90946584 1.90946584 3.24970644 1.90946584\n",
      " 3.24970644 3.24970644 1.90946584 3.24970644 1.95251592 1.90946584\n",
      " 1.90946584 3.24970644 3.24970644 3.24970644 1.95251592 1.41100181\n",
      " 1.90946584 1.90946584 3.24970644 1.90946584 3.24970644 1.90946584\n",
      " 1.90946584 3.24970644 2.18337471 3.24970644 1.95251592 2.18337471\n",
      " 1.90946584 1.90946584 1.90946584 1.90946584 3.24970644 3.24970644\n",
      " 1.95251592 3.52361531 3.24970644 2.18337471 1.95251592 1.90946584\n",
      " 1.90946584 1.90946584 1.90946584 3.24970644 1.90946584 1.95251592\n",
      " 3.25431369 1.90946584 1.95251592 3.25431369 1.90946584 1.95251592\n",
      " 3.24970644 1.90946584 3.24970644 1.95251592 3.24970644 3.52361531\n",
      " 1.90946584 3.24970644 1.90946584 3.25431369 1.90946584 3.29275651\n",
      " 3.24970644 1.90946584 1.90946584 1.90946584 2.18337471 1.90946584\n",
      " 1.90946584 1.90946584 3.24970644 2.18337471 1.95251592 3.24970644\n",
      " 3.24970644 1.90946584 1.90946584 1.95251592 1.90946584 1.90946584\n",
      " 1.90946584 3.24970644 1.90946584 3.24970644 1.90946584 3.24970644\n",
      " 1.90946584 1.90946584 3.24970644 1.90946584 3.24970644 1.90946584\n",
      " 3.24970644 1.90946584 1.90946584 1.90946584]\n",
      "all_sum is: [1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 2.01027243 1.57067975 1.57067975 2.01027243\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 2.01027243 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 3.02745176 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 2.35541811 1.57067975 1.57067975\n",
      " 3.02745176 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975 1.57067975\n",
      " 1.57067975 1.57067975 1.57067975 1.57067975]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.796\n",
      "(*) epoch 2, cost 1.439\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.232\n",
      "(*) epoch 2, cost 1.926\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.17827758 0.17827758 0.17827758 0.80675896 0.26363367 0.80675896\n",
      " 0.17827758 0.17827758 0.17827758 0.17827758 0.17827758 0.17827758\n",
      " 0.17827758 0.80675896 0.17827758 0.17827758 0.17827758 0.17827758\n",
      " 0.17827758 0.17827758 0.17827758 0.17827758 0.26363367 0.17827758\n",
      " 1.42665839 0.17827758 0.17827758 0.17827758 0.17827758 0.79817702\n",
      " 0.17827758 0.17827758 0.17827758 0.79817702 0.91690788 0.26363367\n",
      " 0.17827758 0.17827758 0.17827758 0.17827758 0.17827758 0.89211505\n",
      " 0.17827758 0.80675896 0.17827758 0.80675896 0.17827758 0.17827758\n",
      " 0.17827758 0.17827758 0.17827758 0.80675896 0.17827758 0.17827758\n",
      " 0.26363367 0.17827758 1.2471651  0.26363367 0.17827758 0.17827758\n",
      " 0.26363367 0.79817702 0.17827758 0.17827758 0.26363367 0.17827758\n",
      " 0.79817702 0.79817702 0.89211505 0.17827758 0.17827758 0.89211505\n",
      " 0.17827758 0.80675896 0.17827758 0.17827758 0.79817702 0.17827758\n",
      " 1.42665839 0.17827758 0.26363367 0.17827758 0.80675896 0.17827758\n",
      " 0.17827758 0.17827758 0.17827758 0.89211505 0.26363367 0.17827758\n",
      " 1.42665839 0.17827758 0.17827758 0.79817702 0.17827758 0.17827758\n",
      " 0.80675896 0.80675896 0.79817702 0.17827758]\n",
      "all_sum is: [1.48989316 1.39671819 1.48989316 1.48989316 1.48989316 1.48989316\n",
      " 1.39671819 1.48989316 1.48989316 1.48989316 1.48989316 1.48989316\n",
      " 1.48989316 1.48989316 1.48989316 1.48989316 1.48989316 1.39671819\n",
      " 1.48989316 1.48989316 1.39671819 1.48989316 1.48989316 0.09317497\n",
      " 1.48989316 1.48989316 1.48989316 1.48989316 1.48989316 1.48989316\n",
      " 1.48989316 1.48989316 1.48989316 1.48989316 1.39671819 1.48989316\n",
      " 1.48989316 1.48989316 1.39671819 1.48989316 1.39671819 1.48989316\n",
      " 1.48989316 1.48989316 1.39671819 2.50501701 1.48989316 1.48989316\n",
      " 1.48989316 1.48989316 1.39671819 1.48989316 1.48989316 1.48989316\n",
      " 1.39671819 1.39671819 1.48989316 1.48989316 1.48989316 1.48989316\n",
      " 1.48989316 1.39671819 1.48989316 1.39671819 1.39671819 1.48989316\n",
      " 1.48989316 1.39671819 1.39671819 1.48989316 1.48989316 1.48989316\n",
      " 1.39671819 1.48989316 1.48989316 1.48989316 1.48989316 1.48989316\n",
      " 1.39671819 1.48989316 1.39671819 1.48989316 1.48989316 2.50501701\n",
      " 0.         1.48989316 1.48989316 1.48989316 0.09317497 1.48989316\n",
      " 1.39671819 1.48989316 1.39671819 1.48989316 1.39671819 1.48989316\n",
      " 1.48989316 1.48989316 1.39671819 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.221\n",
      "(*) epoch 2, cost 1.443\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.610\n",
      "(*) epoch 2, cost 3.012\n",
      "(*) epoch 3, cost 2.783\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.83075947 4.87798925 4.87878767 4.87878767 4.87798925 3.83075947\n",
      " 4.87878767 4.87878767 3.83075947 3.82996105 4.87878767 4.87878767\n",
      " 4.87798925 4.87878767 3.83075947 5.05944873 4.87878767 4.87878767\n",
      " 4.87878767 3.83075947 4.87798925 4.87878767 4.87878767 3.83075947\n",
      " 4.87878767 4.87878767 4.87878767 4.87878767 4.87798925 4.87878767\n",
      " 5.05944873 3.83075947 4.87878767 4.87878767 4.87878767 4.87798925\n",
      " 4.87878767 4.87878767 5.05944873 4.87878767 3.83075947 4.87798925\n",
      " 3.83075947 4.87878767 4.87798925 4.87798925 3.83075947 4.87878767\n",
      " 4.87878767 4.87878767 4.87878767 3.83075947 4.87878767 4.87878767\n",
      " 4.87878767 3.82996105 3.83075947 4.87878767 4.87878767 3.83075947\n",
      " 4.87878767 4.87878767 4.87878767 4.87878767 4.87878767 4.87798925\n",
      " 3.82996105 4.87878767 3.83075947 4.87878767 4.87798925 4.87878767\n",
      " 5.05865031 4.87878767 4.87878767 3.83075947 4.87878767 4.87878767\n",
      " 4.87798925 4.87878767 5.05944873 4.87798925 4.87878767 4.87878767\n",
      " 4.87878767 4.87878767 4.87878767 4.87878767 4.87798925 4.87878767\n",
      " 4.87878767 4.87878767 4.87878767 4.87878767 4.87878767 4.87878767\n",
      " 4.87878767 4.87878767 3.83075947 4.87878767]\n",
      "all_sum is: [1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201 1.03895201\n",
      " 1.03895201 1.03895201 1.03895201 1.03895201]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.624\n",
      "(*) epoch 2, cost 2.555\n",
      "(*) epoch 3, cost 2.395\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.667\n",
      "(*) epoch 2, cost 2.330\n",
      "(*) epoch 3, cost 2.065\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.92440029 1.11211296 0.36137169 1.37830255 1.16259197 1.16259197\n",
      " 0.75074128 0.92440029 1.73967424 1.91333325 0.36137169 0.\n",
      " 1.28577197 1.73967424 0.         1.91333325 1.28577197 0.75074128\n",
      " 1.11211296 0.         0.36137169 1.91333325 0.36137169 0.80122028\n",
      " 0.17365901 0.5350307  0.92440029 0.         1.37830255 0.82021386\n",
      " 0.92440029 0.17365901 0.         1.28577197 0.36137169 0.\n",
      " 0.75074128 0.62756127 1.37830255 0.75074128 0.36137169 0.75074128\n",
      " 0.17365901 0.62756127 1.28577197 1.28577197 0.75074128 0.36137169\n",
      " 0.         1.73967424 1.73967424 1.28577197 1.28577197 0.5350307\n",
      " 0.17365901 1.91333325 1.11211296 1.11211296 0.36137169 1.11211296\n",
      " 1.28577197 0.5350307  0.36137169 0.62756127 1.73967424 0.\n",
      " 0.98893296 1.55196156 1.28577197 0.17365901 0.5350307  0.36137169\n",
      " 1.16259197 0.36137169 0.         0.         1.05840554 1.55196156\n",
      " 0.36137169 1.37830255 1.16259197 0.98893296 0.         0.92440029\n",
      " 0.62756127 0.75074128 1.28577197 0.75074128 0.36137169 0.98893296\n",
      " 0.98893296 1.28577197 0.92440029 0.         0.80122028 0.\n",
      " 0.         0.75074128 0.5350307  1.37830255]\n",
      "all_sum is: [0.53074819 0.2527549  0.2527549  0.77187855 0.77187855 0.77187855\n",
      " 0.53074819 0.77187855 0.53074819 0.2527549  0.77187855 0.77187855\n",
      " 0.77187855 0.77187855 0.2527549  0.77187855 0.2527549  0.77187855\n",
      " 0.77187855 0.77187855 0.77187855 0.77187855 0.77187855 0.53074819\n",
      " 0.01162454 0.77187855 0.77187855 0.77187855 0.77187855 0.77187855\n",
      " 1.13188769 0.2527549  0.2527549  0.01162454 0.2527549  0.01162454\n",
      " 0.53074819 0.2527549  0.77187855 0.2527549  0.77187855 0.77187855\n",
      " 0.2527549  1.843438   0.01162454 0.77187855 0.01162454 0.77187855\n",
      " 0.53074819 0.77187855 0.2527549  0.2527549  0.2527549  0.01162454\n",
      " 0.01162454 0.2527549  0.77187855 0.77187855 0.2527549  1.76142215\n",
      " 0.77187855 0.2527549  0.77187855 0.77187855 0.01162454 0.01162454\n",
      " 1.76142215 0.77187855 0.53074819 0.53074819 1.52029179 0.77187855\n",
      " 0.53074819 0.77187855 0.77187855 0.77187855 0.2527549  1.37301805\n",
      " 0.2527549  0.77187855 0.77187855 0.2527549  0.77187855 0.77187855\n",
      " 0.53074819 0.2527549  0.2527549  0.2527549  0.2527549  0.77187855\n",
      " 1.76142215 0.77187855 0.77187855 0.53074819 0.2527549  1.76142215\n",
      " 0.77187855 0.01162454 1.2422985  0.77187855]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.610\n",
      "(*) epoch 2, cost 3.506\n",
      "(*) epoch 3, cost 3.069\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.082\n",
      "(*) epoch 2, cost 2.263\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.24084728 0.70939355 0.95024083 0.24084728\n",
      " 0.         0.95024083 0.70939355 0.         0.         0.\n",
      " 0.24084728 0.         0.70939355 0.         0.24084728 0.\n",
      " 0.95024083 0.         0.70939355 0.         0.70939355 0.\n",
      " 0.         0.         0.70939355 0.70939355 0.70939355 0.70939355\n",
      " 1.53809182 0.         0.24084728 0.70939355 0.70939355 0.\n",
      " 0.         0.         0.70939355 0.70939355 0.         0.70939355\n",
      " 0.70939355 0.70939355 0.70939355 0.70939355 0.70939355 0.24084728\n",
      " 0.95024083 0.         0.         0.70939355 0.24084728 0.95024083\n",
      " 0.         0.         0.         0.70939355 0.         0.95024083\n",
      " 0.         0.         0.70939355 0.         0.         0.70939355\n",
      " 0.         0.         0.         0.70939355 0.         0.\n",
      " 0.         0.70939355 0.70939355 0.         0.70939355 0.70939355\n",
      " 0.70939355 0.         0.         0.70939355 0.70939355 0.\n",
      " 0.24084728 0.24084728 0.70939355 0.         0.         0.\n",
      " 0.24084728 0.         0.         0.70939355 0.         0.95024083\n",
      " 0.70939355 0.70939355 0.70939355 0.        ]\n",
      "all_sum is: [0.         0.         1.58791794 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.58791794 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.58791794 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.58791794 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.58791794]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.416\n",
      "(*) epoch 2, cost 3.149\n",
      "(*) epoch 3, cost 2.944\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.928\n",
      "(*) epoch 2, cost 1.099\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.31548053 2.72756582 0.         4.0901917  0.31548053\n",
      " 0.31548053 1.67810642 2.72756582 2.41208528 0.31548053 2.41208528\n",
      " 2.41208528 0.         2.72756582 2.41208528 2.41208528 0.31548053\n",
      " 4.98092899 2.41208528 2.41208528 4.40567224 2.41208528 4.98092899\n",
      " 1.67810642 0.         0.31548053 0.         0.         2.5688437\n",
      " 2.41208528 2.72756582 0.         2.41208528 0.         4.98092899\n",
      " 2.72756582 2.41208528 4.0901917  2.41208528 2.41208528 0.31548053\n",
      " 2.72756582 2.41208528 2.41208528 4.66544845 2.41208528 0.31548053\n",
      " 4.40567224 1.99358695 1.67810642 0.         2.5688437  0.\n",
      " 2.41208528 2.41208528 1.67810642 0.31548053 4.0901917  2.41208528\n",
      " 2.41208528 2.41208528 1.67810642 1.67810642 0.31548053 1.67810642\n",
      " 4.40567224 2.41208528 4.40567224 0.31548053 0.         2.72756582\n",
      " 2.72756582 2.41208528 2.72756582 1.67810642 4.0901917  4.66544845\n",
      " 0.         0.         4.0901917  2.41208528 2.41208528 2.41208528\n",
      " 0.         0.         2.72756582 2.72756582 2.41208528 2.72756582\n",
      " 1.67810642 2.72756582 2.41208528 1.99358695 0.         2.72756582\n",
      " 0.         2.72756582 4.0901917  2.72756582]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sum is: [2.43025175 5.67519643 4.69729816 5.67519643 5.67519643 4.69729816\n",
      " 5.83778959 4.69729816 5.83778959 4.69729816 5.67519643 5.67519643\n",
      " 5.67519643 4.69729816 3.40815002 3.40815002 5.83778959 2.43025175\n",
      " 2.59284492 3.40815002 3.40815002 5.67519643 5.83778959 5.67519643\n",
      " 3.40815002 5.67519643 5.67519643 5.67519643 5.67519643 5.67519643\n",
      " 4.85989133 5.67519643 5.67519643 2.43025175 5.67519643 5.67519643\n",
      " 5.67519643 5.67519643 3.40815002 5.67519643 3.40815002 5.67519643\n",
      " 5.67519643 5.67519643 5.83778959 3.40815002 5.67519643 5.83778959\n",
      " 4.85989133 4.69729816 5.67519643 4.69729816 4.69729816 3.57074319\n",
      " 5.67519643 2.59284492 4.69729816 3.57074319 5.67519643 2.43025175\n",
      " 5.67519643 3.40815002 5.67519643 5.67519643 2.59284492 2.43025175\n",
      " 5.83778959 5.67519643 5.83778959 5.67519643 4.38674489 3.57074319\n",
      " 4.69729816 5.67519643 5.67519643 5.67519643 4.85989133 3.40815002\n",
      " 5.67519643 5.67519643 3.57074319 4.85989133 5.67519643 5.67519643\n",
      " 2.43025175 5.67519643 5.67519643 5.67519643 5.67519643 5.67519643\n",
      " 3.40815002 5.67519643 5.67519643 5.67519643 5.67519643 5.67519643\n",
      " 5.67519643 5.67519643 5.83778959 3.40815002]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.764\n",
      "(*) epoch 2, cost 2.940\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.478\n",
      "(*) epoch 2, cost 2.581\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.4706096  0.4706096  2.14608781 0.4706096  2.14608781 0.4706096\n",
      " 0.4706096  0.4706096  0.4706096  0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  0.4706096  0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  2.14608781 2.14608781 0.4706096  0.84257361\n",
      " 0.4706096  2.14608781 0.4706096  0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  2.14608781 0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  2.14608781 0.4706096  0.4706096  0.4706096  0.4706096\n",
      " 2.14608781 0.4706096  2.14608781 0.84257361 0.4706096  0.4706096\n",
      " 2.14608781 0.4706096  0.4706096  0.84257361 0.4706096  0.4706096\n",
      " 0.4706096  2.14608781 0.4706096  2.14608781 0.4706096  2.14608781\n",
      " 2.14608781 0.4706096  2.14608781 0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  0.4706096  0.84257361 0.4706096  2.14608781\n",
      " 0.4706096  0.4706096  0.4706096  0.4706096  0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  0.4706096  0.84257361 0.4706096  0.4706096\n",
      " 0.4706096  2.14608781 2.14608781 0.4706096  0.4706096  2.14608781\n",
      " 2.14608781 0.4706096  0.4706096  2.14608781 0.4706096  0.4706096\n",
      " 0.4706096  0.4706096  0.4706096  2.14608781]\n",
      "all_sum is: [2.76590458 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458\n",
      " 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458\n",
      " 4.19231212 2.76590458 2.76590458 2.76590458 2.76590458 2.40388143\n",
      " 2.40388143 3.0756982  2.40388143 2.76590458 2.76590458 2.76590458\n",
      " 4.19231212 4.19231212 2.76590458 2.76590458 2.76590458 3.83028897\n",
      " 2.76590458 4.19231212 2.76590458 2.76590458 2.40388143 2.76590458\n",
      " 2.76590458 4.19231212 2.76590458 2.76590458 3.0756982  2.40388143\n",
      " 2.76590458 4.19231212 2.76590458 2.76590458 2.40388143 2.76590458\n",
      " 2.76590458 2.76590458 2.76590458 2.76590458 2.40388143 2.76590458\n",
      " 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458 2.40388143\n",
      " 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458 2.76590458\n",
      " 2.76590458 4.19231212 2.76590458 4.19231212 2.76590458 2.76590458\n",
      " 2.40388143 4.19231212 4.19231212 4.19231212 3.83028897 4.19231212\n",
      " 4.19231212 2.76590458 1.47863707 2.40388143 2.76590458 2.76590458\n",
      " 2.76590458 2.76590458 4.19231212 2.76590458 2.76590458 2.76590458\n",
      " 1.28726751 2.76590458 2.76590458 2.76590458 2.76590458 4.19231212\n",
      " 2.40388143 2.90504461 4.19231212 2.76590458]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.325\n",
      "(*) epoch 2, cost 1.326\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.513\n",
      "(*) epoch 2, cost 2.391\n",
      "(*) epoch 3, cost 2.144\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.74791431 3.35742415 3.35742415 3.35742415 2.74791431 2.74791431\n",
      " 3.35742415 3.35742415 3.35742415 2.85646308 3.35742415 3.35742415\n",
      " 3.35742415 3.35742415 3.35742415 3.35742415 3.35742415 3.35742415\n",
      " 3.35742415 3.35742415 2.74791431 2.74791431 3.35742415 3.35742415\n",
      " 2.74791431 3.46597291 3.35742415 3.46597291 3.35742415 2.74791431\n",
      " 3.35742415 3.46597291 3.35742415 3.35742415 2.74791431 3.35742415\n",
      " 3.35742415 3.35742415 2.85646308 3.35742415 3.35742415 3.35742415\n",
      " 3.35742415 3.35742415 3.35742415 2.74791431 3.35742415 3.35742415\n",
      " 3.35742415 2.74791431 3.35742415 2.74791431 3.35742415 2.74791431\n",
      " 3.35742415 3.35742415 3.35742415 2.74791431 2.74791431 3.46597291\n",
      " 2.74791431 3.35742415 3.35742415 3.35742415 3.35742415 3.35742415\n",
      " 3.35742415 3.35742415 3.35742415 2.74791431 3.35742415 2.74791431\n",
      " 2.85646308 3.35742415 3.35742415 2.74791431 3.35742415 2.74791431\n",
      " 2.74791431 3.35742415 3.35742415 3.35742415 2.74791431 2.74791431\n",
      " 3.35742415 3.35742415 3.35742415 3.35742415 3.35742415 2.74791431\n",
      " 3.35742415 2.74791431 2.74791431 3.35742415 2.74791431 2.74791431\n",
      " 3.35742415 3.35742415 3.35742415 3.35742415]\n",
      "all_sum is: [0.38248937 0.38248937 0.38248937 0.38248937 0.38248937 1.12192497\n",
      " 0.38248937 0.38248937 1.12192497 1.12192497 1.12192497 1.12192497\n",
      " 0.38248937 1.12192497 1.12192497 0.38248937 1.12192497 0.38248937\n",
      " 1.12192497 0.38248937 1.12192497 0.38248937 0.38248937 0.38248937\n",
      " 0.38248937 1.12192497 0.38248937 0.38248937 1.12192497 1.12192497\n",
      " 1.12192497 1.12192497 1.12192497 1.12192497 0.38248937 0.38248937\n",
      " 1.12192497 0.38248937 0.38248937 1.12192497 0.38248937 1.12192497\n",
      " 1.12192497 0.38248937 0.38248937 1.12192497 1.12192497 0.38248937\n",
      " 0.38248937 0.38248937 1.12192497 1.12192497 0.38248937 1.12192497\n",
      " 0.38248937 0.38248937 1.12192497 1.12192497 1.12192497 1.12192497\n",
      " 1.12192497 1.12192497 0.38248937 1.12192497 1.12192497 1.12192497\n",
      " 0.38248937 1.12192497 1.12192497 1.12192497 1.12192497 1.12192497\n",
      " 1.12192497 1.12192497 0.38248937 1.12192497 1.12192497 1.12192497\n",
      " 0.38248937 1.12192497 0.38248937 1.12192497 1.12192497 1.12192497\n",
      " 1.12192497 0.38248937 0.38248937 1.12192497 0.38248937 0.38248937\n",
      " 1.12192497 0.38248937 1.12192497 1.12192497 1.12192497 1.12192497\n",
      " 0.38248937 0.38248937 0.38248937 0.38248937]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.540\n",
      "(*) epoch 2, cost 3.134\n",
      "(*) epoch 3, cost 2.958\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.671\n",
      "(*) epoch 2, cost 0.681\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.85458129 1.85458129 1.85458129 3.14385299 1.85458129 3.14385299\n",
      " 3.14385299 1.85458129 1.85458129 1.85458129 3.14385299 1.85458129\n",
      " 3.3374737  2.88239534 1.85458129 3.14385299 3.14385299 1.85458129\n",
      " 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129\n",
      " 1.85458129 3.14385299 3.14385299 1.85458129 3.3374737  1.85458129\n",
      " 1.85458129 1.85458129 3.14385299 1.85458129 1.85458129 3.14385299\n",
      " 1.85458129 1.85458129 3.3374737  2.048202   3.14385299 1.85458129\n",
      " 1.85458129 3.14385299 2.048202   1.85458129 1.85458129 3.14385299\n",
      " 1.85458129 3.14385299 3.14385299 3.14385299 1.85458129 1.85458129\n",
      " 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129\n",
      " 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129\n",
      " 1.85458129 3.3374737  1.85458129 2.048202   1.85458129 3.14385299\n",
      " 1.85458129 1.85458129 1.85458129 1.85458129 1.85458129 3.14385299\n",
      " 1.85458129 1.85458129 3.14385299 3.14385299 1.85458129 1.85458129\n",
      " 1.85458129 3.14385299 1.85458129 1.85458129 3.14385299 1.85458129\n",
      " 1.85458129 3.3374737  1.85458129 3.14385299 1.85458129 2.048202\n",
      " 3.14385299 1.85458129 2.88239534 1.85458129]\n",
      "all_sum is: [0.64303993 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 2.22287919 1.97095626 1.97095626 1.97095626\n",
      " 2.22287919 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 2.22287919 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 2.05601693 2.29503379 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 0.64303993 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 2.29503379 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 2.22287919 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626 1.97095626\n",
      " 1.97095626 0.64303993 2.29503379 1.97095626]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.850\n",
      "(*) epoch 2, cost 3.504\n",
      "(*) epoch 3, cost 3.219\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.053\n",
      "(*) epoch 2, cost 3.104\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086 1.19874086\n",
      " 1.19874086 1.19874086 1.19874086 1.19874086]\n",
      "all_sum is: [0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 1.15964939 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 1.15964939 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 1.15964939 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 1.15964939 0.88943847 0.88943847 0.88943847 0.88943847\n",
      " 0.88943847 0.88943847 0.88943847 0.88943847]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.841\n",
      "(*) epoch 2, cost 1.054\n",
      "(*) epoch 3, cost 0.861\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.058\n",
      "(*) epoch 2, cost 1.340\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.26 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.69132325 0.69132325 0.17725051 0.69132325 0.69132325 0.17725051\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.17725051 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.17725051 0.69132325 0.69132325 0.69132325 0.98326273 0.98326273\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.17725051 0.69132325\n",
      " 0.69132325 0.69132325 0.17725051 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.46918999\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.17725051 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325\n",
      " 0.69132325 0.69132325 0.69132325 0.69132325 0.69132325 0.17725051\n",
      " 0.17725051 0.69132325 0.69132325 0.69132325]\n",
      "all_sum is: [2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 1.08231812 2.18325597 1.08231812\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 1.08231812 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597 2.18325597\n",
      " 2.18325597 2.18325597 2.18325597 2.18325597]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.668\n",
      "(*) epoch 2, cost 0.890\n",
      "(*) epoch 3, cost 0.623\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.267\n",
      "(*) epoch 2, cost 1.754\n",
      "(*) epoch 3, cost 1.405\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.84379395 2.04453816 0.16446551 2.84379395 2.04453816 2.04453816\n",
      " 1.88007265 1.88007265 1.88007265 2.04453816 2.04453816 2.04453816\n",
      " 2.04453816 2.04453816 1.88007265 2.04453816 2.04453816 1.88007265\n",
      " 2.84379395 2.04453816 2.04453816 2.04453816 2.04453816 2.84379395\n",
      " 2.04453816 1.88007265 2.67932844 1.88007265 1.88007265 2.84379395\n",
      " 2.04453816 2.04453816 2.04453816 2.04453816 2.04453816 2.04453816\n",
      " 2.84379395 1.88007265 2.04453816 2.04453816 2.04453816 2.04453816\n",
      " 2.04453816 2.04453816 2.04453816 2.84379395 3.62403651 1.88007265\n",
      " 1.88007265 2.04453816 2.81112043 1.88007265 2.04453816 2.04453816\n",
      " 2.04453816 2.04453816 2.04453816 2.04453816 2.04453816 1.88007265\n",
      " 2.30450305 1.88007265 2.04453816 2.84379395 2.84379395 1.88007265\n",
      " 2.04453816 2.04453816 2.04453816 2.04453816 3.77484173 2.04453816\n",
      " 2.04453816 2.04453816 2.84379395 1.88007265 2.04453816 2.04453816\n",
      " 1.88007265 2.14003754 2.04453816 2.04453816 1.88007265 1.88007265\n",
      " 2.67932844 2.04453816 2.04453816 2.04453816 2.84379395 2.84379395\n",
      " 2.84379395 2.04453816 2.04453816 2.04453816 2.04453816 3.459571\n",
      " 1.88007265 2.67932844 1.88007265 1.88007265]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         2.05858702\n",
      " 0.         0.         0.         2.05858702 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         2.05858702 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         2.05858702 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.05858702 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.683\n",
      "(*) epoch 2, cost 2.289\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.139\n",
      "(*) epoch 2, cost 2.229\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.74974476 2.46379442 2.46379442 2.46379442 0.74974476 0.74974476\n",
      " 0.74974476 2.46379442 0.74974476 0.74974476 0.74974476 0.74974476\n",
      " 2.46379442 2.46379442 2.46379442 2.46379442 0.97500619 2.46379442\n",
      " 0.74974476 2.46379442 0.74974476 2.46379442 0.74974476 2.46379442\n",
      " 0.74974476 0.74974476 2.46379442 0.74974476 2.46379442 2.46379442\n",
      " 0.74974476 2.46379442 2.46379442 2.46379442 2.46379442 2.46379442\n",
      " 0.74974476 0.74974476 0.74974476 2.46379442 2.46379442 2.68905585\n",
      " 0.74974476 0.74974476 2.46379442 2.46379442 0.97500619 2.46379442\n",
      " 0.74974476 2.46379442 0.74974476 2.46379442 2.46379442 2.46379442\n",
      " 0.74974476 2.46379442 2.46379442 2.46379442 2.46379442 2.46379442\n",
      " 2.46379442 2.46379442 2.46379442 0.74974476 0.74974476 2.46379442\n",
      " 2.46379442 0.74974476 0.74974476 0.74974476 0.74974476 0.74974476\n",
      " 0.74974476 0.74974476 0.74974476 0.74974476 0.74974476 2.46379442\n",
      " 2.46379442 0.74974476 0.74974476 0.74974476 0.74974476 2.46379442\n",
      " 0.74974476 0.74974476 0.74974476 0.74974476 0.74974476 0.74974476\n",
      " 2.46379442 0.74974476 2.46379442 0.74974476 0.74974476 0.74974476\n",
      " 2.46379442 0.74974476 0.74974476 0.74974476]\n",
      "all_sum is: [1.93870418 1.93870418 1.06237594 1.93870418 1.93870418 1.93870418\n",
      " 1.93870418 1.06237594 1.06237594 1.06237594 1.93870418 1.93870418\n",
      " 1.93870418 1.06237594 1.93870418 1.93870418 1.93870418 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.06237594 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.06237594 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.06237594 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418\n",
      " 1.06237594 1.93870418 3.69195615 1.93870418 1.93870418 1.93870418\n",
      " 1.93870418 1.93870418 0.87632824 1.06237594 1.93870418 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.06237594 1.93870418 1.93870418\n",
      " 1.93870418 1.06237594 1.93870418 1.93870418 1.06237594 1.93870418\n",
      " 1.93870418 1.93870418 1.93870418 1.93870418 1.06237594 1.06237594\n",
      " 1.93870418 1.93870418 1.06237594 1.93870418 1.06237594 1.93870418\n",
      " 1.06237594 1.93870418 0.87632824 1.93870418 1.93870418 1.06237594\n",
      " 1.93870418 1.93870418 1.93870418 1.06237594]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.429\n",
      "(*) epoch 2, cost 3.975\n",
      "(*) epoch 3, cost 3.728\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.655\n",
      "(*) epoch 2, cost 3.410\n",
      "(*) epoch 3, cost 3.131\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.71794575 1.670475   1.670475   1.71794575 1.2528638  1.670475\n",
      " 1.670475   1.2528638  1.71794575 1.670475   1.71794575 1.71794575\n",
      " 1.2528638  1.71794575 1.2528638  1.71794575 1.670475   1.670475\n",
      " 1.20539305 1.71794575 1.71794575 1.2528638  1.2528638  1.20539305\n",
      " 1.2528638  1.71794575 1.670475   1.71794575 1.20539305 1.670475\n",
      " 1.670475   2.10775231 1.670475   1.71794575 1.670475   1.670475\n",
      " 1.670475   1.71794575 1.2528638  1.20539305 1.2528638  1.2528638\n",
      " 1.71794575 1.670475   1.670475   1.2528638  1.670475   1.71794575\n",
      " 1.670475   1.670475   1.670475   1.2528638  1.670475   1.670475\n",
      " 1.20539305 1.71794575 1.71794575 1.71794575 1.670475   1.20539305\n",
      " 1.670475   1.670475   1.20539305 1.71794575 1.670475   1.71794575\n",
      " 1.71794575 1.670475   1.670475   1.71794575 1.71794575 1.71794575\n",
      " 1.670475   1.71794575 1.71794575 1.670475   1.20539305 1.2528638\n",
      " 1.2528638  1.670475   1.670475   1.2528638  1.670475   1.71794575\n",
      " 1.71794575 1.71794575 1.71794575 1.2528638  1.71794575 1.670475\n",
      " 1.71794575 1.670475   1.670475   1.670475   1.87558132 1.20539305\n",
      " 1.20539305 1.670475   1.71794575 1.71794575]\n",
      "all_sum is: [0.         1.00351358 0.         0.         0.06898876 0.\n",
      " 0.         1.00351358 1.07250234 0.         0.06898876 0.\n",
      " 1.62525032 0.         0.         0.         1.81780016 1.81780016\n",
      " 1.00351358 0.         0.         0.         0.         1.07250234\n",
      " 1.00351358 1.00351358 0.06898876 0.         1.00351358 1.00351358\n",
      " 1.07250234 0.         1.00351358 1.07250234 1.00351358 0.\n",
      " 1.00351358 0.         0.         1.00351358 1.69423907 1.00351358\n",
      " 1.62525032 0.06898876 0.         0.81428658 1.00351358 1.00351358\n",
      " 0.06898876 0.         0.         0.         0.81428658 1.00351358\n",
      " 2.6287639  2.6287639  0.         0.         0.         1.00351358\n",
      " 1.07250234 0.         1.00351358 0.         1.00351358 0.\n",
      " 1.00351358 0.         1.00351358 1.00351358 1.88678892 1.00351358\n",
      " 1.00351358 1.07250234 0.81428658 0.         0.         0.\n",
      " 1.00351358 1.00351358 0.         1.62525032 1.00351358 1.07250234\n",
      " 0.         0.06898876 1.00351358 1.07250234 0.         1.00351358\n",
      " 1.00351358 1.00351358 0.         1.00351358 1.00351358 1.00351358\n",
      " 0.         0.06898876 1.62525032 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.137\n",
      "(*) epoch 2, cost 3.257\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.556\n",
      "(*) epoch 2, cost 2.520\n",
      "(*) epoch 3, cost 2.281\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.85465455 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.85465455 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  0.84739151 0.84739151\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 0.84739151 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  1.1553847  1.1553847  1.1553847  1.1553847  1.1553847\n",
      " 1.1553847  0.84739151 0.84739151 1.1553847 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.133\n",
      "(*) epoch 2, cost 1.140\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.119\n",
      "(*) epoch 2, cost 2.679\n",
      "(*) epoch 3, cost 2.374\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.44290256 1.44290256 1.28510499 2.0105599  2.0105599  1.44290256\n",
      " 1.28510499 1.28510499 1.28510499 1.28510499 1.44290256 1.28510499\n",
      " 1.85276233 1.28510499 1.28510499 1.44290256 2.0105599  1.44290256\n",
      " 1.44290256 1.85276233 1.28510499 1.44290256 2.0105599  2.0105599\n",
      " 1.28510499 1.44290256 2.0105599  1.44290256 1.44290256 2.0105599\n",
      " 1.44290256 2.0105599  1.85276233 1.44290256 1.44290256 1.44290256\n",
      " 1.44290256 2.0105599  2.0105599  1.28510499 2.0105599  1.44290256\n",
      " 1.44290256 1.28510499 2.0105599  1.44290256 1.44290256 2.0105599\n",
      " 1.28510499 2.0105599  1.28510499 2.0105599  2.0105599  1.44290256\n",
      " 1.44290256 1.28510499 2.0105599  1.44290256 1.44290256 1.85276233\n",
      " 1.44290256 1.85276233 1.44290256 1.44290256 2.0105599  1.44290256\n",
      " 1.44290256 1.28510499 1.85276233 1.28510499 1.44290256 2.0105599\n",
      " 1.44290256 1.44290256 1.44290256 2.0105599  1.85276233 1.85276233\n",
      " 2.0105599  1.85276233 2.0105599  1.85276233 1.44290256 1.85276233\n",
      " 1.44290256 2.0105599  1.44290256 1.28510499 1.28510499 1.44290256\n",
      " 1.28510499 1.44290256 1.44290256 1.44290256 2.0105599  1.28510499\n",
      " 1.28510499 1.44290256 1.28510499 1.44290256]\n",
      "all_sum is: [1.59674735 1.59674735 1.59674735 2.19514433 1.59674735 1.59674735\n",
      " 1.59674735 2.19514433 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 2.19514433 1.59674735 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 2.19514433 1.59674735 1.59674735 0.36000419 0.36000419 1.59674735\n",
      " 1.59674735 1.59674735 1.59674735 2.19514433 0.36000419 1.59674735\n",
      " 2.19514433 2.19514433 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 1.59674735 1.59674735 1.59674735 2.19514433 1.59674735 1.59674735\n",
      " 1.59674735 0.36000419 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 1.59674735 0.36000419 1.59674735 0.36000419 1.59674735 1.59674735\n",
      " 2.19514433 1.59674735 2.19514433 1.59674735 1.59674735 2.19514433\n",
      " 1.59674735 1.59674735 1.59674735 1.59674735 0.95840116 1.59674735\n",
      " 1.59674735 1.59674735 1.59674735 2.19514433 0.36000419 1.59674735\n",
      " 0.36000419 1.59674735 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 1.59674735 2.19514433 1.59674735 1.59674735 1.59674735 1.59674735\n",
      " 1.59674735 1.59674735 1.59674735 1.59674735 2.19514433 0.36000419\n",
      " 1.59674735 1.59674735 0.36000419 1.59674735 1.59674735 1.59674735\n",
      " 2.19514433 1.59674735 1.59674735 1.59674735]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 6.740\n",
      "(*) epoch 2, cost 4.063\n",
      "(*) epoch 3, cost 3.586\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.233\n",
      "(*) epoch 2, cost 2.768\n",
      "(*) epoch 3, cost 2.384\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [5.18332415 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 4.60997996 3.53203755 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 5.18332415 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 4.60997996 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 4.60997996 6.26126656 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 5.18332415 4.60997996 4.60997996\n",
      " 3.53203755 6.26126656 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 3.53203755 4.60997996 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 4.60997996 4.60997996 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755 5.18332415\n",
      " 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 3.53203755 4.60997996 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755 3.53203755\n",
      " 4.60997996 3.53203755 3.53203755 3.53203755]\n",
      "all_sum is: [2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.71505317 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.71505317 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.71505317 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281 2.42462281\n",
      " 2.42462281 2.42462281 2.42462281 2.42462281]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.296\n",
      "(*) epoch 2, cost 1.902\n",
      "(*) epoch 3, cost 1.561\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.952\n",
      "(*) epoch 2, cost 2.680\n",
      "(*) epoch 3, cost 2.537\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.56672261 4.56672261 4.56420326 4.56420326 1.27491498 3.52455418\n",
      " 4.56420326 3.52707353 4.56672261 4.56420326 4.56420326 1.59081569\n",
      " 3.52455418 3.52455418 4.56672261 4.56420326 1.59333504 3.52707353\n",
      " 4.56420326 4.56420326 4.56420326 4.56420326 4.56420326 4.56420326\n",
      " 4.56420326 3.20613413 4.56420326 4.56672261 4.56672261 3.52455418\n",
      " 4.56420326 4.56420326 3.20865348 4.56420326 3.20865348 2.63298412\n",
      " 1.27239564 4.56420326 4.56672261 4.56420326 3.52455418 4.56420326\n",
      " 4.56420326 3.52455418 3.52707353 1.59081569 3.52455418 4.56420326\n",
      " 4.56420326 3.20613413 3.52707353 3.52455418 1.27239564 4.56420326\n",
      " 4.56420326 4.56420326 4.56420326 4.56420326 4.56420326 3.20613413\n",
      " 4.56420326 2.63046477 2.63046477 4.56420326 4.56420326 3.52455418\n",
      " 4.56420326 4.56672261 3.20613413 4.56420326 4.56420326 3.20865348\n",
      " 4.56420326 4.56672261 4.56420326 4.56420326 4.56420326 4.56420326\n",
      " 4.56420326 4.56420326 2.63046477 2.63046477 1.59081569 3.52455418\n",
      " 3.20613413 4.56420326 4.56672261 1.59081569 4.56420326 2.63046477\n",
      " 3.52455418 4.56420326 1.59081569 2.63298412 3.52707353 3.52455418\n",
      " 3.52455418 3.52455418 4.56420326 2.63298412]\n",
      "all_sum is: [0.51494654 0.51494654 0.51494654 0.55624578 0.51494654 0.55624578\n",
      " 0.51494654 0.55624578 0.51494654 0.55624578 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.51494654 0.55624578 0.51494654 0.55624578\n",
      " 0.55624578 0.55624578 0.55624578 0.51494654 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.51494654 0.51494654 0.55624578 0.55624578\n",
      " 0.51494654 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.51494654 0.55624578 0.55624578 0.51494654\n",
      " 0.55624578 0.51494654 0.55624578 0.55624578 0.55624578 0.51494654\n",
      " 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.55624578 0.55624578 0.51494654 0.51494654\n",
      " 0.55624578 0.51494654 0.55624578 0.55624578 0.55624578 0.51494654\n",
      " 0.55624578 0.55624578 0.55624578 0.55624578 0.51494654 0.55624578\n",
      " 0.55624578 0.55624578 0.55624578 0.51494654 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578 0.55624578\n",
      " 0.55624578 0.51494654 0.55624578 0.51494654 0.55624578 0.55624578\n",
      " 0.55624578 0.55624578 0.51494654 0.55624578]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.136\n",
      "(*) epoch 2, cost 4.495\n",
      "(*) epoch 3, cost 4.081\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.884\n",
      "(*) epoch 2, cost 2.629\n",
      "(*) epoch 3, cost 2.252\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.61432321 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589 1.11666589\n",
      " 1.11666589 1.11666589 1.61432321 1.11666589]\n",
      "all_sum is: [3.19876784 1.72528218 2.61111522 0.         2.61111522 2.06113827\n",
      " 0.         2.61111522 2.06113827 1.13762956 2.06113827 1.72528218\n",
      " 3.19876784 0.43075797 1.72528218 2.61111522 1.47348565 2.06113827\n",
      " 2.06113827 1.47348565 1.47348565 0.58765262 1.13762956 2.61111522\n",
      " 1.47348565 3.19876784 1.47348565 2.61111522 2.06113827 2.61111522\n",
      " 1.13762956 2.06113827 0.58765262 3.19876784 3.04187318 2.06113827\n",
      " 1.13762956 2.06113827 2.61111522 3.19876784 0.58765262 1.13762956\n",
      " 3.19876784 1.13762956 1.13762956 1.72528218 2.61111522 2.61111522\n",
      " 3.19876784 3.19876784 1.13762956 2.61111522 1.47348565 1.47348565\n",
      " 1.47348565 1.13762956 1.72528218 2.61111522 1.72528218 2.06113827\n",
      " 2.06113827 2.06113827 1.72528218 2.61111522 1.72528218 2.61111522\n",
      " 1.13762956 3.19876784 2.06113827 0.58765262 1.13762956 1.13762956\n",
      " 1.13762956 3.19876784 2.06113827 1.13762956 2.06113827 3.19876784\n",
      " 1.13762956 3.19876784 2.15604015 1.13762956 3.19876784 3.19876784\n",
      " 3.19876784 2.61111522 3.19876784 1.47348565 1.47348565 3.19876784\n",
      " 0.58765262 1.47348565 1.13762956 1.13762956 2.06113827 1.47348565\n",
      " 3.19876784 1.13762956 2.61111522 2.61111522]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.214\n",
      "(*) epoch 2, cost 2.226\n",
      "(*) epoch 3, cost 2.035\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.432\n",
      "(*) epoch 2, cost 2.012\n",
      "(*) epoch 3, cost 1.894\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.86009289 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.86009289 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.86009289 2.61720412 2.61720412 0.86009289\n",
      " 0.26495034 0.26495034 0.26495034 0.26495034 0.86009289 0.26495034\n",
      " 0.26495034 0.86009289 0.26495034 0.26495034 0.86009289 0.86009289\n",
      " 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.86009289 0.86009289 0.26495034 0.26495034\n",
      " 0.26495034 0.86009289 0.86009289 0.26495034 0.26495034 0.26495034\n",
      " 0.86009289 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.86009289 0.26495034 0.26495034 0.86009289\n",
      " 0.26495034 0.26495034 0.26495034 0.86009289 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034 0.26495034\n",
      " 0.86009289 0.86009289 0.86009289 0.26495034 0.86009289 0.26495034\n",
      " 0.26495034 0.26495034 0.26495034 0.86009289 0.26495034 0.26495034\n",
      " 0.26495034 0.26495034 0.86009289 0.26495034 0.26495034 0.26495034\n",
      " 0.86009289 0.26495034 0.26495034 0.86009289]\n",
      "all_sum is: [1.09929485 1.09929485 2.24179142 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 2.24179142 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 2.24179142 1.09929485 2.24179142 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485 2.24179142\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 2.24179142 2.24179142 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 2.24179142 1.09929485\n",
      " 1.09929485 2.24179142 1.09929485 1.09929485 1.09929485 2.24179142\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 2.24179142 1.09929485\n",
      " 1.09929485 2.24179142 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 1.09929485 2.24179142 1.09929485 2.24179142\n",
      " 2.24179142 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 2.24179142 1.09929485 2.24179142 2.24179142\n",
      " 2.24179142 1.09929485 1.09929485 1.09929485 1.09929485 1.09929485\n",
      " 1.09929485 1.09929485 2.24179142 1.09929485]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.562\n",
      "(*) epoch 2, cost 0.775\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.558\n",
      "(*) epoch 2, cost 0.898\n",
      "(*) epoch 3, cost 0.682\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.51817141 4.4152565  4.94397116 4.51817141 4.94397116 4.51817141\n",
      " 5.81119305 5.81119305 4.94397116 3.54803462 3.54803462 5.03474883\n",
      " 4.51817141 4.51817141 5.38539329 4.51817141 4.94397116 4.51817141\n",
      " 3.12223486 5.38539329 6.3587782  4.51817141 4.94397116 4.09561977\n",
      " 4.51817141 5.81119305 5.38539329 5.38539329 4.94397116 5.38539329\n",
      " 5.81119305 4.94397116 5.38539329 5.91735608 5.81119305 4.94397116\n",
      " 4.51817141 3.12223486 4.94397116 5.13766374 4.94397116 4.4152565\n",
      " 3.54803462 4.51817141 5.81119305 4.51817141 4.94397116 4.94397116\n",
      " 5.91735608 4.94397116 4.94397116 5.38539329 5.38539329 3.54803462\n",
      " 4.51817141 5.81119305 4.94397116 5.81119305 4.51817141 3.12223486\n",
      " 4.51817141 4.51817141 4.51817141 3.12223486 4.51817141 4.51817141\n",
      " 4.51817141 5.38539329 4.94397116 3.12223486 4.51817141 4.94397116\n",
      " 5.81119305 5.38539329 2.93027622 4.94397116 5.38539329 5.38539329\n",
      " 5.38539329 6.00488562 4.51817141 3.12223486 4.94397116 6.00488562\n",
      " 5.38539329 3.98945674 5.38539329 4.51817141 3.54803462 5.81119305\n",
      " 4.51817141 5.5634635  4.51817141 5.81119305 4.94397116 4.94397116\n",
      " 5.38539329 4.94397116 4.94397116 4.51817141]\n",
      "all_sum is: [0.7320487 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487 2.7435346\n",
      " 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487 2.7435346 2.7435346\n",
      " 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 0.7320487 2.7435346 2.7435346 2.7435346 0.7320487\n",
      " 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487 2.7435346 0.7320487 0.7320487 2.7435346 2.7435346\n",
      " 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 2.7435346 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 2.7435346 0.7320487 0.7320487 0.7320487 0.7320487 0.7320487\n",
      " 0.7320487 0.7320487]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.536\n",
      "(*) epoch 2, cost 2.758\n",
      "(*) epoch 3, cost 2.275\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.494\n",
      "(*) epoch 2, cost 1.924\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.98304051 3.47088149 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 3.47088149 3.47088149 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 3.47088149 3.47088149\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 1.98304051 1.98304051 3.47088149 1.98304051 1.98304051\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 1.98304051 3.47088149 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 1.98304051 1.98304051 1.98304051 1.98304051 3.47088149\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 3.47088149 3.47088149\n",
      " 1.98304051 1.98304051 1.98304051 3.47088149 1.98304051 1.98304051\n",
      " 3.47088149 3.47088149 3.47088149 3.47088149 3.47088149 1.98304051\n",
      " 1.98304051 3.47088149 1.98304051 3.47088149 1.98304051 1.98304051\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 3.47088149 1.98304051\n",
      " 1.98304051 3.47088149 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 3.47088149 1.98304051 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 1.98304051 1.98304051 1.98304051 1.98304051 1.98304051\n",
      " 1.98304051 1.98304051 3.47088149 1.98304051]\n",
      "all_sum is: [1.62310057 2.20676619 1.62310057 2.20676619 1.62310057 2.20676619\n",
      " 1.62310057 1.62310057 1.62310057 1.62310057 1.62310057 1.62310057\n",
      " 1.62310057 1.62310057 1.62310057 2.20676619 2.20676619 1.62310057\n",
      " 2.20676619 2.20676619 2.20676619 1.62310057 1.62310057 2.20676619\n",
      " 1.70982711 2.20676619 1.62310057 2.20676619 2.20676619 1.62310057\n",
      " 2.20676619 1.62310057 1.62310057 1.62310057 2.20676619 1.62310057\n",
      " 1.62310057 2.20676619 2.20676619 2.20676619 1.62310057 1.62310057\n",
      " 1.62310057 2.20676619 1.70982711 1.62310057 2.20676619 1.62310057\n",
      " 2.20676619 1.62310057 1.62310057 1.62310057 1.62310057 1.62310057\n",
      " 1.62310057 1.62310057 2.20676619 1.62310057 2.20676619 2.20676619\n",
      " 1.62310057 2.20676619 1.62310057 2.20676619 2.20676619 1.62310057\n",
      " 1.62310057 1.62310057 2.20676619 2.20676619 1.62310057 1.70982711\n",
      " 2.20676619 1.62310057 2.20676619 2.20676619 1.62310057 2.20676619\n",
      " 1.62310057 1.62310057 2.87736461 2.20676619 2.20676619 2.20676619\n",
      " 1.62310057 1.62310057 1.62310057 2.20676619 2.20676619 2.20676619\n",
      " 2.20676619 1.62310057 1.62310057 2.20676619 1.62310057 1.62310057\n",
      " 2.20676619 1.62310057 1.62310057 1.70982711]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.799\n",
      "(*) epoch 2, cost 3.459\n",
      "(*) epoch 3, cost 3.047\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.768\n",
      "(*) epoch 2, cost 4.415\n",
      "(*) epoch 3, cost 4.210\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.0346016  2.61679619 2.61679619 2.0346016  2.61679619 2.61679619\n",
      " 2.0346016  2.61679619 2.61679619 2.0346016  2.61679619 2.0346016\n",
      " 2.61679619 2.0346016  2.61679619 2.61679619 2.61679619 2.61679619\n",
      " 2.61679619 2.61679619 2.0346016  2.0346016  2.0346016  2.0346016\n",
      " 2.61679619 2.0346016  2.0346016  2.61679619 2.0346016  2.0346016\n",
      " 2.0346016  2.61679619 2.61679619 2.0346016  2.0346016  2.0346016\n",
      " 2.61679619 2.61679619 2.0346016  2.61679619 2.61679619 2.61679619\n",
      " 2.61679619 2.0346016  2.61679619 2.0346016  2.61679619 2.61679619\n",
      " 2.61679619 2.61679619 2.0346016  2.0346016  2.61679619 2.0346016\n",
      " 2.0346016  2.0346016  2.61679619 2.61679619 2.61679619 2.0346016\n",
      " 2.61679619 2.0346016  2.0346016  2.61679619 2.61679619 2.61679619\n",
      " 2.61679619 2.61679619 2.0346016  2.0346016  2.61679619 2.61679619\n",
      " 2.0346016  2.61679619 2.61679619 2.61679619 2.61679619 2.0346016\n",
      " 2.61679619 2.61679619 2.0346016  2.61679619 2.61679619 2.0346016\n",
      " 2.61679619 2.0346016  2.61679619 2.0346016  2.0346016  2.61679619\n",
      " 2.0346016  2.0346016  2.61679619 2.0346016  2.0346016  2.61679619\n",
      " 2.61679619 2.0346016  2.61679619 2.0346016 ]\n",
      "all_sum is: [0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 1.24643685 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.09098678 0.28590991 0.28590991 0.28590991 0.19492313 0.28590991\n",
      " 0.28590991 0.09098678 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 1.69766472 0.28590991 0.28590991 0.28590991 1.05151372 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.09098678 0.28590991 0.09098678\n",
      " 1.24643685 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.09098678 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.09098678 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991 0.28590991\n",
      " 0.28590991 1.24643685 0.28590991 0.28590991 0.19492313 0.28590991\n",
      " 0.28590991 0.28590991 0.28590991 0.28590991]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.499\n",
      "(*) epoch 2, cost 1.847\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.596\n",
      "(*) epoch 2, cost 3.080\n",
      "(*) epoch 3, cost 2.806\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.0170706  2.00380326 4.0170706  4.65721139 4.65721139 1.36366246\n",
      " 1.36366246 2.89567682 1.36366246 3.53581761 4.65721139 4.0170706\n",
      " 2.89567682 4.65721139 1.36366246 6.39285818 2.00380326 3.53581761\n",
      " 1.36366246 1.36366246 1.36366246 6.18922574 2.00380326 2.00380326\n",
      " 1.36366246 2.20743569 4.65721139 6.39285818 2.00380326 4.0170706\n",
      " 1.36366246 4.65721139 2.84757648 4.65721139 4.0170706  2.00380326\n",
      " 4.65721139 2.00380326 4.65721139 4.65721139 4.0170706  2.00380326\n",
      " 4.65721139 4.0170706  3.53581761 3.53581761 6.18922574 4.0170706\n",
      " 5.50098462 1.36366246 4.86084382 4.65721139 1.36366246 1.36366246\n",
      " 4.65721139 4.65721139 2.00380326 2.00380326 3.53581761 2.00380326\n",
      " 4.65721139 4.0170706  1.36366246 5.54908495 4.65721139 2.00380326\n",
      " 5.54908495 2.00380326 4.65721139 2.00380326 4.65721139 4.65721139\n",
      " 2.89567682 4.0170706  4.65721139 4.65721139 2.00380326 4.0170706\n",
      " 4.0170706  5.50098462 5.50098462 4.0170706  1.36366246 2.00380326\n",
      " 4.65721139 2.00380326 2.89567682 4.65721139 4.0170706  6.18922574\n",
      " 6.18922574 4.65721139 4.65721139 4.0170706  4.65721139 4.0170706\n",
      " 5.54908495 6.18922574 5.54908495 4.0170706 ]\n",
      "all_sum is: [0.18464166 0.18464166 0.         0.         0.         0.18464166\n",
      " 0.18464166 0.18464166 0.18464166 0.18464166 0.18464166 0.18464166\n",
      " 0.18464166 0.18464166 0.18464166 0.         0.18464166 0.\n",
      " 0.         0.18464166 0.18464166 0.         0.         0.\n",
      " 0.18464166 0.         0.18464166 0.18464166 0.18464166 0.18464166\n",
      " 0.18464166 0.         0.18464166 0.18464166 0.18464166 0.18464166\n",
      " 0.         0.18464166 0.         0.18464166 0.         0.18464166\n",
      " 0.18464166 0.18464166 0.         0.18464166 0.         0.\n",
      " 0.         0.         0.18464166 0.18464166 0.18464166 0.\n",
      " 0.18464166 0.18464166 0.         0.         0.         0.18464166\n",
      " 0.         0.18464166 0.         0.18464166 0.18464166 0.\n",
      " 0.18464166 0.         0.         0.18464166 0.         0.18464166\n",
      " 0.18464166 0.18464166 0.         0.18464166 0.18464166 0.18464166\n",
      " 0.         0.18464166 0.18464166 0.18464166 0.         0.\n",
      " 0.18464166 0.18464166 0.18464166 0.18464166 0.         0.18464166\n",
      " 0.18464166 0.18464166 0.18464166 0.         0.18464166 0.18464166\n",
      " 0.         0.18464166 0.18464166 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.841\n",
      "(*) epoch 2, cost 3.316\n",
      "(*) epoch 3, cost 2.973\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.116\n",
      "(*) epoch 2, cost 0.500\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 0.21659841 1.44667103 1.44667103 0.21659841 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 0.21659841\n",
      " 1.44667103 1.44667103 0.21659841 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 0.21659841 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 0.21659841 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 0.21659841 1.44667103 1.44667103 0.21659841 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 0.21659841 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 0.21659841 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 0.21659841 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 0.21659841 0.21659841 1.44667103 1.44667103 1.44667103\n",
      " 1.44667103 1.44667103 1.44667103 1.44667103]\n",
      "all_sum is: [3.33354214 3.33354214 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 1.52273796 3.33354214 1.52273796 3.33354214 1.52273796 1.52273796\n",
      " 3.33354214 3.33354214 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 3.33354214 1.55589289 3.33354214 1.52273796 1.52273796 3.33354214\n",
      " 1.52273796 1.52273796 1.52273796 3.33354214 3.33354214 1.52273796\n",
      " 1.52273796 3.33354214 3.33354214 1.52273796 3.33354214 1.52273796\n",
      " 3.33354214 3.33354214 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 3.33354214 1.52273796 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 3.33354214 3.33354214 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 3.33354214 1.52273796 3.33354214 3.33354214 3.33354214 3.33354214\n",
      " 1.52273796 1.52273796 3.33354214 3.33354214 3.33354214 3.33354214\n",
      " 1.52273796 3.33354214 3.33354214 1.52273796 3.33354214 3.33354214\n",
      " 3.33354214 1.52273796 1.52273796 3.33354214 1.52273796 3.33354214\n",
      " 3.33354214 1.52273796 1.52273796 1.52273796 1.52273796 3.33354214\n",
      " 1.52273796 3.33354214 1.52273796 1.52273796 1.52273796 1.52273796\n",
      " 3.33354214 1.52273796 4.42035665 3.33354214 3.33354214 1.52273796\n",
      " 1.52273796 1.52273796 3.33354214 1.52273796]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.010\n",
      "(*) epoch 2, cost 1.475\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.992\n",
      "(*) epoch 2, cost 1.093\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.         0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557 0.12847557\n",
      " 0.12847557 0.12847557 0.12847557 0.12847557]\n",
      "all_sum is: [4.8325251  2.33845624 4.34278898 3.17043046 2.33845624 2.33845624\n",
      " 3.17043046 2.33845624 2.82819236 2.82819236 2.33845624 4.34278898\n",
      " 2.82819236 2.82819236 2.33845624 2.33845624 4.34278898 2.33845624\n",
      " 4.34278898 2.33845624 2.33845624 2.33845624 2.33845624 2.33845624\n",
      " 2.82819236 2.33845624 2.33845624 2.33845624 2.33845624 2.33845624\n",
      " 2.33845624 2.82819236 2.33845624 2.82819236 2.82819236 2.33845624\n",
      " 2.82819236 2.33845624 2.33845624 2.82819236 2.33845624 2.33845624\n",
      " 2.82819236 2.82819236 2.33845624 2.82819236 2.33845624 2.82819236\n",
      " 3.66016658 2.33845624 3.66016658 2.82819236 2.33845624 2.33845624\n",
      " 2.33845624 4.8325251  2.33845624 3.17043046 2.33845624 2.33845624\n",
      " 2.82819236 4.34278898 3.17043046 2.33845624 2.82819236 2.33845624\n",
      " 2.82819236 2.82819236 2.33845624 2.82819236 2.82819236 2.82819236\n",
      " 3.17043046 2.33845624 2.33845624 2.33845624 5.0417415  2.33845624\n",
      " 2.33845624 3.66016658 2.33845624 2.82819236 2.33845624 4.55200538\n",
      " 4.8325251  3.17043046 2.33845624 2.82819236 2.33845624 2.33845624\n",
      " 2.33845624 2.33845624 3.17043046 2.33845624 2.33845624 3.66016658\n",
      " 3.66016658 2.82819236 3.17043046 4.34278898]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.070\n",
      "(*) epoch 2, cost 1.718\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.643\n",
      "(*) epoch 2, cost 2.132\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.54041387 0.63712946 0.54041387 0.54041387 0.54041387 0.54041387\n",
      " 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387 0.63712946\n",
      " 0.54041387 1.51447936 0.54041387 0.54041387 0.54041387 0.54041387\n",
      " 1.51447936 0.54041387 0.54041387 0.54041387 0.54041387 0.63712946\n",
      " 0.63712946 0.63712946 0.54041387 1.92784068 0.54041387 1.92784068\n",
      " 1.51447936 0.54041387 0.54041387 0.54041387 0.54041387 1.51447936\n",
      " 1.51447936 1.51447936 0.54041387 0.54041387 0.54041387 1.0433651\n",
      " 0.54041387 0.54041387 2.27023302 0.54041387 0.54041387 0.63712946\n",
      " 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387\n",
      " 0.63712946 0.54041387 1.51447936 0.54041387 1.51447936 0.54041387\n",
      " 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387 1.51447936\n",
      " 2.3669486  0.54041387 0.54041387 0.54041387 0.54041387 0.54041387\n",
      " 0.54041387 0.54041387 1.0433651  1.61119494 0.54041387 1.51447936\n",
      " 2.27023302 1.51447936 0.54041387 0.54041387 1.61119494 1.51447936\n",
      " 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387 0.54041387\n",
      " 0.54041387 0.54041387 2.27023302 1.51447936 0.54041387 0.54041387\n",
      " 0.54041387 0.54041387 0.54041387 0.54041387]\n",
      "all_sum is: [1.61897011 1.08071874 2.52606986 2.52606986 1.08071874 2.94472981\n",
      " 1.61897011 2.52606986 2.52606986 2.52606986 1.08071874 2.94472981\n",
      " 1.08071874 1.61897011 1.61897011 2.52606986 2.52606986 1.08071874\n",
      " 1.08071874 1.08071874 2.52606986 2.52606986 0.173619   2.52606986\n",
      " 1.08071874 2.52606986 1.61897011 2.52606986 0.173619   2.52606986\n",
      " 1.49937869 1.08071874 1.61897011 2.52606986 1.61897011 0.173619\n",
      " 2.52606986 1.08071874 2.52606986 0.59227895 1.08071874 1.08071874\n",
      " 2.52606986 2.52606986 2.52606986 2.52606986 0.173619   1.08071874\n",
      " 2.52606986 1.08071874 1.61897011 1.61897011 1.08071874 0.173619\n",
      " 2.52606986 2.52606986 1.08071874 1.08071874 1.61897011 0.173619\n",
      " 1.08071874 1.61897011 1.08071874 1.61897011 2.52606986 2.94472981\n",
      " 1.08071874 1.08071874 2.52606986 2.52606986 0.173619   2.52606986\n",
      " 1.08071874 2.52606986 2.52606986 1.49937869 0.173619   1.08071874\n",
      " 2.52606986 2.52606986 2.52606986 0.173619   2.52606986 2.52606986\n",
      " 1.08071874 2.52606986 1.08071874 2.52606986 2.52606986 2.52606986\n",
      " 0.173619   0.173619   2.52606986 0.173619   1.08071874 2.52606986\n",
      " 2.52606986 2.52606986 1.08071874 2.52606986]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.312\n",
      "(*) epoch 2, cost 3.346\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.278\n",
      "(*) epoch 2, cost 4.200\n",
      "(*) epoch 3, cost 4.010\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.19370078 3.29003205 3.29003205 3.88444234 3.17737623 2.63698309\n",
      " 3.29003205 3.99709816 4.84674974 3.88444234 4.13968364 3.29003205\n",
      " 3.37397885 3.29003205 3.29003205 3.17737623 3.99709816 2.63698309\n",
      " 3.17737623 3.17737623 2.63698309 2.52432727 3.29003205 2.52432727\n",
      " 2.52432727 2.52432727 2.63698309 2.63698309 3.29003205 2.52432727\n",
      " 2.63698309 3.17737623 2.63698309 4.84674974 3.29003205 2.52432727\n",
      " 2.63698309 3.29003205 3.99709816 2.52432727 3.29003205 2.63698309\n",
      " 3.29003205 3.29003205 2.52432727 4.19370078 3.29003205 4.13968364\n",
      " 2.63698309 3.23139337 3.99709816 3.34404919 3.29003205 2.63698309\n",
      " 3.17737623 4.13968364 4.84674974 3.17737623 3.29003205 3.99709816\n",
      " 2.52432727 4.84674974 3.29003205 3.17737623 2.52432727 3.29003205\n",
      " 2.63698309 2.63698309 3.88444234 4.84674974 3.34404919 4.13968364\n",
      " 3.29003205 3.29003205 2.63698309 2.63698309 4.13968364 4.08104496\n",
      " 3.29003205 3.29003205 2.63698309 2.52432727 3.34404919 4.02702782\n",
      " 3.29003205 2.52432727 2.63698309 4.19370078 3.99709816 3.17737623\n",
      " 3.48663467 2.63698309 2.63698309 3.29003205 3.29003205 2.52432727\n",
      " 2.52432727 4.84674974 3.99709816 4.02702782]\n",
      "all_sum is: [0.         2.49965021 2.05797052 0.95414855 2.77249389 0.4225171\n",
      " 0.68130487 1.37666565 1.10382197 0.4225171  0.4225171  0.4225171\n",
      " 2.77249389 0.95414855 0.95414855 0.         2.05797052 0.68130487\n",
      " 0.95414855 0.95414855 0.95414855 1.37666565 2.05797052 0.95414855\n",
      " 0.4225171  0.95414855 0.         0.95414855 0.95414855 0.4225171\n",
      " 2.05797052 0.4225171  1.63545341 0.95414855 1.37666565 0.4225171\n",
      " 1.37666565 0.95414855 0.68130487 1.10382197 0.95414855 1.10382197\n",
      " 2.05797052 1.37666565 0.4225171  0.95414855 0.4225171  1.37666565\n",
      " 1.37666565 0.4225171  1.37666565 3.03128165 0.         0.4225171\n",
      " 0.4225171  1.63545341 1.10382197 1.37666565 0.4225171  2.05797052\n",
      " 1.63545341 0.         0.68130487 1.37666565 1.10382197 0.4225171\n",
      " 1.37666565 0.         1.37666565 1.37666565 0.4225171  0.\n",
      " 1.10382197 0.4225171  0.95414855 0.68130487 0.4225171  1.37666565\n",
      " 0.4225171  1.10382197 2.05797052 0.4225171  0.4225171  0.95414855\n",
      " 2.05797052 1.37666565 0.95414855 1.10382197 2.05797052 0.4225171\n",
      " 0.68130487 2.05797052 0.         0.         1.37666565 0.4225171\n",
      " 0.4225171  0.95414855 0.95414855 1.37666565]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.978\n",
      "(*) epoch 2, cost 2.441\n",
      "(*) epoch 3, cost 2.085\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.887\n",
      "(*) epoch 2, cost 3.926\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.35742628 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.35742628 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.35742628 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [3.2675027  3.2675027  3.2675027  3.2675027  3.2675027  4.96964702\n",
      " 3.2675027  4.96964702 3.2675027  3.2675027  4.96964702 3.2675027\n",
      " 4.96964702 3.2675027  3.2675027  3.2675027  4.96964702 3.2675027\n",
      " 3.2675027  4.96964702 4.96964702 3.2675027  3.2675027  3.2675027\n",
      " 3.2675027  3.2675027  4.96964702 3.2675027  4.96964702 3.2675027\n",
      " 3.2675027  3.2675027  3.2675027  4.96964702 3.2675027  3.2675027\n",
      " 4.96964702 3.2675027  4.96964702 3.2675027  4.96964702 4.96964702\n",
      " 4.96964702 3.2675027  3.2675027  3.2675027  3.2675027  4.96964702\n",
      " 3.2675027  3.2675027  3.2675027  4.96964702 4.96964702 3.2675027\n",
      " 3.2675027  4.96964702 3.2675027  3.2675027  4.96964702 4.96964702\n",
      " 3.2675027  4.96964702 3.2675027  4.96964702 3.2675027  3.2675027\n",
      " 3.2675027  3.2675027  3.2675027  3.2675027  4.96964702 3.2675027\n",
      " 3.2675027  4.96964702 3.2675027  4.96964702 3.2675027  4.96964702\n",
      " 3.2675027  4.96964702 3.2675027  3.2675027  3.2675027  4.96964702\n",
      " 3.2675027  3.2675027  4.96964702 4.96964702 3.2675027  3.2675027\n",
      " 4.96964702 3.2675027  3.2675027  4.96964702 3.2675027  3.2675027\n",
      " 4.96964702 3.2675027  3.2675027  3.2675027 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.899\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.129\n",
      "(*) epoch 2, cost 1.920\n",
      "(*) epoch 3, cost 1.527\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.84171479 2.65131559 2.35322428 2.35322428 2.84171479 1.80971146\n",
      " 1.80971146 1.80971146 2.16282509 2.35322428 1.80971146 2.35322428\n",
      " 2.16282509 2.84171479 2.35322428 2.35322428 2.35322428 1.80971146\n",
      " 2.84171479 2.35322428 2.16282509 2.35322428 2.84171479 2.35322428\n",
      " 1.80971146 2.35322428 1.80971146 2.35322428 2.35322428 1.80971146\n",
      " 1.80971146 2.35322428 2.16282509 2.35322428 2.84171479 2.16282509\n",
      " 1.80971146 1.80971146 2.35322428 2.35322428 2.35322428 1.80971146\n",
      " 2.35322428 2.35322428 2.35322428 1.80971146 1.80971146 2.35322428\n",
      " 2.35322428 2.35322428 2.35322428 2.35322428 2.35322428 2.35322428\n",
      " 1.80971146 2.35322428 2.35322428 2.35322428 2.35322428 2.84171479\n",
      " 2.10780277 2.10780277 2.35322428 2.16282509 2.35322428 2.16282509\n",
      " 2.35322428 1.80971146 2.35322428 2.35322428 2.35322428 2.35322428\n",
      " 1.80971146 1.80971146 2.35322428 1.80971146 2.35322428 2.35322428\n",
      " 2.35322428 2.35322428 2.65131559 2.84171479 1.80971146 2.29820196\n",
      " 2.35322428 2.35322428 1.80971146 2.35322428 2.16282509 2.16282509\n",
      " 2.35322428 2.35322428 2.35322428 1.61931226 2.35322428 2.35322428\n",
      " 2.35322428 2.16282509 2.16282509 2.29820196]\n",
      "all_sum is: [1.249117   2.55076043 1.83378956 1.96608788 3.31704729 1.249117\n",
      " 2.55076043 1.96608788 2.55076043 2.55076043 2.73237474 1.83378956\n",
      " 1.83378956 1.96608788 1.83378956 1.249117   2.55076043 1.249117\n",
      " 2.55076043 1.83378956 1.249117   1.249117   1.96608788 2.55076043\n",
      " 1.83378956 2.55076043 1.249117   2.55076043 1.249117   1.249117\n",
      " 1.83378956 2.55076043 1.249117   1.96608788 1.249117   1.96608788\n",
      " 1.249117   3.44934561 2.55076043 1.96608788 1.249117   2.55076043\n",
      " 1.83378956 1.83378956 2.55076043 2.55076043 1.249117   2.55076043\n",
      " 2.55076043 2.55076043 1.96608788 1.83378956 1.249117   2.55076043\n",
      " 1.83378956 1.96608788 2.55076043 3.31704729 2.55076043 1.96608788\n",
      " 1.83378956 1.249117   2.55076043 1.83378956 1.96608788 1.96608788\n",
      " 1.96608788 2.55076043 1.96608788 2.55076043 3.44934561 1.96608788\n",
      " 1.96608788 1.96608788 1.83378956 1.96608788 1.96608788 1.249117\n",
      " 2.55076043 2.55076043 1.83378956 1.83378956 1.249117   1.249117\n",
      " 1.83378956 3.31704729 1.249117   1.83378956 2.55076043 1.249117\n",
      " 1.83378956 2.55076043 1.83378956 2.55076043 3.44934561 1.249117\n",
      " 1.96608788 1.96608788 1.83378956 1.249117  ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 5.700\n",
      "(*) epoch 2, cost 3.994\n",
      "(*) epoch 3, cost 3.749\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.131\n",
      "(*) epoch 2, cost 2.275\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.4368925  0.41330575 0.4368925  0.4368925  0.41330575 0.41330575\n",
      " 0.4368925  1.39617361 0.41330575 0.4368925  0.41330575 0.41330575\n",
      " 0.41330575 0.41330575 0.41330575 0.41330575 0.41330575 0.41330575\n",
      " 0.4368925  0.41330575 0.41330575 0.41330575 0.41330575 0.41330575\n",
      " 0.4368925  0.41330575 0.4368925  0.41330575 0.41330575 0.41330575\n",
      " 0.4368925  0.41330575 0.4368925  0.4368925  0.4368925  0.41330575\n",
      " 0.4368925  0.41330575 0.4368925  0.4368925  0.41330575 0.41330575\n",
      " 0.4368925  0.4368925  0.41330575 0.41330575 0.41330575 0.4368925\n",
      " 0.41330575 0.41330575 0.41330575 0.4368925  0.4368925  0.41330575\n",
      " 0.4368925  0.4368925  0.41330575 0.41330575 0.4368925  0.41330575\n",
      " 0.4368925  0.41330575 0.4368925  0.41330575 0.41330575 0.41330575\n",
      " 0.41330575 0.4368925  2.00487044 0.41330575 1.39617361 0.4368925\n",
      " 0.4368925  0.4368925  0.4368925  0.4368925  0.4368925  0.4368925\n",
      " 0.41330575 0.41330575 0.41330575 0.41330575 0.4368925  0.41330575\n",
      " 0.41330575 0.4368925  0.41330575 0.4368925  0.4368925  0.41330575\n",
      " 0.4368925  0.41330575 0.4368925  0.4368925  0.4368925  0.41330575\n",
      " 0.41330575 0.4368925  0.41330575 0.4368925 ]\n",
      "all_sum is: [1.47339987 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987\n",
      " 1.47339987 1.47339987 2.91650731 1.47339987 1.47339987 2.91650731\n",
      " 1.47339987 1.47339987 2.91650731 1.47339987 1.47339987 2.91650731\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 2.91650731 1.47339987\n",
      " 2.91650731 1.47339987 2.91650731 1.47339987 2.91650731 2.91650731\n",
      " 2.91650731 2.91650731 1.47339987 1.47339987 1.47339987 2.91650731\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987\n",
      " 2.91650731 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 2.91650731 1.47339987\n",
      " 1.47339987 2.91650731 2.91650731 2.91650731 1.47339987 1.47339987\n",
      " 2.91650731 1.47339987 1.47339987 2.91650731 1.47339987 1.47339987\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 2.91650731 1.47339987\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987\n",
      " 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987 1.47339987\n",
      " 1.47339987 1.47339987 2.91650731 2.91650731 1.47339987 1.47339987\n",
      " 1.47339987 2.91650731 2.91650731 1.47339987 1.47339987 1.47339987\n",
      " 2.91650731 1.47339987 2.91650731 1.47339987]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.469\n",
      "(*) epoch 2, cost 1.417\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.664\n",
      "(*) epoch 2, cost 1.534\n",
      "(*) epoch 3, cost 1.236\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.70372778 0.70372778 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 0.89566024 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 1.17758217 1.17758217 0.70372778 0.70372778 0.70372778 1.17758217\n",
      " 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 1.17758217 0.70372778 0.70372778 1.17758217 1.17758217 0.70372778\n",
      " 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 1.17758217 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778\n",
      " 0.70372778 0.70372778 0.70372778 0.70372778 0.89566024 0.70372778\n",
      " 0.70372778 0.70372778 0.70372778 0.70372778 0.70372778 0.70372778\n",
      " 1.17758217 0.70372778 0.70372778 0.70372778 0.70372778 1.17758217\n",
      " 0.70372778 0.70372778 1.17758217 0.70372778 0.70372778 0.70372778\n",
      " 0.70372778 0.70372778 0.70372778 0.70372778]\n",
      "all_sum is: [4.78425958 4.78425958 4.93718473 4.78425958 4.78425958 3.26740925\n",
      " 4.78425958 4.78425958 4.93718473 4.78425958 3.26740925 4.78425958\n",
      " 4.78425958 4.58439865 4.78425958 4.78425958 4.93718473 3.42033439\n",
      " 4.43147351 3.26740925 3.26740925 4.78425958 4.78425958 3.26740925\n",
      " 4.78425958 3.06754832 4.78425958 3.26740925 4.78425958 4.58439865\n",
      " 4.93718473 4.78425958 4.78425958 4.43147351 4.78425958 4.78425958\n",
      " 4.78425958 4.78425958 4.58439865 4.93718473 4.78425958 4.78425958\n",
      " 4.78425958 4.78425958 4.93718473 4.78425958 4.78425958 4.78425958\n",
      " 4.78425958 4.78425958 4.78425958 3.26740925 4.93718473 4.78425958\n",
      " 4.43147351 4.93718473 4.78425958 4.78425958 4.78425958 4.78425958\n",
      " 4.93718473 3.26740925 4.78425958 4.93718473 4.58439865 4.58439865\n",
      " 4.93718473 4.78425958 4.78425958 4.78425958 4.93718473 4.43147351\n",
      " 4.78425958 3.42033439 4.78425958 4.78425958 4.93718473 4.93718473\n",
      " 3.26740925 4.93718473 4.78425958 4.93718473 4.58439865 4.43147351\n",
      " 4.78425958 2.91462317 3.26740925 4.78425958 4.43147351 4.78425958\n",
      " 4.78425958 4.78425958 4.58439865 4.43147351 3.26740925 4.78425958\n",
      " 4.78425958 4.93718473 4.78425958 4.78425958]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.263\n",
      "(*) epoch 2, cost 0.905\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.187\n",
      "(*) epoch 2, cost 4.578\n",
      "(*) epoch 3, cost 4.029\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.01261053 2.87189254 3.11160486 2.87189254 3.67122393 2.6300931\n",
      " 3.25440997 3.42942449 3.74824887 3.81194192 3.25440997 2.6300931\n",
      " 3.74824887 2.16150071 3.01261053 3.74824887 3.11160486 3.42942449\n",
      " 4.37256575 4.1307663  4.54758026 3.25440997 3.74824887 3.25440997\n",
      " 3.99004832 3.74824887 3.01261053 4.37256575 3.99004832 3.99004832\n",
      " 2.6300931  2.87189254 4.93009769 2.87189254 3.67122393 3.99004832\n",
      " 4.93009769 2.6300931  3.74824887 4.05374136 2.6300931  2.87189254\n",
      " 3.01261053 5.70647265 2.6300931  2.87189254 2.6300931  4.37256575\n",
      " 3.74824887 1.77898328 3.74824887 2.6300931  3.99004832 4.1307663\n",
      " 3.99004832 3.74824887 2.6300931  2.6300931  2.72908743 3.81194192\n",
      " 2.6300931  4.1307663  3.99004832 4.1307663  1.8527311  3.01261053\n",
      " 2.6300931  3.25440997 2.6300931  2.6300931  2.87189254 3.67122393\n",
      " 2.6300931  4.78937971 4.1307663  3.01261053 3.25440997 2.6300931\n",
      " 4.93009769 3.01261053 4.54758026 1.8527311  4.1307663  2.6300931\n",
      " 4.05374136 3.74824887 2.87189254 3.01261053 1.53718383 2.6300931\n",
      " 3.67122393 2.6300931  4.54758026 2.6300931  3.74824887 3.99004832\n",
      " 2.16150071 2.6300931  3.99004832 3.01261053]\n",
      "all_sum is: [1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332 1.91457332\n",
      " 1.91457332 1.91457332 1.91457332 1.91457332]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.982\n",
      "(*) epoch 2, cost 3.344\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.401\n",
      "(*) epoch 2, cost 0.958\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [5.05802402 5.05802402 5.47839694 4.91656651 5.70504026 5.47839694\n",
      " 5.47839694 6.26687069 5.70504026 6.26687069 5.53234121 4.49619359\n",
      " 5.70504026 4.88532497 6.26687069 6.26687069 5.47839694 5.84649777\n",
      " 5.84649777 4.74386746 4.88532497 4.91656651 4.49619359 5.53234121\n",
      " 6.26687069 6.26687069 5.28466734 5.47839694 5.05802402 4.49619359\n",
      " 6.26687069 5.05802402 5.67379873 4.88532497 4.91656651 5.05802402\n",
      " 5.47839694 4.91656651 5.47839694 5.53234121 5.84649777 4.91656651\n",
      " 4.49619359 4.49619359 4.49619359 5.30569789 5.70504026 5.05802402\n",
      " 5.05802402 6.26687069 4.49619359 5.47839694 6.09417164 5.84649777\n",
      " 4.88532497 6.26687069 5.30569789 4.49619359 5.28466734 4.91656651\n",
      " 5.1119683  5.47839694 5.05802402 5.67379873 5.05802402 5.30569789\n",
      " 4.49619359 5.84649777 5.67379873 5.84649777 6.09417164 6.26687069\n",
      " 4.49619359 5.1119683  4.49619359 4.91656651 4.91656651 4.91656651\n",
      " 5.05802402 4.91656651 4.49619359 4.49619359 6.26687069 5.07864526\n",
      " 5.05802402 6.26687069 5.47839694 6.26687069 5.84649777 5.47839694\n",
      " 4.91656651 5.67379873 6.09417164 5.53234121 4.91656651 6.26687069\n",
      " 5.30569789 4.32349454 5.28466734 5.47839694]\n",
      "all_sum is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 5.582\n",
      "(*) epoch 2, cost 3.652\n",
      "(*) epoch 3, cost 3.370\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.324\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.10985097\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 2.19267451 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 2.19267451 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 2.19267451\n",
      " 1.58028532 1.10985097 1.58028532 2.19267451 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532 1.58028532\n",
      " 1.58028532 1.58028532 1.58028532 1.58028532]\n",
      "all_sum is: [0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.05622108 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.05622108 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.05622108 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.05622108 0.05622108 0.35576018\n",
      " 1.44119759 0.35576018 0.35576018 0.35576018 0.35576018 0.35576018\n",
      " 0.35576018 0.35576018 0.35576018 0.35576018]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.715\n",
      "(*) epoch 2, cost 1.524\n",
      "(*) epoch 3, cost 1.395\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.588\n",
      "(*) epoch 2, cost 2.519\n",
      "(*) epoch 3, cost 2.339\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.07507862 2.07507862 2.07507862 3.26518667 2.70217328 3.26518667\n",
      " 3.26518667 2.07507862 2.70217328 2.70217328 2.07507862 2.70217328\n",
      " 2.70217328 2.07507862 2.07507862 2.07507862 2.70217328 2.07507862\n",
      " 4.07498626 2.07507862 2.70217328 2.07507862 2.70217328 2.8848782\n",
      " 3.26518667 3.26518667 2.70217328 2.07507862 2.07507862 2.70217328\n",
      " 3.89228134 2.07507862 3.26518667 2.07507862 2.07507862 2.70217328\n",
      " 2.70217328 2.70217328 2.70217328 2.25778354 2.07507862 2.07507862\n",
      " 3.26518667 2.07507862 3.89228134 2.70217328 2.07507862 2.70217328\n",
      " 2.70217328 2.70217328 3.89228134 2.07507862 2.07507862 4.07498626\n",
      " 2.07507862 2.70217328 2.07507862 2.07507862 2.8848782  3.89228134\n",
      " 2.70217328 2.70217328 2.70217328 2.07507862 2.70217328 2.70217328\n",
      " 2.07507862 3.26518667 2.70217328 3.89228134 2.07507862 3.89228134\n",
      " 2.07507862 2.07507862 2.70217328 2.07507862 2.07507862 2.70217328\n",
      " 2.70217328 2.07507862 3.89228134 2.70217328 2.70217328 2.70217328\n",
      " 3.89228134 2.70217328 2.07507862 2.07507862 3.89228134 3.89228134\n",
      " 2.07507862 2.70217328 2.07507862 2.70217328 2.07507862 2.07507862\n",
      " 2.07507862 2.70217328 2.07507862 2.07507862]\n",
      "all_sum is: [1.78015471 0.         2.72315312 1.78015471 2.72315312 1.78015471\n",
      " 1.78015471 0.         1.78015471 2.72315312 2.72315312 1.78015471\n",
      " 1.78015471 1.78015471 2.72315312 1.78015471 1.78015471 1.78015471\n",
      " 1.78015471 1.78015471 1.78015471 2.72315312 1.78015471 1.78015471\n",
      " 1.78015471 1.78015471 1.78015471 1.78015471 1.78015471 1.78015471\n",
      " 1.78015471 0.         1.78015471 1.78015471 1.78015471 1.78015471\n",
      " 2.72315312 1.78015471 1.78015471 1.78015471 1.78015471 1.78015471\n",
      " 1.78015471 1.78015471 0.         1.78015471 1.78015471 1.78015471\n",
      " 1.78015471 2.72315312 1.78015471 1.78015471 1.78015471 2.72315312\n",
      " 1.78015471 1.78015471 1.78015471 1.78015471 1.78015471 2.72315312\n",
      " 1.78015471 2.72315312 1.78015471 2.72315312 1.78015471 1.78015471\n",
      " 1.78015471 1.78015471 1.78015471 1.78015471 0.         1.78015471\n",
      " 1.78015471 1.78015471 1.78015471 2.72315312 1.78015471 1.78015471\n",
      " 1.78015471 1.78015471 1.78015471 0.         0.94299841 0.\n",
      " 1.78015471 2.72315312 1.78015471 2.72315312 1.78015471 1.78015471\n",
      " 2.72315312 1.78015471 1.78015471 0.         1.78015471 1.78015471\n",
      " 2.72315312 1.78015471 2.72315312 1.78015471]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 3.704\n",
      "(*) epoch 2, cost 1.955\n",
      "(*) epoch 3, cost 1.551\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.705\n",
      "(*) epoch 2, cost 2.967\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.81849384 0.81849384 0.81849384 2.4400491  0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.53340705 0.81849384 0.81849384 0.81849384\n",
      " 2.4400491  0.81849384 0.81849384 0.81849384 0.81849384 2.4400491\n",
      " 0.81849384 0.81849384 0.81849384 0.53340705 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384\n",
      " 0.81849384 0.53340705 0.81849384 0.81849384 0.90026386 0.81849384\n",
      " 0.53340705 0.81849384 0.81849384 2.4400491  0.81849384 0.53340705\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 2.1549623  0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 1.16817437 0.81849384\n",
      " 0.81849384 0.53340705 0.81849384 0.53340705 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.53340705 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 2.4400491  0.53340705\n",
      " 0.81849384 0.81849384 2.4400491  0.81849384 0.81849384 0.81849384\n",
      " 0.81849384 0.81849384 0.81849384 0.81849384 0.81849384 2.4400491\n",
      " 0.53340705 0.81849384 0.81849384 0.81849384]\n",
      "all_sum is: [0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934 0.50033934\n",
      " 0.50033934 0.50033934 0.50033934 0.50033934]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.040\n",
      "(*) epoch 2, cost 1.906\n",
      "(*) epoch 3, cost 1.677\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.016\n",
      "(*) epoch 2, cost 1.888\n",
      "(*) epoch 3, cost 1.353\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.70752383 0.32332082 0.32332082 0.         0.32332082 0.\n",
      " 0.32332082 0.32332082 0.32332082 0.         0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082 0.\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082 0.70752383\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082\n",
      " 0.32332082 0.         0.         0.32332082 0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.32332082 0.         0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.70752383 0.\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082 0.\n",
      " 0.         0.32332082 0.32332082 0.32332082 0.32332082 0.32332082\n",
      " 0.         0.32332082 0.32332082 0.32332082 0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.         0.         0.32332082 0.70752383\n",
      " 0.         0.         0.         0.32332082 0.         0.32332082\n",
      " 0.32332082 0.         0.32332082 0.32332082 0.93937672 0.32332082\n",
      " 0.         0.         0.32332082 0.         0.32332082 0.32332082\n",
      " 0.32332082 0.32332082 0.32332082 0.32332082]\n",
      "all_sum is: [2.74590713 2.58745394 2.58745394 2.74590713 2.58745394 2.58745394\n",
      " 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394\n",
      " 2.74590713 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394\n",
      " 2.58745394 2.74590713 2.74590713 1.40300341 2.74590713 2.58745394\n",
      " 1.40300341 2.58745394 2.58745394 2.58745394 2.74590713 2.58745394\n",
      " 1.40300341 2.58745394 2.9691121  2.58745394 2.74590713 2.58745394\n",
      " 2.58745394 2.58745394 2.58745394 1.40300341 2.58745394 2.58745394\n",
      " 2.58745394 2.58745394 2.58745394 2.58745394 2.74590713 2.58745394\n",
      " 2.58745394 1.5614566  1.40300341 2.58745394 1.40300341 2.58745394\n",
      " 2.58745394 2.58745394 2.74590713 2.58745394 2.9691121  2.58745394\n",
      " 2.58745394 2.58745394 2.58745394 2.58745394 2.74590713 1.40300341\n",
      " 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394\n",
      " 2.58745394 2.74590713 2.58745394 1.5614566  2.74590713 2.58745394\n",
      " 2.58745394 2.58745394 2.74590713 2.58745394 2.58745394 2.58745394\n",
      " 2.58745394 2.58745394 2.58745394 2.58745394 2.58745394 2.74590713\n",
      " 2.74590713 2.58745394 2.58745394 2.58745394 2.58745394 2.74590713\n",
      " 2.58745394 2.58745394 2.58745394 2.74590713]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.779\n",
      "(*) epoch 2, cost 0.734\n",
      "(*) epoch 3, cost 0.526\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.086\n",
      "(*) epoch 2, cost 1.408\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.54998309 4.54998309 6.56763364 5.55219139 5.55219139 5.55219139\n",
      " 5.47147598 6.47368428 4.54998309 5.55219139 5.47147598 6.64834905\n",
      " 4.54998309 5.55219139 4.54998309 5.08317512 5.55219139 5.47147598\n",
      " 4.54998309 4.54998309 4.54998309 5.7938345  4.54998309 5.55219139\n",
      " 5.47147598 6.47368428 4.54998309 4.69767684 5.47147598 4.54998309\n",
      " 5.55219139 4.54998309 7.00687631 6.47368428 5.55219139 4.54998309\n",
      " 5.55219139 5.55219139 6.08538342 4.54998309 4.54998309 4.54998309\n",
      " 5.55219139 5.61916973 5.08317512 5.47147598 4.69767684 5.55219139\n",
      " 5.47147598 5.47147598 5.08317512 5.64614075 6.47368428 5.55219139\n",
      " 4.54998309 4.54998309 4.54998309 4.54998309 4.54998309 5.47147598\n",
      " 7.56984194 6.64834905 5.47147598 4.54998309 5.47147598 4.54998309\n",
      " 6.47368428 4.54998309 5.55219139 4.54998309 5.47147598 5.47147598\n",
      " 5.55219139 6.47368428 4.54998309 5.47147598 4.54998309 5.47147598\n",
      " 4.69767684 5.55219139 4.54998309 6.08538342 4.54998309 5.55219139\n",
      " 4.54998309 5.47147598 5.55219139 5.64614075 5.47147598 6.47368428\n",
      " 4.54998309 4.54998309 5.55219139 4.54998309 4.54998309 4.54998309\n",
      " 4.54998309 5.55219139 5.47147598 4.54998309]\n",
      "all_sum is: [1.78750814 1.4669647  1.4669647  1.4669647  1.4669647  1.4669647\n",
      " 1.78750814 1.4669647  1.78750814 1.78750814 1.78750814 1.4669647\n",
      " 1.4669647  1.78750814 1.78750814 1.78750814 1.4669647  1.78750814\n",
      " 1.4669647  1.78750814 1.78750814 1.78750814 1.4669647  1.78750814\n",
      " 1.78750814 1.4669647  1.4669647  1.78750814 1.78750814 1.78750814\n",
      " 1.4669647  1.78750814 1.78750814 1.78750814 1.4669647  1.78750814\n",
      " 2.10357445 1.78750814 1.78750814 1.78750814 1.78750814 1.78750814\n",
      " 1.78750814 1.78750814 1.78750814 1.78750814 2.10357445 1.4669647\n",
      " 1.78750814 1.4669647  1.4669647  1.4669647  1.78750814 1.78750814\n",
      " 1.4669647  1.4669647  1.78750814 1.78750814 1.78750814 1.4669647\n",
      " 1.78750814 1.4669647  1.4669647  1.4669647  1.4669647  1.78750814\n",
      " 1.78750814 1.78750814 1.78750814 1.4669647  1.78750814 1.4669647\n",
      " 1.78750814 1.4669647  1.78750814 1.78750814 1.78750814 1.78750814\n",
      " 1.78750814 1.78750814 1.78750814 1.78750814 1.4669647  1.4669647\n",
      " 1.78750814 1.78750814 1.4669647  1.78750814 1.4669647  1.78750814\n",
      " 1.78750814 1.78750814 1.78750814 1.4669647  1.78750814 1.78750814\n",
      " 1.78750814 1.4669647  1.78750814 1.78750814]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.499\n",
      "(*) epoch 2, cost 3.227\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.497\n",
      "(*) epoch 2, cost 3.435\n",
      "(*) epoch 3, cost 2.970\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.46223225 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629 2.0462737\n",
      " 0.98160629 1.46223225 0.98160629 1.46223225 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629 2.52689966\n",
      " 2.0462737  1.46223225 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 1.46223225 0.98160629 0.98160629 0.98160629\n",
      " 1.46223225 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 2.0462737  0.98160629 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 0.98160629 1.46223225 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 0.98160629 1.46223225 0.98160629 2.0462737\n",
      " 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629 0.98160629\n",
      " 1.46223225 0.98160629 0.98160629 0.98160629 1.46223225 0.98160629\n",
      " 0.98160629 0.98160629 1.46223225 0.98160629 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 1.46223225 0.98160629 2.30213224 0.98160629\n",
      " 0.98160629 0.98160629 0.98160629 2.30213224 1.46223225 0.98160629\n",
      " 0.98160629 0.98160629 0.98160629 1.46223225 0.98160629 0.98160629\n",
      " 0.98160629 0.98160629 2.0462737  0.98160629]\n",
      "all_sum is: [0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705 0.17564705\n",
      " 0.17564705 0.17564705 0.17564705 0.17564705]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.272\n",
      "(*) epoch 2, cost 2.412\n",
      "(*) epoch 3, cost 2.015\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.883\n",
      "(*) epoch 2, cost 1.255\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.37202652 0.\n",
      " 0.37202652 0.         0.         0.37202652 0.37202652 0.\n",
      " 0.         0.80460557 0.         0.37202652 0.37202652 0.\n",
      " 1.17663209 0.37202652 2.09767432 0.         0.         0.37202652\n",
      " 0.         0.         0.37202652 0.         0.         1.17663209\n",
      " 0.         0.         0.         0.37202652 0.         0.\n",
      " 0.         0.         0.         0.37202652 0.         0.37202652\n",
      " 0.80460557 0.         0.37202652 0.80460557 0.         0.\n",
      " 0.80460557 0.         0.80460557 0.37202652 0.         0.\n",
      " 0.37202652 0.37202652 0.37202652 0.         0.         0.37202652\n",
      " 0.         0.         0.         0.37202652 0.         0.\n",
      " 0.37202652 0.37202652 0.37202652 0.37202652 1.7256478  0.\n",
      " 0.37202652 0.         0.         0.37202652 0.         0.\n",
      " 0.         0.         0.37202652 0.         0.         0.37202652\n",
      " 1.7256478  1.17663209 0.         0.         0.37202652 0.37202652\n",
      " 0.         0.37202652 0.         0.         0.         0.\n",
      " 0.80460557 0.         0.         0.37202652]\n",
      "all_sum is: [0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589 0.9479589\n",
      " 0.9479589 0.9479589]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.125\n",
      "(*) epoch 2, cost 2.679\n",
      "(*) epoch 3, cost 2.464\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.220\n",
      "(*) epoch 2, cost 2.727\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.24589863 0.24589863 0.24589863 0.24589863 0.\n",
      " 0.         0.24589863 0.         0.         0.         0.\n",
      " 0.         0.24589863 0.         0.         0.         0.24589863\n",
      " 0.24589863 0.24589863 0.         0.24589863 0.24589863 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.24589863 0.         0.         0.24589863 0.24589863 0.11832557\n",
      " 0.24589863 0.24589863 0.         0.24589863 0.24589863 0.\n",
      " 0.         0.24589863 0.24589863 0.24589863 0.24589863 0.\n",
      " 0.         0.24589863 0.24589863 0.         0.         0.24589863\n",
      " 0.         0.24589863 0.         0.         0.24589863 0.\n",
      " 0.24589863 0.24589863 0.24589863 0.24589863 0.         0.24589863\n",
      " 0.24589863 0.         0.         0.24589863 0.         0.\n",
      " 0.         0.         0.         0.24589863 0.         0.\n",
      " 0.24589863 0.         0.         0.         0.24589863 0.24589863\n",
      " 0.24589863 0.24589863 0.24589863 0.24589863 0.         0.24589863\n",
      " 0.24589863 0.         0.         0.         0.         0.\n",
      " 0.         0.24589863 0.         0.        ]\n",
      "all_sum is: [2.60762299 2.21634917 2.21634917 2.21634917 2.21634917 2.21634917\n",
      " 2.60762299 2.60762299 1.62483635 2.21634917 2.21634917 2.21634917\n",
      " 3.47072029 2.21634917 2.60762299 2.60762299 2.60762299 2.21634917\n",
      " 2.21634917 2.21634917 2.21634917 2.21634917 2.21634917 2.21634917\n",
      " 2.21634917 2.21634917 2.21634917 2.21634917 2.21634917 2.60762299\n",
      " 2.21634917 2.21634917 2.60762299 2.76388211 2.60762299 2.21634917\n",
      " 2.21634917 2.60762299 3.07944646 2.60762299 2.21634917 2.21634917\n",
      " 2.60762299 2.60762299 2.21634917 2.60762299 2.60762299 2.60762299\n",
      " 2.60762299 2.21634917 2.21634917 2.21634917 2.60762299 2.21634917\n",
      " 2.60762299 2.60762299 3.07944646 2.60762299 2.21634917 2.21634917\n",
      " 2.60762299 2.21634917 2.21634917 2.60762299 2.21634917 2.21634917\n",
      " 2.21634917 2.21634917 2.21634917 3.07944646 2.21634917 2.21634917\n",
      " 2.21634917 2.60762299 2.21634917 2.21634917 2.60762299 2.21634917\n",
      " 2.21634917 2.21634917 2.21634917 2.60762299 2.60762299 2.21634917\n",
      " 2.60762299 2.21634917 2.21634917 2.21634917 3.07944646 2.21634917\n",
      " 2.21634917 2.21634917 2.21634917 2.60762299 2.21634917 2.21634917\n",
      " 2.21634917 2.21634917 2.60762299 2.21634917]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.054\n",
      "(*) epoch 2, cost 3.988\n",
      "(*) epoch 3, cost 3.701\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.993\n",
      "(*) epoch 2, cost 1.308\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.83371932 1.49664084 1.49664084 1.83371932 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.83371932\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.83371932 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.83371932 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.83371932\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.83371932 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.83371932 1.49664084\n",
      " 1.49664084 1.83371932 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.49664084 1.83371932 1.49664084 1.49664084\n",
      " 1.49664084 1.49664084 1.83371932 1.49664084]\n",
      "all_sum is: [0.35739879 0.35739879 0.         0.         0.         1.01174422\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.         0.35739879 0.         0.35739879 0.35739879\n",
      " 0.         0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879 0.35739879\n",
      " 0.35739879 0.35739879 0.35739879 0.35739879]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.604\n",
      "(*) epoch 2, cost 1.600\n",
      "(*) epoch 3, cost 1.276\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.877\n",
      "(*) epoch 2, cost 0.488\n",
      "(*) epoch 3, cost 0.332\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.89027682 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.09718774 2.89027682 2.89027682 2.09718774 2.89027682 2.09718774\n",
      " 2.89027682 2.89027682 2.09718774 2.09718774 2.89027682 2.89027682\n",
      " 2.09718774 2.09718774 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.09718774 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.09718774 2.09718774\n",
      " 2.89027682 2.89027682 2.09718774 2.89027682 2.89027682 2.09718774\n",
      " 2.09718774 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.89027682 2.89027682 2.09718774 2.89027682 2.89027682 2.09718774\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682 2.89027682\n",
      " 2.09718774 2.89027682 2.89027682 2.89027682 2.09718774 2.09718774\n",
      " 2.89027682 2.89027682 2.89027682 2.89027682 2.09718774 2.89027682\n",
      " 2.89027682 2.89027682 2.89027682 2.09718774 2.09718774 2.09718774\n",
      " 2.89027682 2.89027682 2.89027682 2.09718774]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         2.22761184 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         2.22761184 0.         0.         0.\n",
      " 0.         0.         0.         0.         4.25982323 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 2.22761184 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.533\n",
      "(*) epoch 2, cost 0.870\n",
      "(*) epoch 3, cost 0.660\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.218\n",
      "(*) epoch 2, cost 0.599\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.40901956 2.09017668 3.66316966 2.09017668 3.66316966 2.09017668\n",
      " 2.09017668 2.09017668 3.66316966 2.09017668 2.09017668 1.40901956\n",
      " 2.09017668 2.09017668 2.56615161 1.40901956 2.09017668 1.40901956\n",
      " 2.98201254 2.09017668 2.09017668 2.09017668 2.09017668 3.24730874\n",
      " 2.09017668 1.40901956 2.09017668 2.98201254 1.40901956 2.09017668\n",
      " 2.09017668 1.40901956 3.66316966 2.09017668 2.09017668 2.09017668\n",
      " 1.40901956 2.09017668 2.09017668 2.98201254 3.24730874 1.40901956\n",
      " 2.09017668 1.40901956 3.66316966 3.66316966 2.09017668 2.09017668\n",
      " 1.40901956 0.68115712 2.09017668 2.09017668 2.09017668 2.09017668\n",
      " 3.66316966 2.09017668 2.09017668 2.09017668 2.09017668 2.98201254\n",
      " 2.09017668 2.09017668 2.09017668 2.09017668 2.09017668 2.09017668\n",
      " 2.09017668 1.40901956 2.09017668 2.09017668 0.68115712 2.09017668\n",
      " 2.09017668 1.40901956 3.66316966 2.09017668 2.09017668 2.09017668\n",
      " 2.09017668 2.09017668 3.24730874 2.09017668 2.09017668 1.40901956\n",
      " 2.09017668 3.66316966 2.09017668 0.68115712 2.09017668 2.09017668\n",
      " 3.66316966 2.09017668 1.40901956 2.09017668 1.40901956 3.66316966\n",
      " 3.66316966 2.56615161 2.09017668 3.66316966]\n",
      "all_sum is: [4.91098313 4.58626489 4.91098313 5.82825926 4.58626489 4.91098313\n",
      " 4.91098313 4.91098313 4.91098313 4.91098313 4.91098313 4.91098313\n",
      " 5.82825926 3.66898876 3.66898876 3.66898876 4.91098313 4.91098313\n",
      " 5.49124911 5.82825926 3.66898876 4.91098313 3.66898876 4.91098313\n",
      " 3.66898876 4.24925474 4.58626489 4.91098313 4.91098313 3.66898876\n",
      " 4.91098313 3.66898876 3.66898876 4.91098313 4.91098313 5.82825926\n",
      " 3.66898876 4.91098313 3.66898876 5.82825926 5.82825926 3.66898876\n",
      " 4.58626489 4.91098313 5.82825926 4.91098313 5.82825926 4.91098313\n",
      " 5.82825926 4.91098313 4.91098313 4.91098313 3.66898876 5.82825926\n",
      " 4.91098313 5.82825926 4.58626489 4.91098313 3.66898876 4.91098313\n",
      " 4.58626489 4.91098313 5.82825926 3.66898876 4.91098313 4.58626489\n",
      " 4.91098313 3.66898876 4.91098313 5.82825926 4.91098313 4.58626489\n",
      " 4.91098313 4.91098313 4.91098313 4.91098313 5.82825926 4.91098313\n",
      " 4.91098313 4.91098313 3.66898876 5.82825926 4.91098313 4.91098313\n",
      " 3.66898876 4.91098313 4.91098313 4.27369814 4.91098313 4.91098313\n",
      " 3.66898876 4.91098313 3.66898876 4.91098313 3.66898876 3.66898876\n",
      " 3.66898876 4.91098313 5.82825926 5.82825926]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.696\n",
      "(*) epoch 2, cost 3.364\n",
      "(*) epoch 3, cost 2.962\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.766\n",
      "(*) epoch 2, cost 2.751\n",
      "(*) epoch 3, cost 2.617\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.92475879 0.70401007 0.92475879 0.92475879 0.         0.92475879\n",
      " 1.62876886 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.         0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 1.62876886 0.92475879 0.92475879 1.62876886 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 0.92475879 1.62876886 0.92475879 1.62876886 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 1.62876886 0.92475879 0.92475879 1.62876886\n",
      " 1.62876886 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 0.92475879 1.62876886\n",
      " 0.92475879 1.62876886 0.92475879 0.92475879 0.92475879 1.62876886\n",
      " 0.92475879 0.92475879 0.92475879 0.92475879 1.62876886 1.62876886\n",
      " 0.92475879 1.62876886 1.62876886 1.62876886 0.92475879 0.92475879\n",
      " 0.92475879 0.92475879 1.62876886 0.92475879 0.92475879 1.62876886\n",
      " 0.92475879 0.92475879 1.62876886 0.92475879]\n",
      "all_sum is: [3.34617018 3.34617018 1.89129008 1.89129008 3.34617018 1.89129008\n",
      " 1.89129008 1.89129008 1.89129008 3.34617018 3.34617018 3.34617018\n",
      " 1.89129008 3.34617018 1.89129008 3.34617018 1.89129008 3.34617018\n",
      " 3.34617018 1.89129008 1.89129008 1.89129008 3.34617018 1.89129008\n",
      " 3.38690211 1.89129008 1.89129008 1.89129008 3.34617018 1.89129008\n",
      " 1.89129008 3.34617018 1.89129008 3.66310087 1.89129008 3.34617018\n",
      " 1.89129008 1.89129008 1.89129008 1.89129008 1.89129008 3.34617018\n",
      " 1.89129008 3.34617018 1.89129008 3.34617018 1.89129008 1.89129008\n",
      " 3.34617018 1.89129008 3.34617018 1.89129008 3.66310087 3.34617018\n",
      " 3.34617018 1.89129008 1.89129008 3.34617018 1.89129008 1.89129008\n",
      " 3.34617018 1.89129008 1.89129008 3.34617018 3.34617018 1.89129008\n",
      " 1.89129008 1.89129008 1.89129008 3.34617018 1.89129008 1.89129008\n",
      " 1.89129008 3.34617018 1.89129008 3.34617018 1.89129008 1.89129008\n",
      " 1.89129008 3.34617018 1.89129008 1.89129008 3.34617018 3.34617018\n",
      " 1.89129008 3.34617018 1.89129008 3.34617018 3.34617018 3.34617018\n",
      " 3.34617018 3.34617018 1.89129008 3.34617018 1.89129008 1.89129008\n",
      " 1.89129008 1.89129008 3.34617018 1.89129008]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.166\n",
      "(*) epoch 2, cost 2.565\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.850\n",
      "(*) epoch 2, cost 1.311\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.92701418 3.41440939 3.41440939 3.41440939 1.92701418 1.92701418\n",
      " 1.92701418 3.41440939 1.92701418 1.92701418 1.92701418 3.41440939\n",
      " 1.92701418 1.92701418 3.41440939 3.41440939 3.41440939 3.41440939\n",
      " 3.41440939 3.41440939 4.08716663 4.08716663 3.41440939 3.41440939\n",
      " 2.7285872  3.41440939 2.7285872  3.41440939 1.92701418 1.92701418\n",
      " 3.41440939 1.92701418 1.92701418 3.41440939 3.41440939 1.92701418\n",
      " 1.92701418 3.41440939 3.41440939 3.41440939 1.2661196  3.41440939\n",
      " 3.41440939 3.41440939 1.92701418 3.41440939 1.92701418 1.92701418\n",
      " 3.41440939 1.92701418 3.41440939 3.41440939 3.41440939 3.41440939\n",
      " 3.41440939 1.92701418 3.41440939 1.92701418 3.41440939 3.41440939\n",
      " 3.41440939 3.41440939 3.41440939 1.92701418 3.41440939 3.41440939\n",
      " 1.92701418 3.41440939 3.41440939 1.92701418 3.41440939 1.92701418\n",
      " 3.41440939 3.41440939 3.41440939 3.41440939 3.41440939 3.41440939\n",
      " 3.41440939 3.41440939 3.41440939 3.41440939 3.41440939 3.41440939\n",
      " 3.41440939 2.75351481 3.41440939 3.41440939 2.59977142 1.92701418\n",
      " 3.41440939 3.41440939 1.92701418 1.92701418 3.41440939 1.92701418\n",
      " 1.92701418 4.30120002 3.41440939 3.41440939]\n",
      "all_sum is: [2.1282181  2.43424817 2.1282181  2.1282181  2.68420369 2.43424817\n",
      " 2.1282181  2.1282181  2.68420369 2.43424817 2.1282181  2.37817363\n",
      " 2.37817363 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181\n",
      " 2.1282181  2.1282181  2.1282181  2.1282181  2.37817363 2.43424817\n",
      " 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181  2.1282181\n",
      " 2.43424817 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181\n",
      " 2.1282181  2.1282181  2.43424817 2.1282181  2.1282181  2.1282181\n",
      " 2.43424817 2.43424817 2.1282181  2.37817363 2.37817363 2.1282181\n",
      " 2.1282181  2.1282181  2.1282181  2.1282181  2.68420369 2.1282181\n",
      " 2.32960327 2.43424817 2.1282181  2.1282181  2.37817363 2.43424817\n",
      " 2.43424817 2.1282181  2.1282181  2.1282181  2.43424817 2.1282181\n",
      " 2.1282181  2.37817363 2.1282181  2.43424817 2.43424817 2.37817363\n",
      " 2.43424817 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181\n",
      " 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181  2.43424817\n",
      " 2.1282181  2.1282181  2.1282181  2.1282181  2.1282181  2.1282181\n",
      " 2.43424817 2.1282181  2.68420369 2.68420369 2.1282181  2.68420369\n",
      " 2.43424817 2.1282181  2.37817363 2.1282181 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.102\n",
      "(*) epoch 2, cost 2.718\n",
      "(*) epoch 3, cost 2.444\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.988\n",
      "(*) epoch 2, cost 3.562\n",
      "(*) epoch 3, cost 3.317\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.32567013 0.         0.46718095 0.46718095 0.         0.\n",
      " 0.46718095 1.85848918 0.         0.46718095 0.         0.\n",
      " 0.46718095 0.46718095 0.46718095 0.46718095 2.32567013 0.\n",
      " 0.         0.46718095 0.         0.         0.46718095 0.\n",
      " 0.46718095 0.46718095 0.46718095 0.46718095 0.46718095 0.\n",
      " 0.46718095 0.46718095 0.46718095 0.46718095 0.         0.46718095\n",
      " 0.46718095 0.46718095 0.46718095 0.46718095 0.46718095 0.46718095\n",
      " 0.         0.         0.46718095 0.         0.46718095 0.\n",
      " 0.         0.46718095 0.         0.         0.46718095 0.46718095\n",
      " 0.46718095 0.         0.46718095 0.46718095 0.46718095 0.46718095\n",
      " 0.46718095 0.46718095 0.46718095 0.46718095 0.46718095 0.46718095\n",
      " 0.46718095 0.         0.46718095 0.46718095 0.         0.46718095\n",
      " 0.46718095 0.         0.         0.46718095 0.         0.\n",
      " 0.46718095 0.         0.46718095 0.46718095 0.46718095 0.46718095\n",
      " 0.         0.46718095 0.46718095 0.46718095 0.         0.\n",
      " 0.         0.         0.         0.         1.85848918 0.46718095\n",
      " 0.46718095 0.         0.46718095 0.        ]\n",
      "all_sum is: [0.09801652 0.         0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.         0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.         0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.         0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.         0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.         0.09801652 0.09801652 0.09801652 0.09801652 0.\n",
      " 0.09801652 0.         0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652 0.09801652\n",
      " 0.         0.09801652 0.09801652 0.09801652]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.285\n",
      "(*) epoch 2, cost 2.182\n",
      "(*) epoch 3, cost 1.712\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.805\n",
      "(*) epoch 2, cost 1.219\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.25124343 1.39310386 0.44020815 1.25124343 1.25124343 1.25124343\n",
      " 1.25124343 1.25124343 0.58206858 1.25124343 1.25124343 1.25124343\n",
      " 0.44020815 1.25124343 0.44020815 1.25124343 1.25124343 1.25124343\n",
      " 0.44020815 1.25124343 1.25124343 1.25124343 0.44020815 1.25124343\n",
      " 1.25124343 1.39310386 1.25124343 1.25124343 0.44020815 1.25124343\n",
      " 1.39310386 0.44020815 0.44020815 1.39310386 1.25124343 1.25124343\n",
      " 1.39310386 1.25124343 0.44020815 1.25124343 0.44020815 0.44020815\n",
      " 1.25124343 0.44020815 1.25124343 1.25124343 0.44020815 1.25124343\n",
      " 1.25124343 0.44020815 1.25124343 0.58206858 1.25124343 1.25124343\n",
      " 1.25124343 1.25124343 1.25124343 1.25124343 1.25124343 1.25124343\n",
      " 1.39310386 1.25124343 1.25124343 1.25124343 0.44020815 0.44020815\n",
      " 1.25124343 1.25124343 1.25124343 1.25124343 1.25124343 1.39310386\n",
      " 1.25124343 1.25124343 0.44020815 1.25124343 1.25124343 1.25124343\n",
      " 0.44020815 1.25124343 1.25124343 1.25124343 1.39310386 1.25124343\n",
      " 1.25124343 0.44020815 1.25124343 1.25124343 0.44020815 1.25124343\n",
      " 1.25124343 1.25124343 1.25124343 1.39310386 0.44020815 1.25124343\n",
      " 0.44020815 0.44020815 1.25124343 1.25124343]\n",
      "all_sum is: [0.41390905 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 1.08961294 1.0727184  0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.         0.41390905\n",
      " 0.41390905 0.         0.         0.41390905 0.         0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905\n",
      " 0.41390905 0.41390905 0.         0.         0.81126413 0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.81126413 0.41390905\n",
      " 0.         0.41390905 0.41390905 0.41390905 0.41390905 0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.         0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.81126413 0.41390905\n",
      " 0.41390905 0.41390905 0.         0.         0.         0.41390905\n",
      " 0.81126413 0.41390905 0.41390905 0.41390905 0.41390905 0.\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905 0.41390905 0.\n",
      " 0.41390905 0.41390905 1.08961294 0.41390905 0.81126413 0.41390905\n",
      " 0.41390905 0.         0.41390905 0.39735507 0.41390905 0.41390905\n",
      " 0.41390905 0.41390905 0.41390905 0.41390905]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.201\n",
      "(*) epoch 2, cost 3.021\n",
      "(*) epoch 3, cost 2.904\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.184\n",
      "(*) epoch 2, cost 2.276\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.18727114 1.18727114 1.18727114 1.52784936 1.52784936 1.52784936\n",
      " 1.52784936 1.18727114 1.18727114 1.52784936 1.18727114 1.18727114\n",
      " 1.18727114 1.18727114 2.13240112 2.47297935 1.52784936 1.18727114\n",
      " 1.18727114 1.18727114 1.18727114 1.52784936 1.18727114 1.52784936\n",
      " 1.18727114 1.18727114 1.18727114 1.18727114 1.52784936 1.18727114\n",
      " 1.52784936 1.18727114 1.18727114 1.18727114 2.13240112 1.52784936\n",
      " 1.18727114 1.18727114 1.18727114 1.18727114 1.18727114 1.52784936\n",
      " 1.18727114 1.18727114 1.52784936 1.52784936 1.18727114 1.52784936\n",
      " 1.18727114 1.18727114 1.18727114 1.18727114 1.18727114 1.52784936\n",
      " 1.18727114 1.52784936 1.52784936 1.18727114 1.52784936 1.18727114\n",
      " 1.18727114 1.52784936 1.18727114 1.18727114 1.18727114 1.52784936\n",
      " 1.18727114 1.18727114 1.52784936 1.52784936 1.18727114 1.18727114\n",
      " 1.18727114 1.18727114 1.18727114 1.18727114 1.18727114 1.52784936\n",
      " 1.18727114 1.18727114 1.18727114 1.52784936 1.18727114 1.18727114\n",
      " 1.18727114 1.18727114 1.52784936 1.52784936 1.52784936 1.18727114\n",
      " 1.52784936 1.52784936 1.18727114 1.18727114 1.18727114 1.18727114\n",
      " 1.18727114 1.52784936 1.18727114 1.18727114]\n",
      "all_sum is: [1.41505984 2.86816377 2.86816377 2.28697171 2.86816377 2.28697171\n",
      " 2.28697171 2.86816377 2.86816377 1.41505984 1.41505984 2.28697171\n",
      " 2.86816377 2.28697171 2.86816377 1.41505984 2.86816377 2.86816377\n",
      " 2.28697171 2.86816377 2.28697171 2.86816377 2.86816377 2.86816377\n",
      " 2.86816377 2.86816377 2.86816377 2.86816377 2.86816377 2.28697171\n",
      " 2.86816377 2.28697171 2.86816377 2.86816377 2.28697171 2.86816377\n",
      " 1.41505984 2.86816377 2.28697171 2.86816377 2.86816377 2.86816377\n",
      " 1.41505984 2.86816377 2.86816377 2.28697171 2.86816377 1.41505984\n",
      " 2.28697171 2.86816377 1.41505984 0.83386779 2.86816377 2.86816377\n",
      " 2.86816377 2.86816377 2.86816377 2.28697171 2.28697171 2.86816377\n",
      " 2.86816377 2.86816377 2.28697171 2.86816377 2.28697171 2.28697171\n",
      " 2.86816377 2.28697171 4.05184941 2.86816377 2.86816377 0.83386779\n",
      " 1.41505984 4.05184941 1.41505984 2.28697171 2.86816377 2.86816377\n",
      " 2.86816377 2.28697171 2.86816377 2.28697171 2.86816377 2.28697171\n",
      " 2.86816377 2.86816377 2.86816377 2.86816377 2.86816377 2.86816377\n",
      " 2.86816377 2.86816377 1.41505984 2.28697171 2.86816377 2.86816377\n",
      " 2.86816377 2.28697171 1.41505984 2.86816377]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.364\n",
      "(*) epoch 2, cost 1.916\n",
      "(*) epoch 3, cost 1.539\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.062\n",
      "(*) epoch 2, cost 2.290\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373 0.47096373\n",
      " 0.47096373 0.47096373 0.47096373 0.47096373]\n",
      "all_sum is: [0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 3.54567743 0.99147664\n",
      " 0.99147664 3.54567743 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 3.38440909\n",
      " 0.99147664 3.54567743 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 3.38440909 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 3.38440909 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 3.38440909 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 3.38440909 0.99147664 0.99147664 0.99147664 0.99147664\n",
      " 0.99147664 0.99147664 0.99147664 0.99147664]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.658\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.781\n",
      "(*) epoch 2, cost 1.343\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.06441994 0.06441994 0.06441994 0.06441994 0.06441994 0.32627459\n",
      " 0.06441994 0.06441994 2.25702713 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.32627459 0.06441994 0.06441994 0.06441994 0.32627459\n",
      " 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.32627459 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.32627459 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.32627459 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.32627459 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.32627459 0.06441994 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.32627459 0.32627459 0.06441994 0.06441994 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.06441994 0.06441994 0.\n",
      " 0.32627459 0.32627459 0.06441994 0.32627459 0.06441994 0.06441994\n",
      " 0.06441994 0.06441994 0.32627459 0.06441994 0.32627459 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.32627459 0.32627459 0.06441994\n",
      " 0.06441994 0.06441994 0.06441994 0.06441994]\n",
      "all_sum is: [3.54856411 3.54856411 3.54856411 3.54856411 4.21619438 4.21619438\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 4.21619438\n",
      " 4.26676254 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 4.21619438 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 4.21619438 3.54856411 4.21619438\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 4.26676254 3.54856411 3.54856411 4.21619438 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 4.21619438 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 3.54856411 3.54856411 4.21619438 3.54856411 3.54856411\n",
      " 3.54856411 4.26676254 3.54856411 3.54856411 3.54856411 4.26676254\n",
      " 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411 3.54856411\n",
      " 3.54856411 4.93439281 4.21619438 3.54856411]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.766\n",
      "(*) epoch 2, cost 3.193\n",
      "(*) epoch 3, cost 2.716\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.295\n",
      "(*) epoch 2, cost 2.297\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.134768   2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.134768\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338 2.74287338\n",
      " 2.74287338 2.74287338 2.74287338 2.74287338]\n",
      "all_sum is: [2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 3.08728113 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 1.73223246 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 3.08728113 2.88913999 2.88913999 2.88913999 3.08728113 2.88913999\n",
      " 1.9303736  2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 3.08728113 2.88913999\n",
      " 2.88913999 3.08728113 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 3.08728113\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999 2.88913999\n",
      " 2.88913999 2.88913999 2.88913999 2.88913999]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.885\n",
      "(*) epoch 2, cost 1.909\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.547\n",
      "(*) epoch 2, cost 2.563\n",
      "(*) epoch 3, cost 2.282\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.30 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.99463018 1.99463018 1.99463018 3.92751278 1.99463018 1.99463018\n",
      " 2.53009565 1.99463018 1.99463018 1.99463018 2.53009565 1.99463018\n",
      " 2.53009565 1.99463018 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 2.53009565 2.53009565 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 2.53009565 1.99463018 2.53009565 2.53009565 2.53009565\n",
      " 1.99463018 1.99463018 2.53009565 2.53009565 1.99463018 1.99463018\n",
      " 1.99463018 3.13600141 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 2.53009565 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 2.53009565 1.99463018 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 1.99463018 2.53009565 1.99463018 1.99463018 2.53009565\n",
      " 1.99463018 1.99463018 2.53009565 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 1.99463018 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 2.53009565 1.99463018 2.53009565 1.99463018 2.53009565 2.53009565\n",
      " 1.99463018 1.99463018 1.99463018 1.99463018 1.99463018 2.53009565\n",
      " 1.99463018 2.53009565 1.99463018 1.99463018 1.99463018 1.99463018\n",
      " 2.53009565 2.53009565 2.53009565 1.99463018 1.99463018 1.99463018\n",
      " 1.99463018 1.99463018 1.99463018 1.99463018]\n",
      "all_sum is: [1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.82922421 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 2.95579803 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 2.95579803 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 2.95579803 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 2.95579803 2.95579803\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 2.95579803\n",
      " 1.82922421 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 2.95579803 1.38699205 2.95579803 1.38699205\n",
      " 1.38699205 2.95579803 1.38699205 1.38699205 2.95579803 1.38699205\n",
      " 1.38699205 1.38699205 2.95579803 2.95579803 2.95579803 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205 1.38699205\n",
      " 1.38699205 1.38699205 1.38699205 1.82922421]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.341\n",
      "(*) epoch 2, cost 0.928\n",
      "(*) epoch 3, cost 0.731\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 2\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 0.191\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00675617 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.53668321 0.7822999  0.38312184 1.89433124 2.24953649 1.89433124\n",
      " 1.49515318 1.89433124 2.24953649 0.38312184 1.13750515 2.24953649\n",
      " 1.89433124 1.49515318 1.13750515 1.89433124 1.49515318 2.64871455\n",
      " 1.89433124 0.7822999  1.89433124 1.89433124 1.53668321 0.7822999\n",
      " 0.7822999  1.89433124 0.7822999  1.89433124 1.49515318 2.64871455\n",
      " 2.24953649 1.49515318 1.49515318 0.38312184 2.64871455 1.89433124\n",
      " 2.24953649 1.49515318 1.89433124 0.7822999  0.38312184 1.89433124\n",
      " 1.13750515 1.49515318 1.49515318 1.89433124 2.24953649 2.64871455\n",
      " 1.89433124 1.89433124 0.7822999  1.89433124 1.53668321 0.7822999\n",
      " 1.89433124 1.89433124 1.89433124 1.49515318 2.64871455 0.7822999\n",
      " 2.64871455 1.13750515 1.89433124 2.24953649 1.89433124 0.7822999\n",
      " 1.49515318 2.64871455 1.49515318 1.49515318 1.49515318 2.64871455\n",
      " 1.89433124 1.49515318 1.53668321 0.7822999  0.7822999  1.89433124\n",
      " 1.49515318 1.49515318 1.89433124 2.64871455 1.49515318 1.89433124\n",
      " 1.49515318 1.89433124 2.64871455 1.89433124 0.38312184 1.49515318\n",
      " 1.89433124 1.49515318 1.89433124 1.89433124 1.89433124 1.49515318\n",
      " 1.89433124 2.64871455 1.89433124 1.49515318]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 1.730\n",
      "(*) epoch 2, cost 0.392\n",
      "(*) epoch 3, cost 0.310\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.691\n",
      "(*) epoch 2, cost 2.764\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.6001941  1.78535576 2.6001941  2.6001941  1.78535576 0.84151236\n",
      " 1.78535576 1.78535576 1.78535576 1.78535576 2.6001941  1.78535576\n",
      " 1.78535576 2.6001941  2.6001941  2.6001941  1.78535576 2.6001941\n",
      " 1.78535576 2.6001941  2.6001941  1.78535576 1.78535576 2.6001941\n",
      " 1.78535576 2.6001941  2.6001941  1.78535576 1.78535576 2.6001941\n",
      " 1.78535576 2.6001941  1.78535576 1.78535576 1.78535576 2.6001941\n",
      " 1.78535576 1.78535576 1.78535576 1.78535576 1.78535576 2.6001941\n",
      " 1.78535576 2.6001941  2.6001941  1.78535576 1.78535576 2.6001941\n",
      " 0.84151236 1.78535576 2.6001941  2.6001941  1.78535576 1.78535576\n",
      " 1.78535576 0.02667403 2.6001941  2.6001941  2.6001941  1.78535576\n",
      " 1.78535576 2.6001941  0.84151236 1.78535576 1.78535576 2.6001941\n",
      " 1.78535576 1.78535576 1.78535576 1.78535576 1.78535576 1.78535576\n",
      " 1.78535576 1.78535576 1.78535576 2.6001941  1.78535576 2.6001941\n",
      " 2.6001941  2.6001941  2.6001941  1.78535576 2.6001941  0.02667403\n",
      " 2.6001941  1.78535576 2.6001941  1.78535576 1.78535576 1.78535576\n",
      " 1.78535576 2.6001941  1.78535576 2.6001941  2.6001941  1.78535576\n",
      " 2.66303056 2.66303056 2.6001941  1.78535576]\n",
      "all_sum is: [1.38485979 1.38485979 1.38485979 1.38485979 2.00283479 2.6209609\n",
      " 1.38485979 1.38485979 1.38485979 2.00283479 1.38485979 1.38485979\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 1.38485979 1.38485979 2.00283479 2.6209609  1.38485979 1.38485979\n",
      " 1.38485979 1.38485979 2.6209609  1.38485979 2.00283479 1.38485979\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979 2.6209609  1.38485979\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 2.6209609  1.38485979 1.38485979 1.38485979 1.38485979 2.6209609\n",
      " 1.38485979 1.38485979 1.38485979 2.6209609  2.6209609  1.38485979\n",
      " 2.6209609  1.38485979 1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979 2.6209609\n",
      " 2.6209609  1.38485979 1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 2.6209609  1.38485979 1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 1.38485979 2.6209609  1.38485979 1.38485979 1.38485979 1.38485979\n",
      " 2.00283479 2.6209609  2.00283479 1.38485979 1.38485979 2.6209609\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979 1.38485979 2.6209609\n",
      " 1.38485979 1.38485979 1.38485979 1.38485979]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.098\n",
      "(*) epoch 2, cost 1.274\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.299\n",
      "(*) epoch 2, cost 2.018\n",
      "(*) epoch 3, cost 1.691\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.11460978 0.11460978 0.11460978 0.11460978 1.11096074 0.11460978\n",
      " 1.11096074 0.11460978 0.11460978 0.11460978 1.11096074 1.11096074\n",
      " 0.11460978 0.11460978 0.11460978 0.11460978 1.11096074 0.11460978\n",
      " 0.11460978 1.11096074 0.11460978 1.11096074 0.11460978 0.11460978\n",
      " 1.11096074 1.11096074 1.11096074 0.11460978 0.11460978 0.11460978\n",
      " 1.11096074 0.11460978 0.11460978 0.11460978 0.11460978 1.11096074\n",
      " 1.11096074 1.11096074 1.11096074 0.11460978 1.11096074 0.11460978\n",
      " 1.11096074 1.11096074 1.11096074 1.11096074 1.11096074 0.11460978\n",
      " 1.11096074 0.11460978 0.11460978 1.11096074 1.11096074 0.11460978\n",
      " 0.11460978 0.11460978 1.11096074 0.11460978 0.11460978 1.11096074\n",
      " 1.11096074 0.11460978 0.11460978 0.11460978 0.11460978 0.11460978\n",
      " 0.11460978 0.11460978 0.11460978 0.11460978 1.11096074 1.11096074\n",
      " 0.11460978 0.11460978 0.11460978 0.11460978 1.11096074 0.11460978\n",
      " 0.11460978 0.11460978 1.11096074 0.11460978 0.11460978 1.11096074\n",
      " 0.11460978 0.11460978 0.11460978 1.11096074 1.11096074 0.11460978\n",
      " 0.11460978 0.11460978 1.11096074 0.11460978 0.11460978 1.11096074\n",
      " 0.11460978 1.11096074 0.11460978 1.11096074]\n",
      "all_sum is: [3.09809351 3.09809351 1.655943   1.655943   3.09809351 1.655943\n",
      " 1.655943   3.09809351 1.655943   3.09809351 1.655943   1.655943\n",
      " 3.09809351 1.655943   3.09809351 1.655943   3.09809351 3.09809351\n",
      " 3.09809351 3.09809351 1.78414178 1.655943   3.09809351 1.655943\n",
      " 3.09809351 1.655943   3.09809351 1.655943   3.2262923  3.09809351\n",
      " 1.655943   3.09809351 1.655943   3.09809351 1.655943   3.2262923\n",
      " 1.655943   3.09809351 1.655943   1.655943   3.09809351 1.655943\n",
      " 1.655943   1.655943   3.2262923  1.655943   3.09809351 3.09809351\n",
      " 3.09809351 1.655943   3.09809351 3.09809351 3.09809351 3.09809351\n",
      " 3.09809351 1.655943   3.2262923  3.09809351 1.655943   3.09809351\n",
      " 1.655943   1.78414178 3.09809351 3.09809351 3.09809351 3.09809351\n",
      " 1.78414178 3.09809351 3.2262923  1.78414178 1.78414178 3.09809351\n",
      " 1.655943   1.655943   3.09809351 1.655943   1.655943   1.655943\n",
      " 3.09809351 1.655943   1.655943   1.78414178 3.2262923  3.09809351\n",
      " 1.655943   3.09809351 1.655943   3.09809351 3.09809351 3.09809351\n",
      " 3.09809351 3.09809351 3.09809351 3.09809351 3.09809351 1.655943\n",
      " 3.09809351 3.09809351 1.655943   1.655943  ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.762\n",
      "(*) epoch 2, cost 3.483\n",
      "(*) epoch 3, cost 3.122\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.403\n",
      "(*) epoch 2, cost 2.622\n",
      "(*) epoch 3, cost 2.147\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.46531342 1.15043508 1.15043508 2.0245799  1.15043508 1.15043508\n",
      " 1.15043508 1.15043508 1.15043508 1.15043508 1.15043508 0.\n",
      " 1.15043508 0.78727699 1.15043508 0.         0.78727699 0.78727699\n",
      " 1.15043508 0.36315809 1.15043508 1.15043508 0.78727699 0.78727699\n",
      " 1.15043508 1.15043508 0.78727699 1.15043508 1.15043508 0.78727699\n",
      " 0.36315809 1.15043508 0.78727699 0.36315809 0.78727699 2.0245799\n",
      " 0.78727699 1.15043508 1.15043508 1.15043508 0.78727699 0.36315809\n",
      " 1.15043508 0.78727699 1.15043508 0.78727699 2.17256978 0.36315809\n",
      " 0.36315809 1.15043508 1.15043508 0.36315809 1.15043508 1.15043508\n",
      " 1.15043508 1.15043508 0.78727699 1.15043508 1.15043508 1.24129261\n",
      " 0.87414483 1.15043508 1.15043508 1.10215534 1.46531342 1.15043508\n",
      " 1.15043508 1.15043508 1.15043508 0.36315809 2.0245799  0.78727699\n",
      " 0.36315809 1.15043508 1.15043508 0.78727699 0.78727699 1.15043508\n",
      " 1.15043508 1.15043508 0.78727699 0.36315809 1.15043508 1.15043508\n",
      " 1.15043508 1.15043508 0.78727699 1.15043508 0.78727699 0.78727699\n",
      " 1.15043508 1.15043508 1.15043508 0.78727699 0.36315809 0.36315809\n",
      " 1.15043508 0.78727699 0.36315809 0.78727699]\n",
      "all_sum is: [1.43045431 1.55546831 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 0.33353193 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 0.33353193 1.43045431 1.43045431 1.43045431 1.43045431 0.33353193\n",
      " 0.40348755 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431 1.55546831\n",
      " 1.43045431 1.43045431 0.33353193 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 0.33353193 1.43045431\n",
      " 1.50040993 1.43045431 0.33353193 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 0.33353193 1.43045431 0.45854593 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 0.33353193 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 0.33353193 1.43045431 1.43045431\n",
      " 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431 1.43045431\n",
      " 1.43045431 0.33353193 1.43045431 1.43045431 0.33353193 1.43045431\n",
      " 0.33353193 1.43045431 1.43045431 1.43045431]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.248\n",
      "(*) epoch 2, cost 2.428\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.145\n",
      "(*) epoch 2, cost 4.541\n",
      "(*) epoch 3, cost 4.227\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.18743564 3.18743564 2.30928567 3.18743564 3.18743564 2.30928567\n",
      " 2.43343601 1.95817981 2.30928567 4.30371647 2.30928567 2.30928567\n",
      " 3.18743564 2.30928567 2.30928567 2.30928567 2.30928567 2.30928567\n",
      " 2.30928567 2.30928567 2.30928567 2.30928567 2.30928567 3.18743564\n",
      " 2.30928567 3.18743564 2.30928567 3.31158598 2.30928567 2.30928567\n",
      " 3.18743564 2.30928567 2.30928567 2.83632978 2.30928567 4.30371647\n",
      " 3.18743564 3.18743564 3.18743564 3.18743564 3.18743564 2.30928567\n",
      " 3.18743564 3.18743564 2.30928567 3.18743564 3.18743564 3.18743564\n",
      " 3.18743564 2.30928567 3.18743564 3.4255665  3.4255665  2.30928567\n",
      " 3.18743564 2.30928567 3.4255665  2.30928567 2.30928567 2.30928567\n",
      " 3.18743564 2.30928567 3.18743564 2.30928567 3.18743564 4.30371647\n",
      " 4.30371647 2.30928567 2.30928567 3.18743564 3.31158598 3.18743564\n",
      " 2.30928567 3.18743564 2.30928567 3.18743564 2.30928567 2.30928567\n",
      " 3.18743564 2.43343601 2.30928567 2.30928567 3.18743564 3.18743564\n",
      " 3.18743564 3.54971684 2.30928567 3.31158598 3.4255665  3.18743564\n",
      " 3.18743564 2.30928567 2.30928567 2.30928567 3.18743564 2.30928567\n",
      " 2.30928567 3.18743564 3.18743564 3.18743564]\n",
      "all_sum is: [1.16486741 1.16486741 1.16486741 0.97869876 1.16486741 1.16486741\n",
      " 1.16486741 1.16486741 1.16486741 1.16486741 0.56639646 1.16486741\n",
      " 0.97161741 1.16486741 1.16486741 0.56639646 1.16486741 1.16486741\n",
      " 1.16486741 1.16486741 1.16486741 1.16486741 1.16486741 0.56639646\n",
      " 1.16486741 0.56639646 0.56639646 1.16486741 1.16486741 1.16486741\n",
      " 1.16486741 1.16486741 0.56639646 0.97161741 1.16486741 0.56639646\n",
      " 1.16486741 0.56639646 1.16486741 1.5771697  0.56639646 1.16486741\n",
      " 0.56639646 1.16486741 0.56639646 1.16486741 0.97161741 1.16486741\n",
      " 0.56639646 0.56639646 0.56639646 1.16486741 1.16486741 1.16486741\n",
      " 0.56639646 0.56639646 0.56639646 1.16486741 0.56639646 0.56639646\n",
      " 0.56639646 1.16486741 1.16486741 0.56639646 1.16486741 1.16486741\n",
      " 0.56639646 0.56639646 0.56639646 1.16486741 1.16486741 1.16486741\n",
      " 1.16486741 0.56639646 1.16486741 0.56639646 1.16486741 0.56639646\n",
      " 1.16486741 0.97161741 0.97869876 0.56639646 1.16486741 1.16486741\n",
      " 1.56878599 1.16486741 1.16486741 0.56639646 1.16486741 0.56639646\n",
      " 1.16486741 1.16486741 1.16486741 1.16486741 0.56639646 1.16486741\n",
      " 1.16486741 1.16486741 1.16486741 0.56639646]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.247\n",
      "(*) epoch 2, cost 1.910\n",
      "(*) epoch 3, cost 1.665\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.449\n",
      "(*) epoch 2, cost 2.058\n",
      "(*) epoch 3, cost 1.890\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.65766091 0.65766091 0.65766091 0.65766091 0.65766091 0.65766091\n",
      " 0.65766091 1.93274521 0.65766091 0.65766091 0.65766091 0.65766091\n",
      " 1.00638569 0.65766091 0.65766091 0.65766091 0.65766091 0.65766091\n",
      " 0.65766091 1.00638569 0.65766091 0.65766091 0.65766091 0.65766091\n",
      " 0.65766091 0.65766091 1.00638569 0.65766091 0.65766091 0.65766091\n",
      " 0.65766091 0.65766091 1.00638569 1.00638569 1.00638569 1.00638569\n",
      " 0.65766091 0.65766091 0.65766091 1.00638569 0.65766091 1.00638569\n",
      " 0.65766091 0.65766091 0.65766091 0.65766091 1.00638569 0.65766091\n",
      " 0.65766091 0.65766091 0.65766091 0.65766091 0.65766091 0.65766091\n",
      " 1.00638569 0.65766091 1.00638569 1.62491889 1.00638569 0.65766091\n",
      " 1.00638569 0.65766091 0.65766091 0.65766091 1.00638569 0.65766091\n",
      " 0.65766091 0.65766091 0.65766091 1.00638569 1.00638569 0.65766091\n",
      " 1.00638569 0.65766091 0.65766091 0.65766091 0.65766091 1.00638569\n",
      " 0.65766091 0.65766091 0.65766091 2.28146999 0.65766091 0.65766091\n",
      " 0.65766091 0.65766091 1.00638569 1.00638569 0.65766091 0.65766091\n",
      " 1.00638569 0.65766091 0.65766091 0.65766091 0.65766091 1.00638569\n",
      " 0.65766091 0.65766091 1.00638569 1.00638569]\n",
      "all_sum is: [4.39095186 2.7254749  1.32460458 5.15605029 2.7254749  3.70206759\n",
      " 2.7254749  2.77858728 3.70206759 3.70206759 5.15605029 5.84493456\n",
      " 5.84493456 3.46747155 5.84493456 5.15605029 5.84493456 5.84493456\n",
      " 5.84493456 3.70206759 3.41435916 5.84493456 5.15605029 1.2714922\n",
      " 5.20916268 3.41435916 5.15605029 3.41435916 3.41435916 4.44406424\n",
      " 1.2714922  5.15605029 3.41435916 3.41435916 3.41435916 2.77858728\n",
      " 5.84493456 3.70206759 4.39095186 5.84493456 3.41435916 4.39095186\n",
      " 3.70206759 5.84493456 5.84493456 1.2714922  4.39095186 3.70206759\n",
      " 4.39095186 3.41435916 3.41435916 5.84493456 5.15605029 5.84493456\n",
      " 5.15605029 5.15605029 5.84493456 5.84493456 4.39095186 5.89804694\n",
      " 4.39095186 5.15605029 4.39095186 4.39095186 2.7254749  3.41435916\n",
      " 3.41435916 4.39095186 3.70206759 3.70206759 1.2714922  1.32460458\n",
      " 1.96037646 3.41435916 3.41435916 3.41435916 3.41435916 5.84493456\n",
      " 1.2714922  4.39095186 5.84493456 5.84493456 3.70206759 5.84493456\n",
      " 4.39095186 5.84493456 4.39095186 1.96037646 5.89804694 1.2714922\n",
      " 4.39095186 4.39095186 4.39095186 4.39095186 5.15605029 1.2714922\n",
      " 3.46747155 5.84493456 1.96037646 3.70206759]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.652\n",
      "(*) epoch 2, cost 2.792\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.071\n",
      "(*) epoch 2, cost 3.345\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.21043499 2.1441932  2.1441932  2.1441932  4.51917461 3.21043499\n",
      " 3.21043499 3.45293281 2.1441932  4.51917461 2.1441932  2.1441932\n",
      " 4.51917461 4.51917461 3.21043499 2.1441932  3.21043499 3.45293281\n",
      " 2.1441932  4.51917461 2.1441932  3.45293281 2.1441932  3.21043499\n",
      " 3.21043499 2.1441932  2.1441932  3.21043499 3.21043499 3.21043499\n",
      " 2.1441932  3.21043499 2.1441932  2.1441932  3.21043499 3.21043499\n",
      " 3.21043499 3.45293281 3.21043499 2.1441932  3.21043499 2.1441932\n",
      " 2.1441932  2.1441932  4.51917461 2.1441932  2.1441932  2.1441932\n",
      " 3.21043499 2.1441932  3.21043499 3.21043499 2.1441932  3.21043499\n",
      " 3.45293281 2.1441932  3.45293281 3.45293281 4.51917461 2.1441932\n",
      " 2.1441932  4.51917461 2.1441932  2.1441932  3.45293281 3.21043499\n",
      " 2.1441932  2.1441932  2.1441932  2.1441932  3.21043499 2.1441932\n",
      " 2.1441932  2.1441932  2.1441932  3.21043499 3.21043499 2.1441932\n",
      " 3.21043499 3.21043499 4.51917461 3.21043499 3.21043499 3.45293281\n",
      " 2.1441932  3.45293281 3.21043499 2.1441932  3.21043499 2.1441932\n",
      " 3.21043499 2.1441932  3.45293281 2.1441932  3.45293281 3.21043499\n",
      " 3.45293281 3.21043499 2.1441932  3.45293281]\n",
      "all_sum is: [2.9389661  2.9389661  2.71688676 2.71688676 2.9389661  2.71688676\n",
      " 2.71688676 2.9389661  2.9389661  2.9389661  4.17819664 2.71688676\n",
      " 2.9389661  2.71688676 4.40027597 2.9389661  2.71688676 2.71688676\n",
      " 2.71688676 2.9389661  4.40027597 2.9389661  4.40027597 2.71688676\n",
      " 2.71688676 4.17819664 2.9389661  2.9389661  2.9389661  2.9389661\n",
      " 4.40027597 2.9389661  4.17819664 2.71688676 2.71688676 2.9389661\n",
      " 2.9389661  2.9389661  2.9389661  2.71688676 2.9389661  2.71688676\n",
      " 2.71688676 2.9389661  2.9389661  4.40027597 2.71688676 2.9389661\n",
      " 2.71688676 2.71688676 2.71688676 2.9389661  2.9389661  4.17819664\n",
      " 2.9389661  2.71688676 2.9389661  2.71688676 2.9389661  4.40027597\n",
      " 4.40027597 2.9389661  2.9389661  4.17819664 2.9389661  2.9389661\n",
      " 2.9389661  2.71688676 2.9389661  2.9389661  2.9389661  2.71688676\n",
      " 2.9389661  4.40027597 2.9389661  4.17819664 2.71688676 2.71688676\n",
      " 2.9389661  2.71688676 2.71688676 2.9389661  2.9389661  2.9389661\n",
      " 2.9389661  4.17819664 2.9389661  2.9389661  4.17819664 2.9389661\n",
      " 2.9389661  2.9389661  4.40027597 4.17819664 4.40027597 4.17819664\n",
      " 2.9389661  4.40027597 4.40027597 2.71688676]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.003\n",
      "(*) epoch 2, cost 1.989\n",
      "(*) epoch 3, cost 1.622\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.155\n",
      "(*) epoch 2, cost 3.213\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519 0.49441519\n",
      " 0.49441519 0.49441519 0.49441519 0.49441519]\n",
      "all_sum is: [4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 5.66284342 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018 4.00399018\n",
      " 4.00399018 4.00399018 4.00399018 4.00399018]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.172\n",
      "(*) epoch 2, cost 0.289\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.966\n",
      "(*) epoch 2, cost 1.128\n",
      "(*) epoch 3, cost 0.968\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.36817836 1.36817836 1.36817836 0.32154695 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.77045239\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 3.25935376 0.32154695 1.36817836 1.0466314  1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 0.32154695\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.77045239 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.77045239 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836 1.36817836\n",
      " 1.36817836 1.36817836 1.36817836 1.77045239]\n",
      "all_sum is: [0.47611164 1.82638225 0.47611164 0.47611164 2.61642475 1.82638225\n",
      " 1.82638225 1.82638225 0.47611164 0.47611164 0.47611164 0.47611164\n",
      " 0.47611164 0.47611164 0.47611164 1.35027061 1.82638225 0.47611164\n",
      " 0.47611164 0.47611164 0.47611164 0.         0.47611164 1.35027061\n",
      " 1.82638225 0.47611164 0.47611164 0.47611164 0.47611164 0.47611164\n",
      " 0.47611164 1.82638225 0.47611164 0.47611164 0.47611164 0.47611164\n",
      " 0.47611164 0.47611164 0.47611164 0.47611164 0.         1.82638225\n",
      " 1.82638225 0.47611164 0.47611164 0.47611164 0.47611164 1.82638225\n",
      " 0.47611164 0.47611164 0.47611164 0.47611164 1.82638225 0.47611164\n",
      " 0.         0.47611164 0.         0.         0.47611164 1.82638225\n",
      " 1.26615414 1.26615414 1.82638225 1.82638225 0.47611164 0.\n",
      " 0.         1.82638225 0.47611164 0.47611164 1.82638225 0.47611164\n",
      " 0.         1.82638225 0.47611164 1.82638225 0.47611164 0.47611164\n",
      " 1.35027061 1.35027061 0.47611164 0.47611164 0.47611164 1.82638225\n",
      " 0.47611164 0.47611164 0.         1.82638225 0.47611164 1.82638225\n",
      " 1.82638225 1.82638225 0.47611164 1.82638225 0.47611164 0.47611164\n",
      " 0.47611164 0.47611164 1.82638225 1.82638225]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.246\n",
      "(*) epoch 2, cost 2.875\n",
      "(*) epoch 3, cost 2.570\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.067\n",
      "(*) epoch 2, cost 1.544\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.71054404 1.43561393 1.71054404 2.01394846 1.13220951 1.43561393\n",
      " 1.71054404 1.71054404 1.13220951 1.13220951 1.71054404 1.43561393\n",
      " 1.43561393 1.71054404 2.01394846 3.36871976 2.01394846 1.13220951\n",
      " 2.01394846 2.01394846 1.13220951 1.71054404 1.13220951 1.13220951\n",
      " 1.43561393 2.01394846 1.71054404 1.43561393 1.13220951 1.43561393\n",
      " 1.3429755  2.79038523 1.13220951 0.76464097 1.13220951 0.76464097\n",
      " 1.71054404 2.01394846 3.36871976 1.71054404 2.01394846 1.71054404\n",
      " 1.43561393 1.13220951 1.13220951 1.43561393 1.13220951 1.43561393\n",
      " 1.71054404 1.13220951 1.13220951 1.43561393 1.71054404 2.01394846\n",
      " 1.43561393 1.13220951 1.43561393 1.43561393 1.71054404 1.13220951\n",
      " 2.01394846 1.13220951 3.09378965 3.09378965 2.01394846 1.43561393\n",
      " 1.43561393 1.13220951 1.13220951 2.01394846 1.43561393 1.13220951\n",
      " 1.13220951 1.43561393 1.43561393 1.43561393 1.3429755  2.01394846\n",
      " 1.43561393 1.13220951 1.71054404 1.13220951 1.13220951 1.13220951\n",
      " 2.01394846 1.06804539 1.13220951 1.71054404 1.64637992 1.13220951\n",
      " 1.71054404 1.71054404 1.43561393 1.13220951 2.01394846 1.43561393\n",
      " 1.71054404 1.43561393 1.43561393 1.71054404]\n",
      "all_sum is: [2.58334725 2.58334725 2.58334725 2.58334725 1.69531435 1.69531435\n",
      " 2.58334725 2.58334725 1.69531435 2.58334725 2.58334725 2.58334725\n",
      " 2.58334725 1.69531435 1.69531435 2.58334725 1.69531435 2.58334725\n",
      " 1.69531435 2.58334725 2.58334725 1.69531435 2.58334725 1.69531435\n",
      " 1.69531435 2.50040026 1.69531435 2.58334725 1.69531435 2.89548324\n",
      " 1.69531435 1.69531435 2.76037277 2.58334725 1.69531435 1.69531435\n",
      " 1.69531435 1.69531435 1.69531435 1.69531435 2.58334725 2.58334725\n",
      " 2.58334725 2.58334725 1.69531435 1.69531435 3.38843317 2.58334725\n",
      " 1.69531435 2.58334725 1.69531435 1.69531435 2.58334725 2.58334725\n",
      " 1.69531435 1.69531435 1.69531435 1.69531435 1.69531435 2.76037277\n",
      " 1.69531435 1.69531435 1.69531435 2.58334725 1.69531435 2.58334725\n",
      " 2.83005255 2.58334725 1.69531435 2.58334725 1.69531435 1.69531435\n",
      " 2.58334725 1.69531435 1.69531435 2.58334725 1.94201964 2.50040026\n",
      " 1.69531435 3.38843317 2.83005255 1.69531435 1.69531435 2.58334725\n",
      " 1.69531435 1.69531435 2.58334725 1.69531435 3.38843317 1.69531435\n",
      " 1.69531435 2.58334725 1.69531435 2.58334725 2.58334725 1.69531435\n",
      " 2.58334725 1.69531435 1.69531435 2.58334725]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.866\n",
      "(*) epoch 2, cost 3.219\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.700\n",
      "(*) epoch 2, cost 1.150\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.9108661  0.\n",
      " 0.         0.9108661  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.9108661\n",
      " 0.         0.         0.57403726 0.         0.9108661  0.\n",
      " 0.         0.         0.9108661  0.         0.         0.\n",
      " 0.57403726 0.         0.         0.9108661  0.         0.9108661\n",
      " 0.9108661  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.57403726 0.\n",
      " 0.9108661  0.         0.         0.57403726 0.         0.\n",
      " 0.57403726 0.9108661  0.9108661  0.         0.         0.9108661\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.9108661  0.9108661  0.9108661  0.9108661  0.9108661  1.48490336\n",
      " 0.         0.         0.         1.48490336 0.57403726 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.9108661\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 2.15217204 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 0.9072781\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 2.15217204\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 3.18033022 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934 1.51420934\n",
      " 1.51420934 1.51420934 1.51420934 1.51420934]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.008\n",
      "(*) epoch 2, cost 0.732\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 7.451\n",
      "(*) epoch 2, cost 3.239\n",
      "(*) epoch 3, cost 2.914\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.37393668 2.47355705 2.47355705 2.37393668 3.75929894 2.54368789\n",
      " 2.47355705 2.47355705 2.47355705 2.47355705 3.75929894 2.47355705\n",
      " 2.47355705 1.257946   2.47355705 3.75929894 2.47355705 3.75929894\n",
      " 2.47355705 3.75929894 3.75929894 3.75929894 3.75929894 3.75929894\n",
      " 1.257946   2.47355705 2.47355705 2.47355705 2.47355705 2.47355705\n",
      " 2.47355705 1.257946   2.47355705 2.47355705 2.47355705 2.54368789\n",
      " 2.47355705 2.47355705 3.75929894 2.47355705 2.47355705 1.257946\n",
      " 2.47355705 2.54368789 1.257946   1.257946   2.47355705 2.37393668\n",
      " 2.47355705 2.47355705 2.47355705 2.47355705 2.47355705 2.47355705\n",
      " 2.47355705 2.47355705 2.47355705 1.257946   2.47355705 2.47355705\n",
      " 2.47355705 2.47355705 2.47355705 3.75929894 2.47355705 1.257946\n",
      " 2.47355705 2.47355705 2.47355705 2.47355705 2.47355705 2.47355705\n",
      " 3.75929894 1.257946   2.47355705 2.47355705 1.257946   2.47355705\n",
      " 1.257946   2.47355705 3.75929894 3.75929894 2.47355705 3.65967858\n",
      " 1.257946   2.47355705 1.257946   2.47355705 2.37393668 2.47355705\n",
      " 2.47355705 3.75929894 2.47355705 1.257946   3.75929894 2.47355705\n",
      " 2.47355705 2.47355705 2.47355705 1.257946  ]\n",
      "all_sum is: [0.86442735 1.9303417  0.86442735 0.86442735 0.12694738 0.12694738\n",
      " 2.66782167 0.86442735 0.86442735 0.86442735 0.12694738 0.86442735\n",
      " 1.02750231 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735\n",
      " 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735\n",
      " 0.86442735 0.86442735 0.86442735 0.86442735 0.12694738 0.86442735\n",
      " 0.12694738 0.86442735 0.12694738 0.86442735 0.86442735 0.86442735\n",
      " 0.12694738 0.86442735 0.12694738 0.12694738 0.86442735 0.12694738\n",
      " 0.86442735 0.86442735 0.12694738 0.86442735 0.12694738 0.86442735\n",
      " 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735\n",
      " 0.86442735 0.86442735 0.86442735 0.12694738 0.12694738 0.86442735\n",
      " 0.86442735 0.12694738 0.86442735 0.12694738 0.86442735 0.12694738\n",
      " 0.86442735 0.12694738 0.86442735 0.86442735 0.86442735 0.86442735\n",
      " 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735 0.86442735\n",
      " 0.86442735 0.12694738 0.12694738 0.86442735 1.02750231 0.86442735\n",
      " 0.12694738 0.86442735 0.86442735 0.12694738 0.86442735 0.86442735\n",
      " 0.86442735 0.86442735 0.12694738 0.86442735 0.12694738 0.86442735\n",
      " 0.86442735 0.12694738 0.86442735 0.86442735]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.673\n",
      "(*) epoch 2, cost 3.649\n",
      "(*) epoch 3, cost 3.346\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.906\n",
      "(*) epoch 2, cost 0.929\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [3.55593227 3.55593227 3.55593227 4.21516699 3.55593227 4.21516699\n",
      " 3.55593227 3.55593227 3.55593227 3.55593227 3.55593227 2.69095569\n",
      " 4.21516699 3.55593227 3.35019041 3.55593227 2.69095569 3.55593227\n",
      " 4.21516699 3.55593227 3.55593227 3.55593227 3.55593227 3.55593227\n",
      " 4.21516699 3.55593227 2.69095569 4.21516699 3.55593227 3.55593227\n",
      " 3.55593227 3.55593227 3.55593227 4.21516699 3.55593227 3.55593227\n",
      " 3.55593227 3.55593227 3.55593227 3.55593227 2.69095569 3.35019041\n",
      " 2.69095569 2.69095569 2.69095569 4.21516699 2.69095569 2.69095569\n",
      " 3.55593227 2.69095569 2.69095569 3.35019041 3.55593227 3.55593227\n",
      " 2.69095569 4.21516699 3.55593227 3.55593227 4.21516699 4.21516699\n",
      " 3.55593227 2.69095569 3.35019041 3.55593227 3.55593227 3.55593227\n",
      " 2.69095569 3.55593227 3.55593227 3.55593227 2.69095569 3.55593227\n",
      " 3.55593227 3.55593227 2.69095569 3.55593227 3.55593227 3.55593227\n",
      " 3.55593227 5.66926061 3.55593227 2.69095569 2.69095569 3.55593227\n",
      " 3.55593227 2.69095569 3.55593227 3.55593227 3.55593227 2.69095569\n",
      " 3.55593227 2.69095569 3.55593227 3.55593227 3.55593227 3.55593227\n",
      " 5.66926061 3.55593227 3.55593227 3.55593227]\n",
      "all_sum is: [0.17879888 0.         0.17879888 0.         0.17879888 0.\n",
      " 0.17879888 0.         0.17879888 0.17879888 0.17879888 0.17879888\n",
      " 0.17879888 0.         0.17879888 0.17879888 0.         0.\n",
      " 0.         0.17879888 0.17879888 0.         0.17879888 0.17879888\n",
      " 0.17879888 0.17879888 0.17879888 0.17879888 0.17879888 0.17879888\n",
      " 0.         0.         0.17879888 0.         0.         0.17879888\n",
      " 0.         0.17879888 0.         0.17879888 0.17879888 0.17879888\n",
      " 0.         0.         0.         0.17879888 0.         0.17879888\n",
      " 0.         0.17879888 0.         0.         0.         0.\n",
      " 0.17879888 0.         0.17879888 0.         0.17879888 0.17879888\n",
      " 0.17879888 0.17879888 0.         0.17879888 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.17879888 0.         0.17879888 0.17879888 0.17879888\n",
      " 0.         0.17879888 0.17879888 0.         0.17879888 0.\n",
      " 0.17879888 0.17879888 0.17879888 0.17879888 0.17879888 0.\n",
      " 0.17879888 0.         0.17879888 0.17879888 0.         0.\n",
      " 0.         0.17879888 0.17879888 0.17879888]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.988\n",
      "(*) epoch 2, cost 1.145\n",
      "(*) epoch 3, cost 0.977\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.621\n",
      "(*) epoch 2, cost 0.883\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.07276313 1.67352895 2.74629208 1.67352895 2.74629208 2.74629208\n",
      " 2.74629208 1.67352895 1.67352895 2.74629208 2.74629208 2.74629208\n",
      " 1.67352895 1.67352895 3.21388743 1.67352895 2.74629208 1.67352895\n",
      " 1.67352895 1.67352895 1.67352895 3.19196415 1.67352895 2.74629208\n",
      " 2.74629208 1.67352895 2.74629208 2.74629208 2.74629208 2.74629208\n",
      " 2.74629208 2.74629208 2.74629208 2.1411243  1.67352895 2.74629208\n",
      " 1.67352895 1.67352895 1.67352895 2.74629208 1.67352895 2.74629208\n",
      " 1.67352895 2.74629208 2.74629208 3.19196415 1.67352895 1.67352895\n",
      " 2.74629208 2.74629208 1.67352895 2.74629208 2.74629208 1.67352895\n",
      " 3.21388743 2.74629208 1.67352895 2.74629208 3.21388743 2.1411243\n",
      " 2.74629208 1.67352895 1.67352895 2.74629208 1.67352895 1.67352895\n",
      " 1.67352895 2.74629208 2.74629208 2.74629208 1.67352895 1.67352895\n",
      " 1.67352895 2.74629208 2.74629208 2.1411243  1.67352895 2.74629208\n",
      " 1.67352895 2.74629208 1.67352895 2.74629208 1.67352895 1.67352895\n",
      " 2.74629208 1.67352895 2.74629208 2.74629208 2.74629208 1.67352895\n",
      " 1.67352895 1.07276313 1.67352895 2.74629208 2.74629208 2.74629208\n",
      " 1.67352895 1.67352895 2.74629208 2.74629208]\n",
      "all_sum is: [4.35146076 4.35146076 4.35146076 4.35146076 6.21510769 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 6.37351787 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 4.35146076 4.50987094 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 5.12639793 4.35146076 4.35146076\n",
      " 4.35146076 6.21510769 5.12639793 6.21510769 4.35146076 4.35146076\n",
      " 6.21510769 4.50987094 4.35146076 4.35146076 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 5.28480811 5.28480811 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 6.21510769 4.35146076 5.12639793\n",
      " 4.35146076 4.35146076 4.35146076 4.35146076 6.21510769 4.35146076\n",
      " 5.12639793 4.35146076 4.35146076 4.35146076 4.35146076 4.50987094\n",
      " 5.12639793 4.35146076 4.35146076 5.12639793 4.35146076 4.35146076\n",
      " 6.21510769 4.35146076 4.50987094 4.35146076 4.35146076 4.35146076\n",
      " 4.50987094 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076 4.35146076\n",
      " 4.35146076 4.35146076 4.35146076 5.12639793]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.880\n",
      "(*) epoch 2, cost 2.362\n",
      "(*) epoch 3, cost 2.038\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.261\n",
      "(*) epoch 2, cost 1.325\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.72383067 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 3.08205765\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.72383067\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 3.08205765\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.72383067 1.53343455\n",
      " 1.53343455 1.98319406 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.98319406 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 3.08205765 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 3.08205765 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455 1.53343455\n",
      " 1.53343455 1.53343455 1.53343455 1.72383067]\n",
      "all_sum is: [0.6710848  0.6710848  0.6710848  0.735917   0.6710848  0.6710848\n",
      " 0.6710848  1.64397047 0.735917   0.6710848  0.6710848  0.735917\n",
      " 0.6710848  0.735917   0.6710848  0.735917   0.735917   0.735917\n",
      " 0.735917   1.57913827 0.6710848  0.6710848  0.6710848  0.735917\n",
      " 0.6710848  0.6710848  0.6710848  0.6710848  0.6710848  0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  0.6710848  1.64397047 0.6710848\n",
      " 0.735917   0.6710848  0.6710848  0.6710848  0.6710848  0.735917\n",
      " 0.735917   0.735917   0.735917   0.6710848  0.6710848  0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  0.6710848  0.6710848  0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  0.6710848  0.6710848  0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  1.64397047 0.6710848  0.735917\n",
      " 0.735917   0.6710848  0.735917   0.6710848  0.6710848  0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  0.6710848  0.735917   0.6710848\n",
      " 0.6710848  0.6710848  0.6710848  0.735917   0.6710848  0.6710848\n",
      " 1.57913827 0.6710848  0.6710848  0.6710848  0.6710848  0.6710848\n",
      " 0.735917   0.6710848  0.735917   0.6710848  0.735917   0.735917\n",
      " 0.735917   0.6710848  0.735917   0.735917  ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.177\n",
      "(*) epoch 2, cost 1.397\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.324\n",
      "(*) epoch 2, cost 2.606\n",
      "(*) epoch 3, cost 2.329\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [5.67095288 5.67095288 6.19231399 5.67095288 6.19231399 5.67095288\n",
      " 6.19231399 5.67095288 5.67095288 5.67095288 2.82375417 3.88474603\n",
      " 5.67095288 6.19231399 5.67095288 5.65268326 5.67095288 6.19231399\n",
      " 5.67095288 5.67095288 5.67095288 5.67095288 5.67095288 5.67095288\n",
      " 5.67095288 5.67095288 5.67095288 5.65268326 5.67095288 3.88474603\n",
      " 5.67095288 5.67095288 5.67095288 5.67095288 5.65268326 5.67095288\n",
      " 6.19231399 5.67095288 6.19231399 5.67095288 6.19231399 5.67095288\n",
      " 5.67095288 5.67095288 5.67095288 6.19231399 3.36338491 5.67095288\n",
      " 5.67095288 5.67095288 5.67095288 5.67095288 5.67095288 5.65268326\n",
      " 5.67095288 5.67095288 5.67095288 6.19231399 6.19231399 6.19231399\n",
      " 6.19231399 6.19231399 6.19231399 6.19231399 5.67095288 5.67095288\n",
      " 6.19231399 5.67095288 5.67095288 5.67095288 5.67095288 5.67095288\n",
      " 5.67095288 5.67095288 5.67095288 5.67095288 5.65268326 6.19231399\n",
      " 5.67095288 6.19231399 5.67095288 5.67095288 5.67095288 5.67095288\n",
      " 5.67095288 5.67095288 5.67095288 5.67095288 6.19231399 6.19231399\n",
      " 5.67095288 5.65268326 6.19231399 5.67095288 5.67095288 5.67095288\n",
      " 3.36338491 5.67095288 5.67095288 5.67095288]\n",
      "all_sum is: [2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394 2.44334394\n",
      " 2.44334394 2.44334394 2.44334394 2.44334394]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 10\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 5.860\n",
      "(*) epoch 2, cost 2.326\n",
      "(*) epoch 3, cost 1.771\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.566\n",
      "(*) epoch 2, cost 0.552\n",
      "(*) epoch 3, cost 0.409\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.78960013 0.78960013 0.62057233 0.78960013 0.78960013 1.32493392\n",
      " 0.62057233 0.78960013 0.62057233 0.78960013 1.20886574 1.20886574\n",
      " 0.78960013 0.78960013 0.78960013 1.03983793 0.62057233 0.78960013\n",
      " 1.74419953 0.78960013 0.62057233 0.62057233 1.20886574 0.62057233\n",
      " 0.78960013 0.62057233 1.20886574 0.78960013 0.78960013 0.62057233\n",
      " 0.78960013 0.78960013 0.78960013 1.20886574 0.78960013 0.78960013\n",
      " 0.78960013 1.32493392 0.62057233 1.20886574 0.78960013 0.78960013\n",
      " 0.78960013 0.78960013 0.78960013 1.74419953 1.20886574 1.20886574\n",
      " 0.78960013 1.03983793 0.78960013 0.78960013 0.78960013 0.78960013\n",
      " 0.78960013 0.78960013 0.78960013 0.78960013 0.78960013 0.78960013\n",
      " 0.78960013 1.20886574 0.62057233 1.03983793 1.20886574 1.20886574\n",
      " 1.03983793 0.62057233 0.62057233 0.62057233 0.62057233 0.62057233\n",
      " 0.62057233 0.78960013 0.62057233 0.78960013 0.62057233 0.78960013\n",
      " 0.62057233 0.78960013 1.15590612 1.03983793 1.32493392 0.78960013\n",
      " 0.78960013 1.20886574 0.78960013 0.78960013 0.78960013 1.32493392\n",
      " 0.78960013 0.62057233 0.78960013 0.78960013 0.62057233 0.78960013\n",
      " 1.20886574 1.03983793 1.03983793 0.78960013]\n",
      "all_sum is: [1.94188256 1.94188256 1.94188256 1.08349714 1.94188256 1.2421434\n",
      " 1.08349714 1.94188256 1.94188256 1.08349714 1.94188256 1.2421434\n",
      " 1.2421434  1.08349714 1.2421434  1.08349714 1.08349714 0.38375797\n",
      " 1.08349714 1.08349714 0.38375797 1.2421434  1.94188256 1.2421434\n",
      " 1.08349714 1.94188256 1.08349714 1.94188256 1.08349714 1.08349714\n",
      " 1.94188256 0.38375797 1.2421434  1.94188256 1.2421434  1.94188256\n",
      " 1.08349714 1.94188256 1.94188256 1.08349714 1.94188256 3.3266956\n",
      " 1.08349714 1.2421434  1.08349714 1.94188256 1.94188256 1.08349714\n",
      " 1.94188256 1.94188256 1.94188256 1.08349714 1.94188256 1.2421434\n",
      " 1.2421434  1.08349714 1.08349714 1.08349714 1.94188256 1.2421434\n",
      " 1.08349714 1.94188256 1.08349714 1.08349714 1.08349714 1.08349714\n",
      " 1.94188256 1.94188256 0.38375797 1.94188256 1.08349714 1.08349714\n",
      " 1.94188256 1.94188256 1.94188256 1.08349714 1.94188256 1.08349714\n",
      " 1.08349714 1.94188256 1.94188256 1.94188256 1.2421434  1.08349714\n",
      " 0.38375797 1.94188256 1.94188256 1.08349714 1.94188256 1.94188256\n",
      " 1.2421434  1.94188256 1.08349714 1.2421434  0.38375797 1.2421434\n",
      " 1.94188256 1.08349714 1.2421434  1.2421434 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.164\n",
      "(*) epoch 2, cost 3.010\n",
      "(*) epoch 3, cost 2.833\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.141\n",
      "(*) epoch 2, cost 2.637\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.12252235 0.30919935 0.12252235 0.12252235 0.12252235 1.53441027\n",
      " 1.53441027 0.12252235 0.12252235 1.53441027 0.12252235 0.12252235\n",
      " 0.12252235 0.12252235 1.53441027 0.30919935 0.12252235 0.12252235\n",
      " 0.12252235 0.12252235 1.53441027 0.12252235 0.12252235 1.53441027\n",
      " 0.12252235 0.12252235 0.12252235 0.12252235 0.12252235 0.30919935\n",
      " 1.53441027 0.12252235 0.12252235 0.12252235 0.12252235 0.12252235\n",
      " 0.12252235 0.12252235 0.12252235 0.12252235 0.12252235 1.53441027\n",
      " 1.53441027 0.12252235 1.72108728 0.12252235 0.12252235 0.12252235\n",
      " 0.30919935 0.12252235 0.30919935 1.53441027 0.12252235 0.12252235\n",
      " 1.53441027 1.53441027 0.12252235 1.53441027 0.12252235 1.53441027\n",
      " 1.53441027 0.30919935 0.12252235 0.12252235 0.30919935 1.53441027\n",
      " 0.12252235 1.53441027 0.30919935 1.53441027 0.12252235 1.72108728\n",
      " 1.53441027 0.12252235 0.12252235 0.12252235 0.12252235 0.12252235\n",
      " 0.12252235 0.12252235 1.53441027 0.12252235 0.12252235 0.30919935\n",
      " 0.12252235 0.12252235 0.12252235 0.12252235 1.53441027 0.12252235\n",
      " 1.53441027 0.12252235 0.12252235 1.53441027 1.72108728 1.53441027\n",
      " 1.53441027 0.12252235 0.12252235 0.12252235]\n",
      "all_sum is: [0.24520288 0.24520288 0.24520288 0.24520288 0.         0.24520288\n",
      " 0.24520288 0.         0.24520288 0.24520288 0.         0.24520288\n",
      " 0.         0.24520288 0.         0.         0.24520288 0.24520288\n",
      " 0.         0.         0.24520288 0.         0.         0.24520288\n",
      " 0.         0.24520288 0.24520288 0.         0.         0.\n",
      " 0.         0.24520288 0.         0.24520288 0.24520288 0.24520288\n",
      " 0.24520288 0.         0.24520288 0.         0.24520288 0.\n",
      " 0.24520288 0.24520288 0.24520288 0.24520288 0.         0.\n",
      " 0.         0.24520288 0.         0.24520288 0.         0.\n",
      " 0.24520288 0.24520288 0.         0.         0.24520288 0.\n",
      " 0.         0.24520288 0.         0.24520288 0.         0.\n",
      " 0.24520288 0.24520288 0.         0.         0.         0.24520288\n",
      " 0.24520288 0.24520288 0.24520288 0.24520288 0.         0.24520288\n",
      " 0.         0.24520288 0.24520288 0.24520288 0.         0.24520288\n",
      " 0.         0.24520288 0.         0.24520288 0.         0.24520288\n",
      " 0.24520288 0.24520288 0.24520288 0.         0.24520288 0.\n",
      " 0.24520288 0.24520288 0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.494\n",
      "(*) epoch 2, cost 1.766\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.272\n",
      "(*) epoch 2, cost 2.307\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.33997347 1.51230064 0.96787544 0.96787544 0.96787544 1.14020261\n",
      " 2.33608673 0.96787544 1.33997347 0.96787544 0.96787544 1.33997347\n",
      " 2.70818476 1.51230064 1.14020261 1.14020261 1.14020261 0.96787544\n",
      " 0.96787544 1.51230064 1.33997347 1.14020261 2.5084139  1.33997347\n",
      " 1.14020261 1.33997347 0.96787544 1.51230064 1.14020261 1.14020261\n",
      " 0.96787544 1.51230064 1.33997347 0.96787544 0.96787544 1.14020261\n",
      " 0.96787544 1.14020261 1.33997347 1.33997347 0.96787544 1.14020261\n",
      " 1.14020261 0.96787544 2.70818476 1.51230064 0.96787544 1.51230064\n",
      " 1.33997347 2.70818476 1.51230064 0.96787544 1.33997347 1.33997347\n",
      " 1.14020261 1.14020261 0.96787544 0.96787544 0.96787544 1.14020261\n",
      " 1.33997347 1.51230064 1.33997347 0.96787544 1.33997347 0.96787544\n",
      " 0.96787544 1.14020261 1.14020261 0.96787544 0.96787544 1.51230064\n",
      " 1.51230064 1.14020261 1.33997347 0.96787544 0.96787544 0.96787544\n",
      " 1.33997347 0.96787544 0.96787544 1.14020261 0.96787544 2.70818476\n",
      " 1.33997347 1.51230064 1.14020261 1.51230064 1.14020261 1.33997347\n",
      " 0.96787544 1.33997347 0.96787544 0.96787544 1.33997347 1.33997347\n",
      " 2.5084139  1.14020261 1.33997347 2.33608673]\n",
      "all_sum is: [0.58256273 0.58256273 0.90824958 0.58256273 0.58256273 0.90824958\n",
      " 0.90824958 0.58256273 0.58256273 0.90824958 0.90824958 0.58256273\n",
      " 0.90824958 0.90824958 0.90824958 0.58256273 0.90824958 0.58256273\n",
      " 0.58256273 0.90824958 0.90824958 0.58256273 0.58256273 0.90824958\n",
      " 0.90824958 0.90824958 0.90824958 0.90824958 0.58256273 0.90824958\n",
      " 0.90824958 0.90824958 0.90824958 0.58256273 0.58256273 0.58256273\n",
      " 0.58256273 0.90824958 0.90824958 0.90824958 0.58256273 0.90824958\n",
      " 0.58256273 0.58256273 0.58256273 0.90824958 0.58256273 0.90824958\n",
      " 0.58256273 0.90824958 0.90824958 0.90824958 0.90824958 0.58256273\n",
      " 0.58256273 0.90824958 0.90824958 0.58256273 0.90824958 0.90824958\n",
      " 0.90824958 0.58256273 0.90824958 0.58256273 0.90824958 0.58256273\n",
      " 0.58256273 0.90824958 0.90824958 0.90824958 0.58256273 0.58256273\n",
      " 0.90824958 0.58256273 0.58256273 0.90824958 0.90824958 0.90824958\n",
      " 0.90824958 0.90824958 0.58256273 0.90824958 0.58256273 0.58256273\n",
      " 0.90824958 0.90824958 0.58256273 0.90824958 0.58256273 0.90824958\n",
      " 0.90824958 0.90824958 0.58256273 0.58256273 0.58256273 0.90824958\n",
      " 0.58256273 0.90824958 0.90824958 0.90824958]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.188\n",
      "(*) epoch 2, cost 1.490\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.455\n",
      "(*) epoch 2, cost 0.840\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.58147073 2.54957816 1.58147073 1.58147073 1.58147073 2.54957816\n",
      " 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073\n",
      " 2.54957816 2.54957816 1.58147073 1.58147073 1.58147073 1.58147073\n",
      " 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073\n",
      " 1.58147073 1.58147073 1.58147073 2.75772628 2.54957816 1.58147073\n",
      " 1.58147073 1.58147073 2.54957816 0.96810743 2.54957816 1.58147073\n",
      " 2.75772628 2.54957816 2.54957816 1.58147073 1.58147073 1.58147073\n",
      " 2.54957816 1.58147073 1.58147073 2.54957816 1.58147073 1.58147073\n",
      " 1.58147073 2.54957816 1.58147073 1.58147073 2.54957816 1.58147073\n",
      " 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073\n",
      " 2.54957816 1.58147073 2.54957816 2.54957816 1.58147073 1.58147073\n",
      " 2.54957816 1.58147073 2.54957816 1.58147073 2.54957816 2.54957816\n",
      " 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073 1.58147073\n",
      " 1.58147073 1.58147073 1.58147073 2.54957816 1.58147073 1.58147073\n",
      " 2.54957816 2.54957816 1.58147073 0.96810743 1.58147073 1.58147073\n",
      " 1.58147073 2.54957816 2.54957816 2.54957816 1.58147073 2.54957816\n",
      " 1.58147073 2.54957816 1.58147073 2.54957816]\n",
      "all_sum is: [0.         0.         0.39250743 0.         0.         0.39250743\n",
      " 0.39250743 0.         0.39250743 0.39250743 0.         0.\n",
      " 0.39250743 0.39250743 0.39250743 0.         0.         0.\n",
      " 0.39250743 0.         0.         0.39250743 0.39250743 0.39250743\n",
      " 0.39250743 0.         0.         0.39250743 0.         0.39250743\n",
      " 0.59682348 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.39250743 0.         0.31740412 0.\n",
      " 0.         0.         0.         0.39250743 0.39250743 0.59682348\n",
      " 0.39250743 0.39250743 0.39250743 0.98933091 0.59682348 0.\n",
      " 0.         0.39250743 0.         0.59682348 0.         0.39250743\n",
      " 0.39250743 0.39250743 0.         0.         0.         0.39250743\n",
      " 0.         0.39250743 0.39250743 0.39250743 0.39250743 0.39250743\n",
      " 0.39250743 0.39250743 0.39250743 0.39250743 0.98933091 0.39250743\n",
      " 0.         0.         0.         0.         0.39250743 0.39250743\n",
      " 0.         0.         0.39250743 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.98933091 0.         0.         0.98933091]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.540\n",
      "(*) epoch 2, cost 1.112\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.881\n",
      "(*) epoch 2, cost 1.001\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418 2.37743418\n",
      " 2.37743418 2.37743418 2.37743418 2.37743418]\n",
      "all_sum is: [0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796 0.02733796\n",
      " 0.02733796 0.02733796 0.02733796 0.02733796]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 2\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.603\n",
      "(*) epoch 2, cost 0.419\n",
      "(*) epoch 3, cost 0.220\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.142\n",
      "(*) epoch 2, cost 2.366\n",
      "(*) epoch 3, cost 2.131\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.84772873 1.11267987 1.11267987 1.84772873 1.84772873 1.11267987\n",
      " 1.11267987 1.11267987 1.84772873 1.11267987 1.11267987 1.84772873\n",
      " 1.84772873 1.84772873 1.84772873 1.84772873 1.11267987 1.84772873\n",
      " 3.56841628 1.84772873 1.11267987 1.11267987 1.84772873 1.84772873\n",
      " 1.11267987 1.21822581 1.11267987 1.11267987 1.84772873 2.08207546\n",
      " 1.84772873 1.84772873 1.11267987 1.84772873 1.84772873 1.84772873\n",
      " 1.84772873 1.11267987 1.84772873 1.84772873 1.84772873 1.21822581\n",
      " 1.11267987 1.11267987 1.84772873 2.08207546 1.84772873 1.11267987\n",
      " 1.11267987 1.11267987 1.84772873 1.11267987 1.84772873 1.84772873\n",
      " 1.11267987 1.84772873 1.84772873 0.48317696 1.84772873 1.11267987\n",
      " 1.84772873 1.84772873 1.84772873 1.84772873 1.84772873 1.84772873\n",
      " 1.11267987 1.84772873 1.84772873 1.84772873 1.84772873 1.11267987\n",
      " 1.84772873 1.84772873 1.84772873 1.11267987 1.11267987 1.84772873\n",
      " 1.84772873 1.84772873 1.11267987 1.11267987 1.11267987 1.11267987\n",
      " 1.11267987 1.11267987 1.11267987 1.84772873 1.11267987 1.11267987\n",
      " 1.11267987 1.11267987 1.11267987 1.11267987 1.84772873 1.84772873\n",
      " 1.11267987 1.84772873 1.84772873 1.11267987]\n",
      "all_sum is: [3.04613122 3.04613122 3.04613122 3.04613122 2.83953662 3.1381585\n",
      " 3.04613122 3.04613122 2.83953662 3.04613122 3.04613122 3.04613122\n",
      " 2.83953662 3.04613122 2.83953662 3.04613122 3.04613122 3.04613122\n",
      " 3.04613122 3.04613122 3.04613122 3.04613122 3.04613122 3.04613122\n",
      " 2.83953662 3.04613122 3.04613122 3.04613122 2.83953662 3.04613122\n",
      " 3.04613122 2.83953662 3.04613122 3.04613122 3.04613122 3.1381585\n",
      " 3.04613122 2.83953662 3.04613122 3.04613122 3.04613122 3.04613122\n",
      " 3.04613122 3.04613122 2.83953662 3.04613122 3.04613122 2.83953662\n",
      " 3.04613122 3.04613122 3.04613122 3.04613122 3.1381585  3.04613122\n",
      " 3.04613122 3.1381585  3.04613122 2.83953662 3.04613122 2.83953662\n",
      " 2.83953662 3.04613122 2.83953662 3.04613122 2.83953662 3.1381585\n",
      " 3.04613122 3.1381585  3.04613122 2.83953662 2.83953662 2.83953662\n",
      " 2.83953662 2.83953662 3.1381585  3.04613122 3.04613122 3.04613122\n",
      " 3.04613122 3.04613122 3.04613122 3.04613122 3.1381585  3.04613122\n",
      " 3.04613122 2.83953662 3.04613122 3.04613122 2.93156391 2.83953662\n",
      " 2.83953662 3.04613122 3.04613122 2.83953662 3.04613122 3.04613122\n",
      " 3.04613122 3.04613122 3.1381585  2.83953662]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.404\n",
      "(*) epoch 2, cost 3.579\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.246\n",
      "(*) epoch 2, cost 2.135\n",
      "(*) epoch 3, cost 1.904\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [5.15168582 4.03369178 3.55680806 5.15168582 2.10608862 4.57030686\n",
      " 6.60809445 4.03369178 5.15168582 1.55299435 3.11389823 1.99590418\n",
      " 1.55299435 4.60428073 1.99590418 4.14387622 5.59459566 3.11389823\n",
      " 3.59078194 4.12739702 3.70096638 1.55299435 5.59459566 5.04719057\n",
      " 4.12739702 3.55680806 4.6804913  5.04719057 1.99590418 1.55299435\n",
      " 6.60809445 5.04719057 5.15168582 3.55680806 3.59078194 5.04719057\n",
      " 4.03369178 3.55680806 1.99590418 4.14387622 4.03369178 1.99590418\n",
      " 1.66317879 1.99590418 2.56649314 1.99590418 3.55680806 3.59078194\n",
      " 4.14387622 5.15168582 3.11389823 3.11389823 1.99590418 4.12739702\n",
      " 3.55680806 3.59078194 3.6669925  5.15168582 1.99590418 4.03369178\n",
      " 5.15737501 3.00940298 1.99590418 6.60809445 1.99590418 3.59078194\n",
      " 3.22408267 1.99590418 3.22408267 5.59459566 6.16518461 3.55680806\n",
      " 5.15168582 1.99590418 5.59459566 3.55680806 1.99590418 4.12739702\n",
      " 3.59078194 2.56649314 3.55680806 5.15168582 5.59459566 4.03369178\n",
      " 6.60809445 3.55680806 3.55680806 2.10608862 5.59459566 1.99590418\n",
      " 1.99590418 4.12739702 3.11389823 4.57030686 1.99590418 5.15168582\n",
      " 5.15168582 4.03369178 5.59459566 3.00940298]\n",
      "all_sum is: [2.05613405 3.54761895 2.05613405 2.8178614  2.05613405 2.05613405\n",
      " 0.97785682 2.78589161 2.8178614  3.54761895 0.97785682 2.98314459\n",
      " 1.73958417 2.05613405 2.8178614  2.05613405 2.05613405 2.05613405\n",
      " 2.8178614  2.05613405 2.05613405 2.05613405 2.05613405 0.97785682\n",
      " 3.54761895 2.61642883 0.97785682 0.97785682 2.78589161 2.05613405\n",
      " 2.05613405 2.78589161 2.46934173 2.05613405 2.05613405 2.8178614\n",
      " 2.05613405 2.8178614  3.37593376 2.78589161 2.05613405 2.25338703\n",
      " 3.94040812 2.05613405 2.05613405 2.05613405 3.54761895 0.97785682\n",
      " 2.98314459 1.70761438 2.05613405 3.54761895 2.78589161 0.97785682\n",
      " 3.21065056 0.97785682 1.90486737 2.05613405 2.05613405 2.78589161\n",
      " 2.8178614  2.05613405 2.44892321 2.05613405 2.25338703 2.8178614\n",
      " 2.05613405 2.25338703 3.54761895 3.74487194 2.8178614  2.98314459\n",
      " 2.8178614  3.01511438 0.97785682 0.97785682 3.94040812 1.17510981\n",
      " 3.54172256 3.37593376 2.25338703 2.25338703 2.05613405 2.05613405\n",
      " 2.78589161 0.97785682 2.8178614  2.25338703 3.54761895 2.25338703\n",
      " 2.78589161 2.05613405 2.98314459 2.05613405 2.8178614  2.05613405\n",
      " 3.54761895 2.05613405 1.70761438 0.97785682]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.332\n",
      "(*) epoch 2, cost 4.013\n",
      "(*) epoch 3, cost 3.609\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.349\n",
      "(*) epoch 2, cost 3.131\n",
      "(*) epoch 3, cost 3.029\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.33483008 2.19967627 2.10386974 2.33483008 2.33483008 2.33483008\n",
      " 2.10386974 2.10386974 2.33483008 2.10386974 2.10386974 2.33483008\n",
      " 2.10386974 2.10386974 2.4306366  2.33483008 2.10386974 2.33483008\n",
      " 2.19967627 2.33483008 2.33483008 2.33483008 2.33483008 2.19967627\n",
      " 2.10386974 2.10386974 2.4306366  2.33483008 2.19967627 2.10386974\n",
      " 2.10386974 2.10386974 2.33483008 2.4306366  2.33483008 2.19967627\n",
      " 2.33483008 2.33483008 2.19967627 2.19967627 2.19967627 2.10386974\n",
      " 2.33483008 2.33483008 2.33483008 2.33483008 2.10386974 2.10386974\n",
      " 2.10386974 2.33483008 2.19967627 2.10386974 2.4306366  2.19967627\n",
      " 2.33483008 2.33483008 2.10386974 2.10386974 2.33483008 2.10386974\n",
      " 2.4306366  2.33483008 2.10386974 2.10386974 2.33483008 2.33483008\n",
      " 2.4306366  2.33483008 2.4306366  2.33483008 2.10386974 2.19967627\n",
      " 2.33483008 2.10386974 2.10386974 2.10386974 2.33483008 2.10386974\n",
      " 2.10386974 2.4306366  2.10386974 2.4306366  2.33483008 2.19967627\n",
      " 2.10386974 2.10386974 2.4306366  2.10386974 2.33483008 2.10386974\n",
      " 2.33483008 2.4306366  2.33483008 2.33483008 2.10386974 2.10386974\n",
      " 2.33483008 2.10386974 2.33483008 2.4306366 ]\n",
      "all_sum is: [0.         0.         0.         1.25878084 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.25878084 0.         0.         0.         0.\n",
      " 0.         0.         0.         1.25878084 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.25878084 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.25878084 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.564\n",
      "(*) epoch 2, cost 2.351\n",
      "(*) epoch 3, cost 1.932\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.776\n",
      "(*) epoch 2, cost 0.653\n",
      "(*) epoch 3, cost 0.352\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386 1.62413386\n",
      " 1.62413386 1.62413386 1.62413386 1.62413386]\n",
      "all_sum is: [2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 3.88985595 2.50662861 2.50662861 0.22155038 2.50662861 2.50662861\n",
      " 3.88985595 2.50662861 2.50662861 2.82453894 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 3.88985595 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 0.22155038 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 3.88985595 3.88985595 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 0.22155038 2.50662861 2.50662861 2.50662861 0.22155038 2.50662861\n",
      " 2.50662861 3.88985595 2.50662861 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861 2.50662861 3.88985595\n",
      " 2.50662861 2.50662861 3.88985595 2.50662861 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 3.88985595 3.88985595 2.50662861 2.50662861\n",
      " 2.50662861 2.50662861 2.50662861 2.50662861]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.567\n",
      "(*) epoch 2, cost 3.290\n",
      "(*) epoch 3, cost 3.119\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.707\n",
      "(*) epoch 2, cost 2.035\n",
      "(*) epoch 3, cost 1.733\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.98996809 1.66067862 0.04058596 1.66067862 0.98996809 0.98996809\n",
      " 0.98996809 1.54428865 1.66067862 1.66067862 1.66067862 0.98996809\n",
      " 0.98996809 2.9423468  0.98996809 2.27163627 0.98996809 0.98996809\n",
      " 0.98996809 1.66067862 1.66067862 0.98996809 0.98996809 0.98996809\n",
      " 1.66067862 1.66067862 2.9423468  0.98996809 0.04058596 1.66067862\n",
      " 1.66067862 1.66067862 0.98996809 0.98996809 0.98996809 0.98996809\n",
      " 0.04058596 0.98996809 0.98996809 2.9423468  1.66067862 0.98996809\n",
      " 0.04058596 1.66067862 0.98996809 1.66067862 0.98996809 1.54428865\n",
      " 0.98996809 0.98996809 2.27163627 0.98996809 0.98996809 1.66067862\n",
      " 2.27163627 1.66067862 1.66067862 1.66067862 0.98996809 0.98996809\n",
      " 0.04058596 0.98996809 2.27163627 1.66067862 2.9423468  2.9423468\n",
      " 1.66067862 0.98996809 0.98996809 0.98996809 0.98996809 0.98996809\n",
      " 1.66067862 0.98996809 1.66067862 0.04058596 0.98996809 0.98996809\n",
      " 0.98996809 1.66067862 1.66067862 2.27163627 0.98996809 1.66067862\n",
      " 2.27163627 1.66067862 0.98996809 1.66067862 0.98996809 0.04058596\n",
      " 2.27163627 1.66067862 0.98996809 0.98996809 0.98996809 2.27163627\n",
      " 0.98996809 0.98996809 1.32225415 0.98996809]\n",
      "all_sum is: [2.14704976 0.88014359 0.88014359 0.88014359 2.14704976 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359 2.14704976\n",
      " 0.88014359 2.14704976 2.14704976 0.88014359 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 2.14704976 0.\n",
      " 0.88014359 2.14704976 0.88014359 0.88014359 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359\n",
      " 2.14704976 0.88014359 0.88014359 0.88014359 0.88014359 2.14704976\n",
      " 0.88014359 0.88014359 1.0066333  0.88014359 0.88014359 0.88014359\n",
      " 2.14704976 0.88014359 2.14704976 0.88014359 0.88014359 0.88014359\n",
      " 2.27353947 0.88014359 0.88014359 0.88014359 0.88014359 2.14704976\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 2.14704976 0.88014359\n",
      " 0.88014359 0.88014359 0.88014359 2.14704976 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 2.14704976 0.88014359 0.         2.27353947\n",
      " 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359 0.88014359\n",
      " 0.88014359 0.88014359 2.14704976 0.88014359]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.131\n",
      "(*) epoch 2, cost 2.292\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 9\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 9.015\n",
      "(*) epoch 2, cost 4.060\n",
      "(*) epoch 3, cost 3.405\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.677711   1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 2.14982794 1.41410408 1.88622101 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.59000264 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.59000264 1.59000264\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.88622101\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.88622101 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.88622101 1.59000264 1.41410408 1.41410408 1.41410408 1.91928941\n",
      " 1.41410408 1.59000264 1.41410408 1.41410408 1.41410408 1.88622101\n",
      " 1.41410408 1.88622101 1.41410408 1.41410408 1.41410408 1.41410408\n",
      " 1.41410408 1.88622101 1.41410408 1.41410408]\n",
      "all_sum is: [0.84691158 1.40225207 0.84691158 0.84691158 0.84691158 0.84691158\n",
      " 0.84691158 0.84691158 0.84691158 0.84691158 0.84691158 0.84691158\n",
      " 1.40225207 0.84691158 1.40225207 1.40225207 1.40225207 0.84691158\n",
      " 1.40225207 0.84691158 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 0.84691158 0.84691158 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 0.84691158 0.84691158 1.40225207 0.84691158 0.84691158 0.84691158\n",
      " 0.84691158 1.40225207 0.84691158 0.84691158 0.84691158 0.84691158\n",
      " 1.40225207 0.84691158 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 0.84691158 1.40225207 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 1.40225207 1.40225207 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 1.40225207 1.40225207 1.40225207 0.84691158 0.84691158 0.84691158\n",
      " 0.84691158 0.84691158 0.84691158 1.40225207 1.40225207 1.40225207\n",
      " 1.40225207 0.84691158 0.84691158 1.40225207 1.40225207 1.40225207\n",
      " 0.84691158 0.84691158 1.40225207 0.84691158 0.84691158 0.84691158\n",
      " 0.84691158 1.40225207 0.84691158 0.84691158 0.84691158 0.84691158\n",
      " 1.40225207 0.84691158 0.84691158 0.84691158 1.40225207 0.84691158\n",
      " 0.84691158 1.40225207 1.40225207 0.84691158]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.160\n",
      "(*) epoch 2, cost 1.903\n",
      "(*) epoch 3, cost 1.650\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.115\n",
      "(*) epoch 2, cost 2.568\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133 0.41936133\n",
      " 0.41936133 0.41936133 0.41936133 0.41936133]\n",
      "all_sum is: [0.6385606  0.6385606  0.6385606  0.6385606  0.6385606  1.74245026\n",
      " 0.67871228 0.67871228 0.         1.74245026 0.6385606  1.74245026\n",
      " 0.6385606  0.6385606  0.6385606  0.         0.6385606  0.6385606\n",
      " 0.6385606  0.6385606  0.         0.6385606  0.67871228 0.\n",
      " 0.6385606  0.67871228 0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  0.6385606  0.         0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  0.6385606  0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  1.78260194 0.         0.6385606  0.6385606  1.74245026\n",
      " 0.6385606  0.6385606  0.6385606  0.6385606  0.67871228 0.6385606\n",
      " 0.67871228 0.6385606  0.04015168 1.74245026 0.6385606  0.6385606\n",
      " 0.6385606  0.         0.6385606  0.6385606  1.74245026 0.6385606\n",
      " 0.6385606  0.6385606  0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  0.6385606  0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.67871228 0.6385606  0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  0.6385606  0.         0.6385606  0.6385606  0.6385606\n",
      " 0.6385606  0.         0.6385606  0.6385606  0.6385606  0.6385606\n",
      " 0.         0.         0.6385606  0.67871228]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.920\n",
      "(*) epoch 2, cost 2.164\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.267\n",
      "(*) epoch 2, cost 1.651\n",
      "(*) epoch 3, cost 1.430\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.55876694 3.92233724 0.55876694 3.02870403 3.02870403 3.02870403\n",
      " 3.02870403 3.02870403 3.02870403 3.92233724 3.02870403 3.02870403\n",
      " 3.92233724 0.55876694 0.55876694 3.92233724 3.02870403 3.92233724\n",
      " 3.02870403 3.02870403 0.55876694 0.55876694 3.02870403 3.92233724\n",
      " 1.45240014 3.02870403 1.45240014 3.02870403 0.55876694 1.45240014\n",
      " 3.92233724 0.55876694 0.55876694 3.02870403 1.45240014 0.55876694\n",
      " 3.02870403 1.45240014 3.02870403 3.02870403 3.02870403 3.02870403\n",
      " 0.55876694 0.55876694 3.02870403 0.55876694 3.02870403 0.55876694\n",
      " 0.55876694 0.55876694 3.92233724 3.02870403 3.02870403 1.45240014\n",
      " 0.55876694 3.02870403 3.02870403 0.55876694 0.55876694 3.02870403\n",
      " 0.55876694 3.02870403 3.02870403 3.02870403 3.02870403 3.92233724\n",
      " 3.02870403 0.55876694 3.02870403 3.02870403 3.02870403 3.92233724\n",
      " 3.02870403 3.02870403 0.55876694 3.02870403 3.02870403 3.02870403\n",
      " 3.02870403 0.55876694 0.55876694 1.45240014 3.02870403 3.92233724\n",
      " 1.45240014 3.02870403 0.55876694 0.55876694 3.02870403 3.02870403\n",
      " 3.02870403 3.02870403 3.02870403 3.92233724 0.55876694 3.92233724\n",
      " 0.55876694 3.02870403 3.02870403 1.45240014]\n",
      "all_sum is: [5.01472985 5.01472985 5.01472985 4.30651236 4.30651236 4.30651236\n",
      " 4.30651236 4.30651236 5.01472985 5.01472985 5.01472985 4.30651236\n",
      " 5.01472985 5.01472985 5.01472985 5.01472985 5.01472985 5.01472985\n",
      " 5.01472985 4.30651236 5.01472985 5.01472985 5.01472985 4.30651236\n",
      " 5.01472985 5.01472985 5.01472985 5.01472985 4.30651236 5.01472985\n",
      " 4.30651236 4.30651236 4.30651236 5.01472985 5.01472985 5.01472985\n",
      " 5.01472985 5.01472985 5.01472985 4.30651236 5.01472985 5.01472985\n",
      " 5.01472985 4.30651236 5.01472985 5.01472985 5.01472985 4.30651236\n",
      " 5.01472985 5.01472985 5.01472985 5.01472985 5.01472985 4.30651236\n",
      " 4.30651236 5.01472985 5.01472985 5.01472985 4.30651236 5.01472985\n",
      " 5.01472985 4.30651236 5.01472985 4.30651236 5.01472985 5.01472985\n",
      " 4.30651236 4.30651236 5.01472985 5.01472985 5.01472985 5.01472985\n",
      " 5.01472985 5.01472985 4.30651236 5.01472985 5.01472985 5.01472985\n",
      " 5.01472985 5.01472985 4.30651236 4.30651236 5.01472985 5.01472985\n",
      " 5.01472985 5.01472985 5.01472985 5.01472985 4.23289612 5.01472985\n",
      " 5.01472985 4.30651236 5.01472985 5.01472985 4.30651236 4.30651236\n",
      " 4.30651236 5.01472985 5.01472985 4.30651236]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.425\n",
      "(*) epoch 2, cost 2.762\n",
      "(*) epoch 3, cost 2.267\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.788\n",
      "(*) epoch 2, cost 3.473\n",
      "(*) epoch 3, cost 3.086\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.09354431 0.09354431 0.09354431 0.09354431 0.09354431 0.38498619\n",
      " 0.63669698 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.09354431 0.09354431 0.09354431 0.09354431 0.38498619 0.38498619\n",
      " 0.09354431 0.38498619 0.09354431 0.09354431 0.09354431 0.38498619\n",
      " 0.09354431 0.38498619 0.38498619 0.09354431 0.09354431 0.09354431\n",
      " 0.63669698 0.09354431 0.09354431 0.38498619 0.09354431 0.09354431\n",
      " 0.09354431 0.09354431 0.38498619 0.09354431 0.09354431 0.09354431\n",
      " 0.09354431 0.38498619 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431 0.38498619\n",
      " 0.09354431 0.38498619 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.09354431 0.38498619 0.09354431 0.09354431 0.38498619 0.09354431\n",
      " 0.63669698 0.09354431 0.09354431 0.09354431 0.38498619 0.09354431\n",
      " 0.09354431 0.09354431 0.38498619 0.09354431 0.09354431 0.09354431\n",
      " 0.38498619 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.38498619 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431 0.09354431\n",
      " 0.38498619 0.09354431 0.09354431 0.09354431]\n",
      "all_sum is: [2.67493312 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 3.73596824 2.22587896 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.22587896 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.22587896 2.22587896 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.22587896 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.67493312 2.02963734 2.67493312\n",
      " 2.22587896 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.22587896 2.67493312 2.67493312 3.3946687  2.67493312\n",
      " 2.22587896 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.22587896 2.67493312 2.22587896 2.67493312\n",
      " 2.67493312 2.22587896 2.67493312 2.67493312 2.22587896 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.67493312 2.22587896 2.67493312\n",
      " 2.67493312 2.22587896 2.67493312 2.22587896 2.22587896 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312 2.67493312\n",
      " 2.67493312 2.67493312 2.67493312 2.67493312 3.73596824 2.67493312\n",
      " 2.22587896 2.67493312 2.67493312 2.67493312]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 3\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.565\n",
      "(*) epoch 2, cost 0.754\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.286\n",
      "(*) epoch 2, cost 3.470\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.20249983 4.20249983 4.20249983 3.96964287 2.03619354 4.20249983\n",
      " 2.03619354 4.20249983 2.90497808 4.20249983 1.80333658 4.20249983\n",
      " 3.96964287 4.87418016 2.03619354 2.03619354 4.20249983 4.20249983\n",
      " 4.20249983 2.03619354 4.20249983 4.20249983 4.20249983 1.80333658\n",
      " 4.20249983 4.20249983 4.20249983 2.90497808 4.20249983 4.20249983\n",
      " 2.90497808 4.20249983 0.73867179 3.96964287 0.73867179 4.20249983\n",
      " 4.20249983 4.20249983 4.20249983 2.03619354 2.03619354 4.20249983\n",
      " 3.96964287 2.03619354 4.20249983 4.20249983 2.03619354 3.96964287\n",
      " 4.20249983 4.20249983 4.20249983 4.20249983 4.20249983 2.03619354\n",
      " 4.20249983 4.20249983 0.73867179 3.96964287 2.03619354 3.96964287\n",
      " 4.20249983 4.20249983 2.90497808 2.03619354 3.96964287 2.90497808\n",
      " 2.90497808 2.90497808 4.20249983 4.20249983 2.03619354 4.20249983\n",
      " 4.20249983 4.20249983 4.20249983 3.96964287 0.73867179 4.20249983\n",
      " 4.20249983 3.96964287 0.73867179 4.20249983 0.73867179 2.03619354\n",
      " 2.03619354 4.20249983 2.03619354 2.90497808 3.96964287 2.90497808\n",
      " 2.90497808 3.96964287 4.20249983 2.03619354 2.03619354 2.90497808\n",
      " 2.90497808 2.03619354 4.20249983 4.20249983]\n",
      "all_sum is: [3.67233    3.67233    3.67233    2.28469759 2.28469759 2.28469759\n",
      " 3.67233    3.67233    2.28469759 3.67233    3.67233    2.28469759\n",
      " 2.28469759 3.67233    2.28469759 2.28469759 3.67233    3.67233\n",
      " 3.67233    3.67233    2.28469759 3.67233    2.28469759 3.67233\n",
      " 2.28469759 3.67233    3.67233    2.28469759 2.28469759 2.28469759\n",
      " 2.28469759 2.28469759 2.28469759 3.67233    3.67233    3.67233\n",
      " 2.28469759 3.67233    3.67233    2.28469759 0.68941975 3.67233\n",
      " 2.28469759 2.28469759 2.28469759 2.28469759 2.29612508 2.28469759\n",
      " 3.67233    2.28469759 3.67233    2.28469759 2.28469759 3.67233\n",
      " 2.28469759 2.28469759 2.28469759 2.28469759 3.67233    2.28469759\n",
      " 2.28469759 4.25277396 2.28469759 4.24134647 3.67233    3.67233\n",
      " 3.67233    2.28469759 3.67233    3.67233    3.67233    3.67233\n",
      " 3.67233    3.67233    3.67233    2.28469759 3.67233    2.28469759\n",
      " 3.67233    2.28469759 2.28469759 3.67233    5.62897888 2.28469759\n",
      " 2.28469759 2.28469759 2.28469759 2.28469759 2.28469759 2.28469759\n",
      " 3.67233    3.67233    3.67233    3.67233    3.67233    2.28469759\n",
      " 2.28469759 2.29612508 5.62897888 2.28469759]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.125\n",
      "(*) epoch 2, cost 3.285\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 6.761\n",
      "(*) epoch 2, cost 4.179\n",
      "(*) epoch 3, cost 3.956\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.92128537 0.7307433  0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.7307433  0.92128537 0.7307433  0.7307433  0.92128537\n",
      " 0.92128537 0.98928722 0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.7307433  0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.7307433  0.92128537 0.92128537 0.92128537 0.7307433\n",
      " 0.92128537 0.92128537 0.7307433  0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.92128537 0.92128537 0.7307433  0.92128537 0.7307433\n",
      " 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.92128537 0.92128537 0.92128537 0.7307433  0.92128537\n",
      " 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.92128537 0.7307433  0.92128537 0.7307433  0.92128537\n",
      " 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.7307433  0.92128537 0.92128537 0.7307433  0.92128537 0.7307433\n",
      " 0.92128537 0.92128537 0.7307433  0.92128537 0.92128537 0.92128537\n",
      " 0.92128537 0.7307433  0.92128537 0.92128537 0.92128537 0.7307433\n",
      " 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537 0.92128537\n",
      " 0.7307433  0.7307433  1.17982929 0.92128537]\n",
      "all_sum is: [1.73954843 1.73954843 2.68726427 2.07635248 1.73954843 2.68726427\n",
      " 2.68726427 2.68726427 2.68726427 2.68726427 1.73954843 1.73954843\n",
      " 1.73954843 2.68726427 1.73954843 1.60309551 1.60309551 1.73954843\n",
      " 2.55081135 2.68726427 2.68726427 2.68726427 1.73954843 2.55081135\n",
      " 1.73954843 2.68726427 1.73954843 1.73954843 1.73954843 1.73954843\n",
      " 2.68726427 1.73954843 1.73954843 2.68726427 2.68726427 2.55081135\n",
      " 1.73954843 1.73954843 1.60309551 2.68726427 2.68726427 1.60309551\n",
      " 2.68726427 1.73954843 1.73954843 1.73954843 2.68726427 1.73954843\n",
      " 1.83228118 2.68726427 2.68726427 1.73954843 1.73954843 2.68726427\n",
      " 2.68726427 1.73954843 2.68726427 2.68726427 2.55081135 1.73954843\n",
      " 1.73954843 1.73954843 1.73954843 1.60309551 1.73954843 1.83228118\n",
      " 1.73954843 2.55081135 1.73954843 1.73954843 1.73954843 2.55081135\n",
      " 1.60309551 2.07635248 1.73954843 1.73954843 2.68726427 1.73954843\n",
      " 2.68726427 2.68726427 1.73954843 1.73954843 1.73954843 1.73954843\n",
      " 1.73954843 1.73954843 2.68726427 2.68726427 2.55081135 0.91915608\n",
      " 2.68726427 1.60309551 1.73954843 2.68726427 1.73954843 1.69582826\n",
      " 1.73954843 1.83228118 1.73954843 2.68726427]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.556\n",
      "(*) epoch 2, cost 2.655\n",
      "(*) epoch 3, cost 2.230\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.500\n",
      "(*) epoch 2, cost 3.163\n",
      "(*) epoch 3, cost 2.951\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.37155275 1.37155275 1.82186059 1.27583887 1.82186059 1.80192865\n",
      " 1.80192865 1.82186059 1.27583887 1.82186059 1.35162081 1.37155275\n",
      " 1.82186059 1.82186059 1.82186059 1.82186059 1.82186059 1.37155275\n",
      " 1.82186059 1.82186059 1.37155275 1.82186059 1.82186059 1.37155275\n",
      " 1.82186059 1.82186059 1.82186059 1.80192865 1.82186059 1.80192865\n",
      " 1.82186059 1.82186059 1.80192865 1.82186059 1.82186059 1.82186059\n",
      " 1.82186059 3.93259459 1.82186059 1.82186059 1.82186059 1.82186059\n",
      " 1.82186059 1.82186059 1.82186059 1.82186059 1.27583887 1.82186059\n",
      " 1.82186059 1.82186059 1.80192865 1.27583887 1.37155275 1.82186059\n",
      " 1.82186059 1.82186059 1.82186059 1.82186059 1.82186059 1.82186059\n",
      " 1.82186059 1.82186059 1.82186059 1.37155275 1.82186059 1.82186059\n",
      " 1.82186059 1.82186059 1.82186059 1.82186059 1.80192865 1.35162081\n",
      " 1.82186059 1.82186059 1.82186059 1.82186059 1.82186059 1.82186059\n",
      " 3.93259459 1.35162081 1.82186059 1.82186059 1.82186059 1.25590693\n",
      " 1.82186059 1.82186059 1.37155275 1.82186059 1.82186059 1.82186059\n",
      " 1.82186059 1.27583887 2.47084964 1.82186059 4.60151559 1.80192865\n",
      " 3.93259459 1.82186059 1.37155275 1.82186059]\n",
      "all_sum is: [1.63558928 1.97481036 1.13911984 2.00629584 2.00629584 1.72227221\n",
      " 1.97481036 1.17060532 1.75375769 1.05243691 3.01723079 0.76841328\n",
      " 1.97481036 2.00629584 1.97481036 1.85664196 1.05243691 0.91806717\n",
      " 2.00629584 2.00629584 1.17060532 1.97481036 1.13911984 2.00629584\n",
      " 2.00629584 1.75375769 1.72227221 1.13911984 1.72227221 1.97481036\n",
      " 2.00629584 2.00629584 2.00629584 1.72227221 1.13911984 2.00629584\n",
      " 2.00629584 0.79989876 2.00629584 1.88812744 2.00629584 1.88812744\n",
      " 1.97481036 2.00629584 1.75375769 1.63558928 2.00629584 1.72227221\n",
      " 2.00629584 1.75375769 1.72227221 3.04871627 1.97481036 1.97481036\n",
      " 1.97481036 1.85664196 1.13911984 1.63558928 2.00629584 1.97481036\n",
      " 1.05243691 1.17060532 1.88812744 1.88812744 2.00629584 1.75375769\n",
      " 2.00629584 1.97481036 1.97481036 2.00629584 1.97481036 1.17060532\n",
      " 1.88812744 2.00629584 2.00629584 2.00629584 1.05243691 2.00629584\n",
      " 2.00629584 1.75375769 2.00629584 1.97481036 1.72227221 2.00629584\n",
      " 2.00629584 1.88812744 2.00629584 1.17060532 1.75375769 0.91806717\n",
      " 1.72227221 1.88812744 1.97481036 0.91806717 2.00629584 2.00629584\n",
      " 1.97481036 2.00629584 2.00629584 1.6041038 ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.994\n",
      "(*) epoch 2, cost 2.762\n",
      "(*) epoch 3, cost 2.395\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.575\n",
      "(*) epoch 2, cost 3.199\n",
      "(*) epoch 3, cost 2.994\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.85275779\n",
      " 0.         0.85275779 0.85275779 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.85275779 0.         0.\n",
      " 0.85275779 0.85275779 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.85275779 0.\n",
      " 0.85275779 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.85275779 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.85275779 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.85275779 0.         0.         0.85275779 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "all_sum is: [0.         0.         0.         0.3849166  0.18697042 0.\n",
      " 0.3849166  0.3849166  0.         0.18697042 0.         0.\n",
      " 0.         0.         0.3849166  0.3849166  0.         0.\n",
      " 0.         0.18697042 0.3849166  0.         0.3849166  0.3849166\n",
      " 0.3849166  0.3849166  0.         0.         0.3849166  0.18697042\n",
      " 0.         0.         0.         0.3849166  0.         0.\n",
      " 0.         0.         0.         0.3849166  0.18697042 0.\n",
      " 0.         0.18697042 0.3849166  0.18697042 0.23410304 0.3849166\n",
      " 0.         0.         0.18697042 0.3849166  0.         0.\n",
      " 0.         0.3849166  0.         0.18697042 0.         0.\n",
      " 0.         0.         0.         0.         0.3849166  0.\n",
      " 0.3849166  0.3849166  0.3849166  0.         0.3849166  0.3849166\n",
      " 0.         0.23410304 0.         0.18697042 0.3849166  0.\n",
      " 0.         0.3849166  0.3849166  0.         0.18697042 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.23410304 0.         0.         0.         0.3849166\n",
      " 0.3849166  0.3849166  0.3849166  0.23410304]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 4.723\n",
      "(*) epoch 2, cost 1.374\n",
      "(*) epoch 3, cost 0.984\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.966\n",
      "(*) epoch 2, cost 1.441\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.66654988 1.40583703 1.43535506 2.17464221 1.43535506 1.43535506\n",
      " 0.66654988 1.40583703 1.40583703 1.43535506 1.40583703 1.40583703\n",
      " 1.43535506 1.40583703 2.17464221 0.66654988 1.43535506 1.43535506\n",
      " 1.43535506 1.40583703 2.17464221 0.66654988 2.17464221 1.40583703\n",
      " 1.43535506 0.66654988 2.17464221 0.66654988 1.40583703 1.43535506\n",
      " 2.17464221 1.43535506 2.17464221 0.66654988 2.17464221 1.40583703\n",
      " 0.66654988 2.17464221 1.43535506 1.40583703 1.40583703 0.66654988\n",
      " 0.66654988 0.66654988 2.17464221 1.43535506 1.43535506 1.43535506\n",
      " 2.17464221 1.40583703 1.40583703 1.40583703 1.43535506 0.66654988\n",
      " 1.40583703 1.40583703 1.43535506 1.40583703 2.17464221 2.17464221\n",
      " 2.17464221 1.40583703 2.17464221 2.17464221 1.40583703 2.17464221\n",
      " 1.43535506 1.40583703 1.40583703 0.66654988 0.66654988 1.40583703\n",
      " 1.40583703 2.17464221 1.40583703 1.43535506 1.40583703 2.17464221\n",
      " 1.40583703 1.40583703 2.17464221 1.40583703 2.17464221 2.17464221\n",
      " 0.66654988 1.40583703 1.43535506 1.40583703 2.17464221 2.17464221\n",
      " 1.40583703 2.17464221 0.66654988 1.40583703 1.43535506 1.40583703\n",
      " 1.40583703 1.40583703 2.17464221 0.66654988]\n",
      "all_sum is: [5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 6.25040017 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.3913422  5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 2.88318631 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 2.88318631\n",
      " 5.26942127 5.26942127 2.88318631 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 2.88318631 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 2.88318631 5.26942127 5.26942127\n",
      " 5.3913422  5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.3913422  5.26942127 2.88318631 5.26942127\n",
      " 5.26942127 2.88318631 5.26942127 2.88318631 5.26942127 2.88318631\n",
      " 5.26942127 2.88318631 2.88318631 5.26942127 5.26942127 5.26942127\n",
      " 5.26942127 5.26942127 5.26942127 5.26942127]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.951\n",
      "(*) epoch 2, cost 1.736\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.16 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.856\n",
      "(*) epoch 2, cost 3.381\n",
      "(*) epoch 3, cost 3.143\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.35011403 0.28607169 0.         0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.         0.28607169 0.28607169\n",
      " 0.28607169 0.         0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.85941097 0.28607169 0.         0.\n",
      " 0.         0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.35011403 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.         0.28607169\n",
      " 0.         0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.35011403 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169 0.28607169\n",
      " 0.28607169 0.28607169 0.28607169 0.28607169]\n",
      "all_sum is: [0.83804305 0.83804305 0.83804305 2.39250684 0.83804305 0.83804305\n",
      " 2.39250684 0.83804305 1.53122275 0.83804305 0.83804305 2.39250684\n",
      " 2.39250684 2.39250684 0.83804305 2.39250684 0.83804305 2.1806392\n",
      " 0.83804305 0.83804305 0.83804305 2.22252377 0.83804305 0.83804305\n",
      " 0.83804305 0.83804305 0.83804305 0.83804305 2.39250684 2.22252377\n",
      " 2.39250684 2.39250684 2.39250684 0.83804305 0.83804305 0.83804305\n",
      " 0.83804305 0.83804305 0.83804305 0.83804305 0.83804305 2.39250684\n",
      " 0.83804305 0.83804305 0.83804305 0.83804305 0.83804305 0.83804305\n",
      " 0.83804305 0.83804305 0.83804305 2.39250684 3.77698756 2.39250684\n",
      " 2.39250684 0.83804305 0.83804305 0.83804305 2.39250684 0.83804305\n",
      " 2.39250684 2.39250684 3.77698756 2.39250684 0.83804305 0.83804305\n",
      " 0.83804305 0.83804305 0.83804305 2.39250684 0.83804305 0.83804305\n",
      " 2.39250684 2.39250684 1.53122275 2.39250684 2.39250684 0.83804305\n",
      " 2.39250684 0.83804305 0.83804305 2.39250684 0.83804305 0.83804305\n",
      " 0.83804305 0.83804305 2.39250684 2.39250684 2.39250684 3.77698756\n",
      " 2.22252377 2.39250684 2.39250684 0.83804305 2.39250684 2.39250684\n",
      " 2.39250684 2.39250684 2.39250684 2.39250684]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 1.186\n",
      "(*) epoch 2, cost 0.865\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.19 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.711\n",
      "(*) epoch 2, cost 2.766\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [2.59398658 2.59398658 2.59398658 3.48821403 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 4.29627492 3.48821403 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 3.48821403 4.14678831 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 3.48821403 2.59398658 3.48821403 2.59398658 2.59398658 2.59398658\n",
      " 3.48821403 2.59398658 2.59398658 2.59398658 2.59398658 3.48821403\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 3.48821403 1.97031607 4.14678831 2.59398658\n",
      " 2.59398658 2.59398658 3.48821403 2.59398658 2.59398658 2.59398658\n",
      " 4.29627492 2.59398658 2.59398658 3.48821403 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 3.48821403 2.59398658 2.59398658\n",
      " 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658 2.59398658\n",
      " 4.14678831 2.59398658 2.59398658 2.59398658 2.59398658 4.14678831\n",
      " 2.59398658 2.59398658 4.14678831 2.59398658]\n",
      "all_sum is: [0.30384789 0.72325966 0.30384789 1.36047921 1.36047921 2.39997271\n",
      " 0.30384789 0.30384789 0.72325966 0.30384789 2.81938448 2.39997271\n",
      " 0.72325966 0.94106743 3.03719225 2.39997271 0.94106743 0.30384789\n",
      " 3.45660403 0.72325966 0.30384789 0.30384789 0.72325966 0.94106743\n",
      " 0.72325966 0.72325966 0.89440498 2.81938448 2.39997271 2.39997271\n",
      " 0.94106743 0.30384789 0.72325966 1.36047921 2.39997271 0.30384789\n",
      " 0.30384789 0.30384789 3.03719225 2.81938448 0.30384789 0.30384789\n",
      " 0.72325966 1.02710876 0.89440498 0.30384789 0.94106743 0.30384789\n",
      " 0.30384789 1.11221275 2.39997271 0.94106743 0.30384789 0.94106743\n",
      " 0.89440498 2.39997271 0.72325966 0.94106743 0.30384789 0.94106743\n",
      " 0.30384789 0.30384789 3.45660403 0.72325966 0.72325966 0.94106743\n",
      " 0.72325966 0.30384789 0.30384789 1.36047921 1.36047921 0.30384789\n",
      " 0.30384789 1.02710876 0.72325966 0.72325966 0.94106743 0.89440498\n",
      " 0.72325966 0.72325966 0.30384789 0.30384789 2.39997271 3.45660403\n",
      " 0.30384789 0.94106743 2.81938448 0.30384789 0.30384789 0.72325966\n",
      " 2.81938448 2.57111803 0.72325966 1.36047921 0.30384789 0.30384789\n",
      " 1.36047921 2.81938448 0.30384789 0.30384789]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.427\n",
      "(*) epoch 2, cost 2.948\n",
      "(*) epoch 3, cost 2.737\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.458\n",
      "(*) epoch 2, cost 3.154\n",
      "(*) epoch 3, cost 2.747\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884 1.26520884\n",
      " 1.26520884 1.26520884 1.26520884 1.26520884]\n",
      "all_sum is: [5.73560976 5.73560976 6.82822999 5.73560976 5.73560976 3.5574652\n",
      " 7.08515716 5.73560976 5.73560976 5.02262577 3.5574652  5.73560976\n",
      " 6.37217316 2.84448121 5.73560976 5.02262577 2.84448121 5.02262577\n",
      " 5.73560976 5.73560976 7.08515716 7.08515716 5.73560976 7.08515716\n",
      " 5.73560976 5.02262577 7.08515716 5.02262577 5.02262577 5.73560976\n",
      " 5.73560976 7.08515716 5.73560976 5.4786826  5.02262577 5.73560976\n",
      " 6.37217316 7.08515716 5.73560976 8.46067587 5.73560976 7.08515716\n",
      " 4.7656986  5.02262577 5.73560976 4.9070126  7.08515716 5.02262577\n",
      " 5.4786826  3.5574652  7.08515716 3.5574652  5.73560976 5.73560976\n",
      " 4.9070126  7.08515716 5.73560976 5.73560976 5.73560976 4.9070126\n",
      " 3.5574652  5.73560976 3.5574652  5.73560976 7.08515716 5.73560976\n",
      " 5.73560976 4.9070126  3.5574652  6.39814448 3.5574652  4.1940286\n",
      " 5.73560976 5.02262577 5.73560976 5.73560976 5.73560976 5.73560976\n",
      " 5.73560976 5.73560976 4.9070126  4.93298392 5.73560976 3.5574652\n",
      " 4.9070126  7.08515716 5.73560976 7.08515716 7.08515716 5.4786826\n",
      " 5.73560976 8.46067587 5.73560976 5.73560976 5.73560976 5.02262577\n",
      " 6.37217316 5.73560976 5.73560976 5.73560976]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 1, cost 2.211\n",
      "(*) epoch 2, cost 0.732\n",
      "(*) epoch 3, cost 0.543\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 8\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.780\n",
      "(*) epoch 2, cost 3.343\n",
      "(*) epoch 3, cost 3.014\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.28 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         0.93790065 0.         0.         0.45734269\n",
      " 1.02901853 0.93790065 1.52342625 0.         1.02901853 0.45734269\n",
      " 0.93790065 1.52342625 1.52342625 0.         0.         0.\n",
      " 0.         0.93790065 0.         0.         0.6766435  1.0428683\n",
      " 0.         0.         1.39524334 1.02901853 0.         0.09111789\n",
      " 1.52342625 0.         1.0428683  0.93790065 1.52342625 0.93790065\n",
      " 0.93790065 0.93790065 0.         0.         0.09111789 0.\n",
      " 0.         1.52342625 0.         1.02901853 0.93790065 0.\n",
      " 0.         0.         0.         0.09111789 0.         0.93790065\n",
      " 0.         0.         0.93790065 0.58552561 0.         0.\n",
      " 1.39524334 0.93790065 0.93790065 0.09111789 0.         0.\n",
      " 0.         0.93790065 0.93790065 0.         0.93790065 1.02901853\n",
      " 1.39524334 0.93790065 0.         0.93790065 0.58552561 0.09111789\n",
      " 0.         0.         0.         1.52342625 0.93790065 0.93790065\n",
      " 0.93790065 0.58552561 0.         1.0428683  0.93790065 0.\n",
      " 0.93790065 0.93790065 1.52342625 0.         0.         0.93790065\n",
      " 0.93790065 0.         2.26506491 1.52342625]\n",
      "all_sum is: [0.         0.         0.06026573 0.06026573 0.06026573 0.41514259\n",
      " 0.06026573 0.06026573 0.41514259 0.06026573 0.06026573 0.06026573\n",
      " 0.06026573 0.         0.06026573 0.06026573 0.         0.06026573\n",
      " 0.06026573 0.         0.         0.41514259 0.06026573 0.35487686\n",
      " 0.35487686 0.35487686 0.41514259 0.         0.35487686 0.06026573\n",
      " 0.         0.         0.06026573 0.35487686 0.41514259 0.41514259\n",
      " 0.06026573 0.06026573 0.06026573 0.06026573 0.06026573 0.41514259\n",
      " 0.         0.         0.41514259 0.         0.         0.06026573\n",
      " 0.06026573 0.06026573 0.         0.06026573 0.06026573 0.\n",
      " 0.06026573 0.         0.06026573 0.41514259 0.06026573 0.\n",
      " 0.06026573 0.41514259 0.41514259 0.         0.         0.06026573\n",
      " 0.06026573 0.         0.         0.06026573 0.06026573 0.\n",
      " 0.         0.06026573 0.41514259 0.06026573 0.06026573 0.06026573\n",
      " 0.35487686 0.06026573 0.         0.06026573 0.06026573 0.06026573\n",
      " 0.06026573 0.06026573 0.06026573 0.         0.         0.41514259\n",
      " 0.         0.06026573 0.         0.06026573 0.         0.\n",
      " 0.35487686 0.         0.35487686 0.41514259]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.226\n",
      "(*) epoch 2, cost 2.464\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 4.517\n",
      "(*) epoch 2, cost 1.950\n",
      "(*) epoch 3, cost 1.626\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615 0.55527615\n",
      " 0.55527615 0.55527615 0.55527615 0.55527615]\n",
      "all_sum is: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.24549215 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.92668331 0.         0.         0.24549215 0.30988306\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.24549215 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.24549215 0.        ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.326\n",
      "(*) epoch 2, cost 3.010\n",
      "(*) epoch 3, cost 2.710\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.893\n",
      "(*) epoch 2, cost 1.355\n",
      "(*) epoch 3, cost 0.938\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [4.18936343 3.9045769  4.43121223 3.9045769  3.9045769  3.9045769\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769  3.9045769  4.18936343\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769  4.18936343 3.9045769\n",
      " 3.9045769  4.1464257  3.9045769  4.1464257  4.18936343 3.9045769\n",
      " 4.18936343 3.9045769  3.9045769  3.9045769  4.18936343 3.9045769\n",
      " 3.9045769  3.9045769  4.18936343 4.18936343 4.18936343 4.18936343\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769  4.18936343 3.9045769\n",
      " 3.9045769  3.9045769  4.18936343 4.18936343 3.9045769  3.9045769\n",
      " 4.1464257  3.9045769  3.9045769  3.9045769  4.1464257  3.9045769\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769  4.18936343 3.9045769\n",
      " 3.9045769  4.18936343 4.18936343 3.9045769  3.9045769  3.9045769\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769  3.9045769  3.9045769\n",
      " 4.18936343 3.9045769  3.9045769  3.9045769  3.9045769  3.9045769\n",
      " 3.9045769  4.18936343 3.9045769  4.18936343 4.18936343 3.9045769\n",
      " 4.18936343 4.18936343 3.9045769  3.9045769  4.18936343 3.9045769\n",
      " 3.9045769  3.9045769  3.9045769  4.18936343 3.9045769  3.9045769\n",
      " 3.9045769  3.9045769  3.9045769  3.9045769 ]\n",
      "all_sum is: [0.34550438 0.         0.34550438 0.         0.34550438 0.\n",
      " 0.34550438 0.         0.34550438 0.         0.34550438 0.\n",
      " 0.34550438 0.         0.         0.34550438 0.         0.\n",
      " 0.         0.34550438 0.34550438 0.         0.         0.\n",
      " 0.         0.         0.         0.34550438 0.         0.34550438\n",
      " 0.         0.         0.         0.34550438 0.         0.\n",
      " 0.         0.34550438 0.34550438 0.         0.         0.\n",
      " 0.34550438 0.         0.         0.         0.34550438 0.\n",
      " 0.         0.34550438 0.         0.         0.         0.\n",
      " 0.34550438 0.         0.         0.         0.         0.34550438\n",
      " 0.34550438 0.         0.34550438 0.34550438 0.         0.\n",
      " 0.         0.34550438 0.         0.         0.34550438 0.\n",
      " 0.34550438 0.34550438 0.         0.34550438 0.34550438 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34550438 0.         0.\n",
      " 0.         0.34550438 0.34550438 0.34550438 0.         0.34550438\n",
      " 0.34550438 0.34550438 0.         0.34550438]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.679\n",
      "(*) epoch 2, cost 1.657\n",
      "(*) epoch 3, cost 1.477\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 5\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.844\n",
      "(*) epoch 2, cost 2.391\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.06781103 0.         0.06781103 0.         0.06781103\n",
      " 0.         0.06781103 0.06781103 0.06781103 0.06781103 0.06781103\n",
      " 0.         0.06781103 0.         0.         0.         0.06781103\n",
      " 0.         0.         0.06781103 0.06781103 0.         0.\n",
      " 0.06781103 0.         0.06781103 0.06781103 0.06781103 0.32645398\n",
      " 0.         0.         0.06781103 0.         0.06781103 0.\n",
      " 0.06781103 0.         0.06781103 0.         0.         0.39426501\n",
      " 0.06781103 0.06781103 0.06781103 0.32645398 0.39426501 0.06781103\n",
      " 0.06781103 0.32645398 0.06781103 0.         0.06781103 1.77100199\n",
      " 0.         0.         0.32645398 0.         0.39426501 0.\n",
      " 0.06781103 0.06781103 0.         0.06781103 0.         0.\n",
      " 0.06781103 0.06781103 0.06781103 0.06781103 0.06781103 0.06781103\n",
      " 0.32645398 0.         0.06781103 0.         0.39426501 0.06781103\n",
      " 0.         0.06781103 0.         0.         0.         0.\n",
      " 0.06781103 0.06781103 0.06781103 0.06781103 0.         0.06781103\n",
      " 0.06781103 0.         0.         0.06781103 0.06781103 0.\n",
      " 0.         0.06781103 0.06781103 0.06781103]\n",
      "all_sum is: [5.51712119 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 5.51712119 3.34384775 5.51712119 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 5.51712119 3.34384775 3.34384775\n",
      " 5.51712119 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.87186325 3.34384775 5.51712119\n",
      " 3.87186325 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 5.51712119 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 5.51712119 3.34384775 3.34384775 3.34384775 3.34384775 3.34384775\n",
      " 3.34384775 5.51712119 3.34384775 3.34384775]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 3.033\n",
      "(*) epoch 2, cost 2.180\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 4\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 2.138\n",
      "(*) epoch 2, cost 0.655\n",
      "(*) epoch 3, cost 0.374\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.27 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "all_sum is: [0.         0.         2.45817181 2.45817181 2.45817181 0.\n",
      " 0.         3.17544786 2.45817181 0.         2.45817181 0.71727606\n",
      " 2.45817181 0.         2.45817181 2.45817181 2.45817181 0.\n",
      " 0.         0.         2.45817181 2.45817181 1.85990072 0.\n",
      " 2.45817181 2.45817181 2.45817181 2.45817181 0.         0.\n",
      " 2.45817181 1.41230125 0.         2.45817181 2.45817181 2.45817181\n",
      " 2.45817181 2.45817181 0.         2.45817181 2.45817181 2.45817181\n",
      " 0.         2.45817181 2.45817181 2.45817181 2.45817181 2.45817181\n",
      " 2.45817181 2.45817181 2.45817181 2.45817181 0.         2.45817181\n",
      " 2.45817181 2.45817181 0.         2.45817181 2.45817181 0.\n",
      " 0.         2.45817181 2.45817181 2.45817181 2.45817181 2.45817181\n",
      " 2.45817181 3.08574821 2.45817181 0.         0.         2.45817181\n",
      " 2.45817181 0.         2.45817181 2.45817181 2.45817181 2.45817181\n",
      " 0.         2.45817181 2.45817181 2.45817181 0.         2.45817181\n",
      " 2.45817181 0.         2.45817181 0.         2.45817181 2.45817181\n",
      " 2.45817181 2.45817181 0.         0.         0.         2.45817181\n",
      " 2.45817181 2.45817181 2.45817181 0.        ]\n",
      "all_sum is: [2.56985    2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.56985    2.67051303 2.56985    2.56985    2.56985\n",
      " 2.67051303 2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.11024098 2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.67051303 2.56985    2.56985    2.56985    2.67051303\n",
      " 2.56985    2.56985    2.56985    2.67051303 2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.67051303\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.67051303 2.56985\n",
      " 2.67051303 2.67051303 2.56985    2.56985    2.67051303 2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.56985\n",
      " 0.95947095 2.56985    2.67051303 2.67051303 2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985    2.56985    2.67051303\n",
      " 2.56985    2.56985    0.95947095 2.56985    2.56985    2.56985\n",
      " 2.56985    2.56985    2.56985    2.56985   ]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 7\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.637\n",
      "(*) epoch 2, cost 3.430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/wanxinli/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) epoch 3, cost 3.086\n",
      "(*) training time: 0.01 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n",
      "initializing: 1-layer SDAs\n",
      "\n",
      "initialized: DA [layer: 1]\n",
      "(*) no. of visible units: 6\n",
      "(*) no. of hidden units: 5\n",
      "(*) data corruption level: 0.05\n",
      "(*) learning rate: 0.10\n",
      "(*) batch size: 5\n",
      "(*) no. of epochs: 10\n",
      "\n",
      "training: 1-layer SDAs\n",
      "\n",
      "training: DA [layer: 1]\n",
      "(*) preprocessing: normalize features\n",
      "(*) epoch 1, cost 5.064\n",
      "(*) epoch 2, cost 2.514\n",
      "(*) epoch 3, cost 2.121\n",
      "(*) training time: 0.00 sec.\n",
      "\n",
      "training time: 0.17 sec.\n",
      "\n",
      "applying: 1-layer SDA\n",
      "(*) applying: DA [layer: 1]\n"
     ]
    }
   ],
   "source": [
    "male_accuracies, male_precisions, male_recalls, \\\n",
    "    female_accuracies, female_precisions, female_recalls, \\\n",
    "    trans_female_accuracies, trans_female_precisions, trans_female_recalls = \\\n",
    "    run_proc_multi(simulate_1, custom_train_reps, n_times=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.78,\n",
       "  0.97,\n",
       "  0.98,\n",
       "  1.0,\n",
       "  0.97,\n",
       "  0.81,\n",
       "  1.0,\n",
       "  0.78,\n",
       "  0.94,\n",
       "  0.74,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.83,\n",
       "  0.99,\n",
       "  0.99,\n",
       "  0.98,\n",
       "  0.94,\n",
       "  0.95,\n",
       "  0.99,\n",
       "  0.97,\n",
       "  1.0,\n",
       "  0.84,\n",
       "  0.98,\n",
       "  0.87,\n",
       "  0.81,\n",
       "  0.97,\n",
       "  0.76,\n",
       "  1.0,\n",
       "  0.87,\n",
       "  1.0,\n",
       "  0.73,\n",
       "  0.74,\n",
       "  0.98,\n",
       "  0.67,\n",
       "  0.97,\n",
       "  0.95,\n",
       "  0.84,\n",
       "  0.98,\n",
       "  0.74,\n",
       "  0.87,\n",
       "  0.79,\n",
       "  0.92,\n",
       "  0.74,\n",
       "  1.0],\n",
       " [0.819672131147541,\n",
       "  1.0,\n",
       "  0.9523809523809523,\n",
       "  1.0,\n",
       "  0.961038961038961,\n",
       "  0.9,\n",
       "  1.0,\n",
       "  0.9230769230769231,\n",
       "  0.918918918918919,\n",
       "  0.6944444444444444,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.8260869565217391,\n",
       "  0.9811320754716981,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9230769230769231,\n",
       "  0.9333333333333333,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.84,\n",
       "  0.9743589743589743,\n",
       "  0.8113207547169812,\n",
       "  0.7931034482758621,\n",
       "  0.9387755102040817,\n",
       "  0.7604166666666666,\n",
       "  1.0,\n",
       "  0.87,\n",
       "  1.0,\n",
       "  0.8064516129032258,\n",
       "  0.7634408602150538,\n",
       "  0.967741935483871,\n",
       "  0.68,\n",
       "  0.896551724137931,\n",
       "  0.9534883720930233,\n",
       "  0.7916666666666666,\n",
       "  1.0,\n",
       "  0.7078651685393258,\n",
       "  1.0,\n",
       "  0.75,\n",
       "  0.92,\n",
       "  0.9032258064516129,\n",
       "  1.0],\n",
       " [0.819672131147541,\n",
       "  0.9230769230769231,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5142857142857142,\n",
       "  1.0,\n",
       "  0.36363636363636365,\n",
       "  1.0,\n",
       "  0.625,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.987012987012987,\n",
       "  1.0,\n",
       "  0.9333333333333333,\n",
       "  0.95,\n",
       "  1.0,\n",
       "  0.7777777777777778,\n",
       "  0.9375,\n",
       "  0.8636363636363636,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9743589743589743,\n",
       "  0.9347826086956522,\n",
       "  0.9857142857142858,\n",
       "  1.0,\n",
       "  0.9864864864864865,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.5434782608695652,\n",
       "  0.9466666666666667,\n",
       "  1.0,\n",
       "  0.6666666666666666,\n",
       "  1.0,\n",
       "  0.9318181818181818,\n",
       "  0.8636363636363636,\n",
       "  0.8666666666666667,\n",
       "  1.0,\n",
       "  0.4090909090909091,\n",
       "  0.9836065573770492,\n",
       "  1.0,\n",
       "  0.5490196078431373,\n",
       "  1.0],\n",
       " [0.28,\n",
       "  0.14,\n",
       "  0.74,\n",
       "  0.27,\n",
       "  0.45,\n",
       "  0.05,\n",
       "  0.29,\n",
       "  0.19,\n",
       "  0.35,\n",
       "  0.29,\n",
       "  0.9,\n",
       "  0.17,\n",
       "  0.55,\n",
       "  0.49,\n",
       "  0.38,\n",
       "  0.22,\n",
       "  0.6,\n",
       "  0.67,\n",
       "  0.43,\n",
       "  0.57,\n",
       "  0.27,\n",
       "  0.45,\n",
       "  0.66,\n",
       "  0.8,\n",
       "  0.74,\n",
       "  0.65,\n",
       "  0.3,\n",
       "  0.62,\n",
       "  0.22,\n",
       "  0.06,\n",
       "  0.63,\n",
       "  0.26,\n",
       "  0.16,\n",
       "  0.04,\n",
       "  0.03,\n",
       "  0.05,\n",
       "  0.37,\n",
       "  0.23,\n",
       "  0.18,\n",
       "  0.33,\n",
       "  0.56,\n",
       "  0.07,\n",
       "  0.17,\n",
       "  0.17],\n",
       " [0.038461538461538464,\n",
       "  0.17391304347826086,\n",
       "  0.21739130434782608,\n",
       "  0.47368421052631576,\n",
       "  0.5,\n",
       "  0.06896551724137931,\n",
       "  0.045454545454545456,\n",
       "  0.05263157894736842,\n",
       "  0.03333333333333333,\n",
       "  0.5,\n",
       "  0.9886363636363636,\n",
       "  0.17647058823529413,\n",
       "  0.5238095238095238,\n",
       "  0.5157894736842106,\n",
       "  0.3103448275862069,\n",
       "  0.2727272727272727,\n",
       "  0.7272727272727273,\n",
       "  0.6071428571428571,\n",
       "  0.92,\n",
       "  0.03571428571428571,\n",
       "  0.9615384615384616,\n",
       "  0.5421686746987951,\n",
       "  0.5135135135135135,\n",
       "  0.5,\n",
       "  0.7647058823529411,\n",
       "  0.8524590163934426,\n",
       "  0.2647058823529412,\n",
       "  0.6262626262626263,\n",
       "  0.08333333333333333,\n",
       "  0.36363636363636365,\n",
       "  0.3333333333333333,\n",
       "  0.65,\n",
       "  0.75,\n",
       "  0.1111111111111111,\n",
       "  0.02040816326530612,\n",
       "  0.10416666666666667,\n",
       "  0.13114754098360656,\n",
       "  0.14516129032258066,\n",
       "  0.3333333333333333,\n",
       "  0.7272727272727273,\n",
       "  0.6,\n",
       "  0.109375,\n",
       "  0.13253012048192772,\n",
       "  0.15151515151515152],\n",
       " [0.020833333333333332,\n",
       "  0.14285714285714285,\n",
       "  0.38461538461538464,\n",
       "  0.125,\n",
       "  0.5818181818181818,\n",
       "  0.08888888888888889,\n",
       "  0.0196078431372549,\n",
       "  0.015625,\n",
       "  0.2222222222222222,\n",
       "  0.352112676056338,\n",
       "  0.90625,\n",
       "  0.3076923076923077,\n",
       "  0.8979591836734694,\n",
       "  0.9074074074074074,\n",
       "  0.9310344827586207,\n",
       "  0.0410958904109589,\n",
       "  0.17777777777777778,\n",
       "  0.4358974358974359,\n",
       "  0.2948717948717949,\n",
       "  0.058823529411764705,\n",
       "  0.25773195876288657,\n",
       "  0.7258064516129032,\n",
       "  0.5428571428571428,\n",
       "  0.05,\n",
       "  0.37142857142857144,\n",
       "  0.6666666666666666,\n",
       "  0.16666666666666666,\n",
       "  0.9841269841269841,\n",
       "  0.034482758620689655,\n",
       "  0.04395604395604396,\n",
       "  0.027777777777777776,\n",
       "  0.3023255813953488,\n",
       "  0.03488372093023256,\n",
       "  0.058823529411764705,\n",
       "  0.6666666666666666,\n",
       "  0.08771929824561403,\n",
       "  0.4444444444444444,\n",
       "  0.2727272727272727,\n",
       "  0.18840579710144928,\n",
       "  0.2926829268292683,\n",
       "  0.25,\n",
       "  0.16279069767441862,\n",
       "  0.5,\n",
       "  0.2702702702702703],\n",
       " [0.59,\n",
       "  0.24,\n",
       "  0.69,\n",
       "  0.27,\n",
       "  0.44,\n",
       "  0.35,\n",
       "  0.31,\n",
       "  0.25,\n",
       "  0.35,\n",
       "  0.47,\n",
       "  0.19,\n",
       "  0.39,\n",
       "  0.57,\n",
       "  0.55,\n",
       "  0.83,\n",
       "  0.35,\n",
       "  0.63,\n",
       "  0.6,\n",
       "  0.33,\n",
       "  0.66,\n",
       "  0.47,\n",
       "  0.62,\n",
       "  0.64,\n",
       "  0.59,\n",
       "  0.48,\n",
       "  0.59,\n",
       "  0.52,\n",
       "  0.39,\n",
       "  0.58,\n",
       "  0.74,\n",
       "  0.45,\n",
       "  0.79,\n",
       "  0.74,\n",
       "  0.18,\n",
       "  0.68,\n",
       "  0.44,\n",
       "  0.52,\n",
       "  0.58,\n",
       "  0.6,\n",
       "  0.27,\n",
       "  0.6,\n",
       "  0.43,\n",
       "  0.55,\n",
       "  0.46],\n",
       " [0.5573770491803278,\n",
       "  0.2222222222222222,\n",
       "  0.2857142857142857,\n",
       "  0.47368421052631576,\n",
       "  0.4935064935064935,\n",
       "  0.0,\n",
       "  0.09090909090909091,\n",
       "  0.07692307692307693,\n",
       "  0.12162162162162163,\n",
       "  0.75,\n",
       "  1.0,\n",
       "  0.23809523809523808,\n",
       "  0.532608695652174,\n",
       "  0.5849056603773585,\n",
       "  0.9285714285714286,\n",
       "  0.6052631578947368,\n",
       "  0.5512820512820513,\n",
       "  0.4666666666666667,\n",
       "  0.8666666666666667,\n",
       "  0.05263157894736842,\n",
       "  0.9583333333333334,\n",
       "  0.62,\n",
       "  0.48717948717948717,\n",
       "  0.3018867924528302,\n",
       "  0.40229885057471265,\n",
       "  0.8775510204081632,\n",
       "  0.53125,\n",
       "  0.5172413793103449,\n",
       "  0.58,\n",
       "  0.9113924050632911,\n",
       "  0.1935483870967742,\n",
       "  0.8494623655913979,\n",
       "  0.9838709677419355,\n",
       "  0.36,\n",
       "  0.0,\n",
       "  0.5116279069767442,\n",
       "  0.1875,\n",
       "  0.15384615384615385,\n",
       "  0.6629213483146067,\n",
       "  1.0,\n",
       "  0.55,\n",
       "  0.43,\n",
       "  0.12903225806451613,\n",
       "  0.20689655172413793],\n",
       " [0.7083333333333334,\n",
       "  0.14285714285714285,\n",
       "  0.9230769230769231,\n",
       "  0.125,\n",
       "  0.6909090909090909,\n",
       "  0.0,\n",
       "  0.0392156862745098,\n",
       "  0.015625,\n",
       "  1.0,\n",
       "  0.38028169014084506,\n",
       "  0.15625,\n",
       "  0.2564102564102564,\n",
       "  1.0,\n",
       "  0.5740740740740741,\n",
       "  0.4482758620689655,\n",
       "  0.3150684931506849,\n",
       "  0.9555555555555556,\n",
       "  0.1794871794871795,\n",
       "  0.16666666666666666,\n",
       "  0.058823529411764705,\n",
       "  0.4742268041237113,\n",
       "  1.0,\n",
       "  0.5428571428571428,\n",
       "  0.8,\n",
       "  1.0,\n",
       "  0.5512820512820513,\n",
       "  0.9444444444444444,\n",
       "  0.47619047619047616,\n",
       "  1.0,\n",
       "  0.7912087912087912,\n",
       "  0.16666666666666666,\n",
       "  0.9186046511627907,\n",
       "  0.7093023255813954,\n",
       "  0.2647058823529412,\n",
       "  0.0,\n",
       "  0.38596491228070173,\n",
       "  0.5,\n",
       "  0.06060606060606061,\n",
       "  0.855072463768116,\n",
       "  0.10975609756097561,\n",
       "  0.9166666666666666,\n",
       "  1.0,\n",
       "  0.18181818181818182,\n",
       "  0.16216216216216217])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_accuracies, male_precisions, male_recalls, \\\n",
    "    female_accuracies, female_precisions, female_recalls, \\\n",
    "    trans_female_accuracies, trans_female_precisions, trans_female_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_path = \"../outputs/scores.csv\"\n",
    "save_scores(male_accuracies, male_precisions, male_recalls, \\\n",
    "        female_accuracies, female_precisions, female_recalls, \\\n",
    "        trans_female_accuracies, trans_female_precisions, trans_female_recalls, score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x1152 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_plot(score_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
